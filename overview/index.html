<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Tutorials - cl-waffe2 Documentation</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Tutorials";
        var mkdocs_page_input_path = "overview.md";
        var mkdocs_page_url = null;
      </script>
    
    <script src="../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> cl-waffe2 Documentation
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../install/">Install</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="./">Tutorials</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#project-structure">Project Structure</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#fundamental-system">Fundamental System</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#standard-apis">Standard APIs</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#standard-backendsimplemenetations">Standard Backends/Implemenetations</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#neural-network">Neural Network</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#utils">Utils</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#to-get-started">To Get Started!</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#basic-building-computation-nodes-lazily">Basic: Building Computation Nodes Lazily</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#abstracttensor-one-operation-multiple-implementations">AbstractTensor - One operation, Multiple implementations.</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#background">Background</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#example-addnode">Example: AddNode</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#compiling-and-in-place-optimizing">Compiling, and In-place optimizing</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#compiled-model">Compiled Model</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#in-place-optimizing">In-place optimizing</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#before-optimized-vs-after-optimized">Before Optimized Vs After Optimized.</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#network-units-node-and-composite">Network Units: Node and Composite</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#node-and-composite">Node and Composite</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#proceed-vs-composite-function">Proceed vs Composite-function</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#sequence-model">Sequence Model</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#shaping-api-with-dsl">Shaping API with DSL</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#view-apisbroadcasting">View APIs/Broadcasting</a>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Tips/">Tips</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../utils/">cl-waffe2</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../nodes/">cl-waffe2/vm.nodes</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../generic-tensor/">cl-waffe2/vm.generic-tensor</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../base-impl/">[Functions] cl-waffe2/base-impl</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../base-impl-nodes/">[Nodes] cl-waffe2/base-impl</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../distributions/">cl-waffe2/distributions</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../nn/">cl-waffe2/nn</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../optimizer/">cl-waffe2/optimizers</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">cl-waffe2 Documentation</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a> &raquo;</li>
      <li>Tutorials</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
  integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs"
  crossorigin="anonymous" />

<style type="text/css">
    .katex img {
      object-fit: fill;
      padding: unset;
      display: block;
      position: absolute;
      width: 100%;
      height: inherit;
    }
</style>
<h1 id="a-road-to-cl-waffe2">A road to cl-waffe2</h1>
<h2 id="project-structure">Project Structure</h2>
<p>Thank you for having interest in my project. Before we start the tutorial, let me explain the structure of cl-waffe2 package.</p>
<p>Mainly, cl-waffe2 consists of the following packages.</p>
<h3 id="fundamental-system">Fundamental System</h3>
<p>These two package form the basis of cl-waffe2:</p>
<pre><code>:cl-waffe2/vm.nodes
:cl-waffe2/vm.generic-tensor
</code></pre>
<p>The package <code>:cl-waffe2/vm.nodes</code> provides a system for constructing neural networks, including <code>AbstractNode</code>, <code>Composite</code>, <code>Shaping API</code> etc...</p>
<p>On the other hand, <code>:cl-waffe2/vm.generic-tensor</code> provides features on <code>AbstractTensor</code>, including <code>JIT Compiler</code>, <code>NodeTensor</code>, <code>Memory-Pool</code> etc...</p>
<h3 id="standard-apis">Standard APIs</h3>
<pre><code class="language-lisp">(Figure: Dependencies of cl-waffe2)

                            [CPUBackend ]
            [base-impl] --- [LispBackend]
                |           [CUDABackend] ...
                |
   [vm.generic-tensor] [vm.nodes]

</code></pre>
<pre><code>:cl-waffe2/base-impl
</code></pre>
<p>Using the basic system of cl-waffe2, <code>:cl-waffe2/vm.generic-tensor</code> and <code>:cl-waffe2/vm.nodes</code>, the package <code>:cl-waffe2/base-impl</code> provides a standard implementation of matrix (sometimes scalar) tensor operations. The operation we say is including: <code>defun</code> parts, and abstract definition of operation.</p>
<p>Before we go any futher: cl-waffe2 is working on <code>AbstractTensor</code> (inspired in Julia's great idea, <code>AbstractArray</code>), which separates <strong>implementation</strong> of the operation from the
<strong>definition.</strong> In that respect, <code>:cl-waffe2/base-impl</code> provides the <strong>definition</strong> of operations, while the packages we about to mention provides <strong>implementation</strong> of operations.</p>
<h3 id="standard-backendsimplemenetations">Standard Backends/Implemenetations</h3>
<p>As of this writing (2023/07/05), we provide two standard implementation of <code>:cl-waffe2/base-impl</code>, both of them are working on CPU.</p>
<pre><code>:cl-waffe2/backends.lisp
:cl-waffe2/backends.cpu
</code></pre>
<p>If only time and money would permit, I'm willing to implement CUDA/Metal Backends.</p>
<p>:cl-waffe2/backends.lisp is <code>work enough</code> first, it is Portable (based on ANSI Common Lisp) and supports AVX2 but far from <code>full speed</code>.</p>
<p>On the other hand :cl-waffe2/backends.cpu is accelerated by OpenBLAS (maybe MKL is ok) and other foreign backends, this is SBCL-Dependant and sometimes could be unsafe, but provides <code>full speed</code>.</p>
<p>(P.S: <code>:cl-waffe2/backends.jit.lisp</code> is now partially available (not tested all), it is still unstable but demonstrates how to extend the JIT compiler on other backends in cl-waffe2.)</p>
<pre><code>:cl-waffe2/backends.fastmath (NOT IMPLEMENTED YET!)
</code></pre>
<p>(TO BE) Supporting vectorized mathematical functions, AVX512 instructions.</p>
<h3 id="neural-network">Neural Network</h3>
<pre><code class="language-lisp">:cl-waffe2/nn ;; Provides Basic neural-network Implementations.
:cl-waffe2/optimizers ;; Provides Basic Optimizers
</code></pre>
<h3 id="utils">Utils</h3>
<pre><code class="language-lisp">:cl-waffe2     ;; Provides multi-threading APIs and config macros!
:cl-waffe2/viz ;; Provides Vizualizing APIs of computation node
etc...
</code></pre>
<h3 id="to-get-started">To Get Started!</h3>
<p>If you're going to start with defining a new package, It is recommended to <code>:use</code> the package to be used.</p>
<p>Read the description above and select and describe the packages you think you need. (or you can just copy and paste it.)</p>
<p>The lisp code below demonstrates an example case of <code>:your-project-name</code> package.</p>
<pre><code class="language-lisp">
(in-package :cl-user)

(defpackage :your-project-name
    (:use :cl
          :cl-waffe2
      :cl-waffe2/vm.generic-tensor
      :cl-waffe2/vm.nodes
      :cl-waffe2/base-impl
      :cl-waffe2/distributions
      :cl-waffe2/backends.lisp
      :cl-waffe2/backends.cpu
      :cl-waffe2/nn
      :cl-waffe2/optimizers
      :cl-waffe2/viz))

(in-package :your-project-name)

;; Your code follows...

</code></pre>
<p>If you're working with REPL (or new to Common Lisp?), you can try cl-waffe2 features like this:</p>
<pre><code class="language-sh">$ ros run
&gt; (load &quot;cl-waffe2.asd&quot;) # cl-waffe2.asd should be placed where SBCL can read it.
&gt; (ql:quickload :cl-waffe2)
&gt; (in-package :cl-waffe2-repl) ;; this is a playground place, and all features are available
</code></pre>
<p>The tutorials below should be also working on REPL, (indeed, cl-waffe2 is REPL-friendly!), you can learn how cl-waffe2 works by copying and pasting the example codes.</p>
<h2 id="basic-building-computation-nodes-lazily">Basic: Building Computation Nodes Lazily</h2>
<p>Since <code>Do not run until the node is optimized</code> is a one of cl-waffe2 policy, all operations in cl-waffe2 is lazy evaluation unless defined by a special macro.</p>
<p>Therefore, calling <code>!add</code> function which finds a sum of given arguments, the retuend tensor isn't still computed, only setting <code>:vec-state</code> = <code>[maybe-not-computed]</code>.</p>
<pre><code class="language-lisp">(!add 3.0 2.0)
</code></pre>
<pre><code class="language-lisp">{SCALARTENSOR[float]  :named ChainTMP23305 
  :vec-state [maybe-not-computed]
  &lt;&lt;Not-Embodied (1) Tensor&gt;&gt;
  :facet :input
  :requires-grad NIL
  :backward &lt;Node: SCALARANDSCALARADD-SCALARTENSOR (A[SCAL] B[SCAL] -&gt; A[SCAL]
                                                    WHERE SCAL = 1)&gt;}
</code></pre>
<p><code>:vec-state</code> indicates the computation state of tensor, and it says exactly what it says.</p>
<p>You can continue the operation by connecting the returned tensor and next operation.</p>
<p>For example, the figure below in cl-waffe is represented as:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>o</mi><mi>u</mi><mi>t</mi><mo>=</mo><mn>3</mn><mo>+</mo><mn>2</mn><mo>∗</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">
out = 3 + 2 * 4
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">4</span></span></span></span></span></p>
<pre><code class="language-lisp">(defparameter out (!add 3 (!mul 2 4))) ;; out &lt;- 3 + 2 * 4
</code></pre>
<p>To obtain the state in which the operation is performed, calling the function <code>(proceed toplevel)</code> is a vaild option.</p>
<pre><code class="language-lisp">(proceed out)

{SCALARTENSOR[int32]  :named ChainTMP28326 
  :vec-state [computed]
    11
  :facet :input
  :requires-grad NIL
  :backward &lt;Node: PROCEEDNODE-T (A[~] -&gt; A[~])&gt;}
</code></pre>
<p><code>proceed</code> is a differentiable operation which instantly compiles and executes all the previous node of <code>toplevel</code>. In addition, there's another way to accept nodes: <code>(build out)</code> or <code>(define-composite-function)</code>, but they're a little complicated, so explained in the other sections.</p>
<p>The moment compiling function is called, cl-waffe2 prunes all unused copying, computes all <code>View Offsets</code>, schedules memory allocation and (Currently it's not working though) multi-threading.</p>
<h2 id="abstracttensor-one-operation-multiple-implementations">AbstractTensor - One operation, Multiple implementations.</h2>
<h3 id="background">Background</h3>
<pre><code class="language-lisp">(Operations in cl-waffe2)

                           [AbstractNode]
                                 |
            |--------------------|------------------------|
 [CPU Implementation1] [CPU Implementation2] [CUDA Implementation1] ...
</code></pre>
<p><code>Julia</code> has introduced <a href="https://docs.julialang.org/en/v1/base/arrays/">AbstractArray</a> in their libraries, separating the common (generic) parts of the array from each backend implementation. Since <code>AbstractTensor</code> increased portability between devices on which they run (even on CPU!), cl-waffe2 wholly introduced this feature.</p>
<p>In cl-waffe2, The generic definition of operations, <code>AbstractNode</code> is a class declared via the <code>defnode</code> macro, and depending on the devices we're working on, the <code>define-impl</code> macro defines an implementation.</p>
<p>Conveniently, there can be more than one implementation for a single device. (e.g.: it is possible to have a normal implementation and an approximate implementation for the exp function on single CPU).</p>
<p>One of the policy is to minimise code re-writing by defining abstract nodes and switching the backends that executes them depending on the device they run on and the speed required.</p>
<h3 id="example-addnode">Example: AddNode</h3>
<p>Here's an example of how I've implemented the operation <code>!add</code>.</p>
<p><code>AddNode-Revisit</code> is <code>AbstractNode</code> of finding the sum of two given matrices A and B and storing the result in A. Here's the segment from the source code.</p>
<pre><code class="language-lisp">;; Reimplementation of AddNode
(defnode (AddNode-Revisit (myself dtype)
            :where (A[~] B[~] -&gt; A[~])
            :documentation &quot;A &lt;- A + B&quot;
            :backward ((self dout dx dy)
                       (declare (ignore dx dy))
                       (values dout dout))))
</code></pre>
<p><code>AbstractNode</code> is a CLOS class with the following operation.</p>
<ol>
<li>shape changes before and after the operation, and which pointer to use? is described in <code>:where</code>. Before <code>-&gt;</code> clause refers to the arguments, after <code>-&gt;</code> clause refers to the shape of matrix after the operation.</li>
</ol>
<p>It says:</p>
<ol>
<li>Takes A and B as arguments and returns a matrix of pointers of A</li>
<li>All matrices have the same shape before and after the operation.</li>
</ol>
<p>Also, <code>:backward</code> defines the operation of backward. This declaration can be made either in <code>defnode</code> or in <code>define-impl</code>, whichever you declare.</p>
<p>The declared node can be initialized using the function <code>(AddNode-Revisit dtype)</code>, but seems returning errors.</p>
<pre><code class="language-lisp">(AddNode-Revisit :float)
;; -&gt; Couldn't find any implementation of AddNode for (CPUTENSOR LISPTENSOR).
</code></pre>
<p>This is because there is not yet a single implementation for <code>AddNode-Revisit</code>.</p>
<p>One operation can be defined for a backend that can be declared by extending the <code>cl-waffe2/vm.generic-tensor:AbstractTensor</code> class. Here's <code>LispTensor</code>, and <code>CPUTensor</code>, and of course, if necessary, you can create a new backend <code>MyTensor</code> by just copying them:</p>
<pre><code class="language-lisp">;; 1. Creating from AbstratTensor.
(defclass MyTensor (AbstractTensor) nil)

;; Initializer/Allocator
(defmethod initialize-instance :before ((tensor MyTensor)
                    &amp;rest initargs
                    &amp;key &amp;allow-other-keys)
  ;; if projected-p -&gt; alloc new vec
  (let* ((shape (getf initargs :shape))
     (dtype (dtype-&gt;lisp-type (getf initargs :dtype)))
     (vec   (getf initargs :vec))
     (facet (getf initargs :facet))
     (initial-element (coerce (or (getf initargs :initial-element) 0) dtype)))
    (when (eql facet :exist)
      (if vec
      (setf (tensor-vec tensor) vec)
      (setf (tensor-vec tensor)
        (make-array
         (apply #'* shape)
         :element-type dtype
         :initial-element initial-element))))))

;; If data storage is differ from CL Array, override vref and (setf vref) method.


;; 2. Using an existing backend.


;; If you want to use a backend that is already implemented, the following line of code is sufficient.

(defclass MyTensor (CPUTensor) nil) ;; Adding a new backend is all done in this code!
</code></pre>
<p>(See also: <a href="https://github.com/hikettei/cl-waffe2/blob/master/source/backends/lisp/tensor.lisp">tensor.lisp</a>)</p>
<p>The devices to use can be switched <code>with-devices</code> macro.</p>
<pre><code class="language-lisp">(with-devices (MyTensor LispTensor) ;; The further to the left, the higher the priority.
    (make-tensor `(10 10)))

;; -&gt; MyTensor is created
{MYTENSOR[float] :shape (10 10)  
  ((0.0 0.0 0.0 ~ 0.0 0.0 0.0)           
   (0.0 0.0 0.0 ~ 0.0 0.0 0.0)   
        ...
   (0.0 0.0 0.0 ~ 0.0 0.0 0.0)
   (0.0 0.0 0.0 ~ 0.0 0.0 0.0))
  :facet :exist
  :requires-grad NIL
  :backward NIL}
</code></pre>
<p><code>MyTensor</code> has no implementation of any operations, but the code below is working.</p>
<pre><code class="language-lisp">(with-devices (MyTensor LispTensor)
    (proceed (!add (randn `(3 3)) (randn `(3 3)))))

{MYTENSOR[float] :shape (3 3) :named ChainTMP28398 
  :vec-state [computed]
  ((-1.4494231  1.0320233   -1.8852448)
   (1.0886636   -0.37185743 0.99227524)
   (2.2778857   -0.82929707 2.3525782))
  :facet :input
  :requires-grad NIL
  :backward &lt;Node: PROCEEDNODE-T (A[~] -&gt; A[~])&gt;}
</code></pre>
<p>This is because <code>MyTensor</code> and <code>LispTensor</code> are pointer compatible, and <code>AddNode</code> for <code>LispTensor</code> is used instead of undefined implementation, <code>AddNode</code> for <code>MyTensor</code>.</p>
<p>Therefore, after defining a new backend, it is <strong>NOT</strong> necessary to give a re-implementation for all standard implementations in cl-waffe2. Select the appropriate backends in order of array compatibility.</p>
<p>The macro <code>define-impl</code> adds a new implementation of <code>device</code>.</p>
<pre><code class="language-lisp">;; The code below is NOT working on REPL, but working in :cl-waffe2/backends.lisp package

(define-impl (AddNode-Revisit :device MyTensor)
         :forward ((self x y)
               (let ((adder (matrix-add (dtype x))))
             `(,@(call-with-view
                  #'(lambda (x-view
                     y-view)
                  `(funcall ,adder
                        (tensor-vec ,x)
                        (tensor-vec ,y)
                        ,(offset-of x-view 0)
                        ,(offset-of y-view 0)
                        ,(size-of x-view 0)
                        ,(stride-of x-view 0)
                        ,(stride-of y-view 0)))
                  `(,x ,y))
               ,x))))
</code></pre>
<p>In <code>:forward</code>, write the expansion expression for the operation in the same way as when defining a macro with <code>defmacro</code>. The <code>call-with-view</code> function is a general-purpose function to iterate the given tensor with computing offsets.</p>
<p>(P.S.: I believe that ideas on this macro needed to be given more thoughts, indeed, this is ugly... but I guess <code>composite-function</code> can be install without writing macros, not tested.)</p>
<p>The forward definition of node can be called with <code>(forward node &amp;rest inputs)</code> function.</p>
<pre><code class="language-lisp">(forward (AddNode :float) (randn `(10 10)) (randn `(10 10)))
{CPUTENSOR[float] :shape (10 10) :named ChainTMP28407 
  :vec-state [maybe-not-computed]
  ((-0.93102205  -0.25396287  0.45237574   ~ 0.54063225   0.56266963   -0.77444124)                    
   (-0.55870235  -0.9794068   -0.21233901  ~ 1.1901267    -0.83241004  -0.69876736)   
                 ...
   (-0.5366255   -0.9118863   1.274197     ~ 0.19851275   0.21501832   1.064277)
   (-0.65124494  0.15393624   -0.6625119   ~ -1.1875637   -2.007647    0.5431197))
  :facet :input
  :requires-grad NIL
  :backward &lt;Node: ADDNODE-CPUTENSOR (A[~] B[~] -&gt; A[~])&gt;}
</code></pre>
<p>Closely Looking at :vec-state, it says the operation isn't done yet. The embodied elements are displayed but this is because <code>AddNode</code> is defined as in-place operation, returning the first argument.</p>
<p>To accept this state instantly, we can use <code>proceed</code>.</p>
<pre><code class="language-lisp">(proceed (forward (AddNode :float) (randn `(10 10)) (randn `(10 10))) :measure-time t)
Proceed-Time: First Trying
Evaluation took:
  0.000 seconds of real time
  0.000028 seconds of total run time (0.000019 user, 0.000009 system)
  100.00% CPU
  26,990 processor cycles
  0 bytes consed

Proceed-Time: Second Trying
Evaluation took:
  0.000 seconds of real time
  0.000003 seconds of total run time (0.000003 user, 0.000000 system)
  100.00% CPU
  6,300 processor cycles
  0 bytes consed

{CPUTENSOR[float] :shape (10 10) :named ChainTMP28477 
  :vec-state [computed]
  ((2.843876    2.3477855   3.3252454   ~ -1.0901415  -1.211004   -2.268893)                   
   (-2.7236757  -0.60536575 -0.61465085 ~ 2.383132    -0.22351071 -0.6449351)   
                ...
   (-0.7634125  0.7340392   2.7052975   ~ 1.1768849   3.609434    -1.3465445)
   (4.1204114   3.696868    -2.1895533  ~ -1.5550013  2.6361299   0.31319892))
  :facet :input
  :requires-grad NIL
  :backward &lt;Node: PROCEEDNODE-T (A[~] -&gt; A[~])&gt;}
</code></pre>
<h2 id="compiling-and-in-place-optimizing">Compiling, and In-place optimizing</h2>
<h3 id="compiled-model">Compiled Model</h3>
<p>Compiling Common Lisp Code at runtime is certainly fast, but isn't enough. In order to re-use compiled nodes, there is <code>Compiled-Composite</code> class to manage the state.</p>
<p><code>Compiled-Composite</code> can be obtained by calling <code>(build toplevel)</code></p>
<pre><code class="language-lisp">(let* ((out (!sum (!add (randn `(10 10)) (randn `(10 10)))))
       (compiled-model (build out)))
    compiled-model)

&lt;Compiled-Composite
    forward:  #&lt;FUNCTION (LAMBDA ()) {53D7ED1B}&gt;
    backward: #&lt;FUNCTION (LAMBDA ()) {53D4D78B}&gt;

+= [Tensors in the computation node] =======+

Subscripts:


Variables:
 NAMES |  SIZE | 


 - The number of tmp variables : 15
 - The number of parameters    : 0
+========================================+
&gt;
</code></pre>
<p><code>(forward compiled-composite)</code>, <code>(backward compiled-composite)</code> calls forward/backward functions respectively.</p>
<pre><code class="language-lisp">(let* ((out (!sum (!add (randn `(10 10)) (randn `(10 10)))))
       (compiled-model (build out)))
     (print (forward compiled-model))
     (print (backward compiled-model)))

{CPUTENSOR[float] :shape (1 1) -&gt; :view (&lt;0&gt; &lt;0&gt;) -&gt; :visible-shape (1 1) :named ChainTMP29625 
  ((-24.876368))
  :facet :input
  :requires-grad NIL
  :backward NIL} 
T 
</code></pre>
<p>But what if one wants to change the value of first argument? Replace <code>(make-tensor)</code> to be replaced later with a <code>(make-input)</code> function.</p>
<pre><code class="language-lisp">(make-input shape input-name)
</code></pre>
<p><code>shape</code> can include symbols, to be determined later.</p>
<pre><code class="language-lisp">(let* ((out (!sum (!add (make-input `(a b) :InputA) (randn `(10 10)))))
       (compiled-model (build out)))
     compiled-model)

&lt;Compiled-Composite
    forward:  #&lt;FUNCTION (LAMBDA ()) {53D9AF7B}&gt;
    backward: #&lt;FUNCTION (LAMBDA ()) {53D9D53B}&gt;

+= [Tensors in the computation node] =======+

Subscripts:
     [A -&gt; ?, max=?]
     [B -&gt; ?, max=?]


Variables:
 NAMES  |  SIZE  | 
––––––––––––––––––
 INPUTA |  (A B) | 


 - The number of tmp variables : 15
 - The number of parameters    : 0
+========================================+
&gt;
</code></pre>
<p>The function <code>(make-input)</code> itself, doesn't have a vector storage. (as long as <code>(tensor-vec tensor)</code> function isn't called). Accordingly, someone has to <strong>embody</strong> the storage of InputTensor with <code>ExistTensor</code>.</p>
<p><code>(set-input compiled-composite input-name actual-tensor)</code> embodies given InputTensor in the computation node with actual-tensor.</p>
<pre><code class="language-lisp">(let* ((out (!sum (!add (make-input `(a b) :InputA) (randn `(10 10)))))
       (compiled-model (build out)))

    (set-input compiled-model :InputA (randn `(10 10)))
    (print (forward compiled-model))
    (print (backward compiled-model))

    (set-input compiled-model :InputA (randn `(10 10)))
    ;; ... working on another input
    )

{CPUTENSOR[float] :shape (1 1) -&gt; :view (&lt;0&gt; &lt;0&gt;) -&gt; :visible-shape (1 1) :named ChainTMP29804 
  ((17.631124))
  :facet :input
  :requires-grad NIL
  :backward NIL} 
T 
</code></pre>
<h3 id="in-place-optimizing">In-place optimizing</h3>
<p>This is a usual function in cl-waffe2, which finds the sum of A and B.</p>
<pre><code class="language-lisp">(!add a b)
</code></pre>
<p>However, this is how <code>!add</code> is defined internally. This makes a copy twice times not to make side effects.</p>
<pre><code class="language-lisp">;; In source/base-impl/arithmetic.lisp

(forward (AddNode dtype) (!copy a) (!copy b))
</code></pre>
<p>Without copying, the content of <code>a</code> is overwritten:</p>
<pre><code class="language-lisp">(let ((a (make-tensor `(3 3) :initial-element 1.0)))
      (print a)
      ;; {CPUTENSOR[float] :shape (3 3)  
      ;;  ((1.0 1.0 1.0)
      ;;   (1.0 1.0 1.0)
      ;;   (1.0 1.0 1.0))
      ;;  :facet :exist
      ;;  :requires-grad NIL
      ;;  :backward NIL}
      ;; (eval A &lt;- A + B)
      (proceed (forward (AddNode :float) a (randn `(3 3))))
      (print a)
      ;; {CPUTENSOR[float] :shape (3 3)  
      ;;  ((2.0100088   0.2906983   1.5334041)
      ;;   (-0.50357413 2.389317    0.7051847)
      ;;   (1.3005692   1.5925546   0.95498145))
      ;;   :facet :exist
      ;;   :requires-grad NIL
      ;;   :backward NIL} )
</code></pre>
<p>To put it bluntly, it is natural to think this copy is just a waste of memory. However, In this case, disabling <code>!copy</code> is a rational way to optimize the performance of the program. (i.e.: replace with in-place operation).</p>
<p>Owing to lazy evaluation of cl-waffe2, unnecessary <code>(!copy)</code> operation can be deleted automatically by checking the number of tensor references in a node.</p>
<p>Let f(x) be a operation defined as:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mi>M</mi><mi>a</mi><mi>y</mi><mi>b</mi><mi>e</mi><mi>C</mi><mi>o</mi><mi>p</mi><mi>y</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">
f(x) = sin(MaybeCopy(x))
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">in</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord mathnormal">b</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">o</span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span></span></span></p>
<p>Let the computation node be below:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>o</mi><mi>u</mi><mi>t</mi><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>I</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi><mo stretchy="false">)</mo><mo>+</mo><mi>f</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>T</mi><mi>e</mi><mi>n</mi><mi>s</mi><mi>o</mi><mi>r</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">
out = f(Input) + f(f(Tensor))
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mord mathnormal">n</span><span class="mord mathnormal">p</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal" style="margin-right:0.02778em;">sor</span><span class="mclose">))</span></span></span></span></span></p>
<p>Formulating the same network in cl-waffe2:</p>
<pre><code class="language-lisp">;; (Let me define the utilities to be used in defnode in advance)

;; Tips:
;; Obtain function of :lazy-evaluation -&gt; immediate execution.

(defmodel (SinModel (self)
             :where  (X[~] -&gt; [~])
             :on-call-&gt; ((self x)
                         (declare (ignore self))
                         (!sin x))))

(define-composite-function (SinModel) !sin-static :dtype :float)

;; (!sin-static (randn `(10 10))) is instantly executed. not lazy-evaluated.
</code></pre>
<pre><code class="language-lisp">;; Basic Units in the network:

;; General Definition of f(x)
(defnode (F-Node (self)
          :documentation &quot;f(x) = sin(x)&quot;
          :where (A[~] -&gt; A[~])))

;; Implementation of f(x)
;; Setting :device = t, -&gt; the impl is working on all devices.

(define-impl (F-Node :device t)
         :forward ((self x) `(!sin-static ,x))
         :backward ((self dout dx) (values (!mul dx (!cos dout)))))

;; The caller of f(x)

(defun !f (x)
    (forward (F-Node) (!copy x)))
</code></pre>
<p>Through <code>:cl-waffe2/viz</code> package, we can visualize how the operation is performed.</p>
<pre><code class="language-lisp">;; (make-input ... nil): creates a caching tensor, being the elements of it isn't guaranteed to be 0.0.
(let ((k (!add (make-input `(3 3) nil)
               (!f (!f (randn `(3 3) :requires-grad t))))))
        (cl-waffe2/viz:viz-computation-node k &quot;assets/bad_node.dot&quot;)
        (build k) ;; optimized
           (cl-waffe2/viz:viz-computation-node k &quot;assets/opt_node.dot&quot;))
</code></pre>
<p>The result is written in <code>dot language</code>.</p>
<pre><code class="language-sh">$ dot -Tpng ./assets/bad_node.dot &gt; ./assets/bad_node.png
$ dot -Tpng ./assets/opt_node.dot &gt; ./assets/opt_node.png
</code></pre>
<h3 id="before-optimized-vs-after-optimized">Before Optimized Vs After Optimized.</h3>
<p><img alt="bf" src="https://github.com/hikettei/cl-waffe2/blob/master/docs/cl-waffe2-docs/docs/assets/bad_node.png?raw=true" width="45%">
<img alt="bf" src="https://github.com/hikettei/cl-waffe2/blob/master/docs/cl-waffe2-docs/docs/assets/opt_node.png?raw=true" width="45%"></p>
<p><code>ExistTensor</code> (created by <code>make-tensor</code>, or tensors whose requires-grad=t) is never overwritten.</p>
<h2 id="network-units-node-and-composite">Network Units: Node and Composite</h2>
<p>In this section, we learn the two key units, <code>Node</code> and <code>Composite</code>, to construct neural networks in cl-waffe2.</p>
<h3 id="node-and-composite">Node and Composite</h3>
<p><code>Node(AbstractNode)</code> is the smallest unit of operation with forward and backward propagation. Its abstract definition is defined by a <code>defnode</code> macro, and It is implemented by a <code>(define-impl)</code> macro. The defined node is invoked by <code>(forward node &amp;rest inputs)</code> function, at the same time, computation nodes are constructed.</p>
<pre><code class="language-lisp">(defnode (SinNode-Revisit (self)
            :where (X[~] -&gt; X[~])
        :save-for-backward (t)
        :backward ((self dout x)
                   (values (!mul dout (!cos x))))))

(define-impl (SinNode-Revisit :device t)
       :forward ((self x) `(!sin-static ,x)))


(forward (SinNode-Revisit) (randn `(10 10)))

{CPUTENSOR[float] :shape (10 10) :named ChainTMP32968 
  :vec-state [maybe-not-computed]
  &lt;&lt;Not-Embodied (10 10) Tensor&gt;&gt;
  :facet :input
  :requires-grad NIL
  :backward &lt;Node: SINNODE-REVISIT-T (X[~] -&gt; X[~])&gt;}

(proceed *)

{CPUTENSOR[float] :shape (10 10) :named ChainTMP32955 
  :vec-state [computed]
  ((0.43090457   -0.24942507  -0.99978673  ~ 0.97256666   -0.9993819   0.37133723)                    
   (0.050297778  -0.048203766 0.11011651   ~ -0.28100008  -0.89788723  0.12841338)   
                 ...
   (0.32419643   0.15791988   -0.95573443  ~ 0.079026684  -0.4924342   0.99993217)
   (-0.04615228  -0.2262427   -0.6637178   ~ 0.8855889    -0.72787035  -0.65471023))
  :facet :input
  :requires-grad NIL
  :backward &lt;Node: PROCEEDNODE-T (A[~] -&gt; A[~])&gt;}
</code></pre>
<p>On the other hand, <code>Composite</code> is a unit made up of several <code>Nodes</code>, defined by a <code>defmodel</code> macro. <code>(call model &amp;rest inputs)</code> method invokes the <code>on-call-&gt;</code> form lazily, being compiled in the same way as nodes. Moreover, the defined <code>Composite</code> also can define a function for immeditate function by using the macro, <code>define-composite-function</code>. The behaviour is similar to <code>TorchScript</code>, cl-waffe2 traces the computation node, calling <code>(build toplevel)</code> and defines a <code>Composite-function</code>.</p>
<pre><code class="language-lisp">(defmodel (Softmax-Model (self)
       :where (X[~] -&gt; [~]) ;; :where for Composite is optional!
       :on-call-&gt; ((self x)
               (declare (ignore self))
               (let* ((x1 (!sub x (!mean x  :axis 1 :keepdims t)))
                          (z  (!sum   (!exp x1) :axis 1 :keepdims t)))
                           (!div (!exp x1) z)))))

;; won't be evaluated until proceed/build is called.
(call (Softmax-Model) (randn `(10 10))

(proceed *)


{CPUTENSOR[float] :shape (10 10) :named ChainTMP6184 
  :vec-state [computed]
  ((0.29810402  0.11953584  0.16032213  ~ 0.033787794 0.01729085  0.03808046)                   
   (0.032921903 0.085420445 0.10371924  ~ 0.06863596  0.10435363  0.07114864)   
                ...
   (0.23044951  0.14320189  0.16871664  ~ 0.019123536 0.03614414  0.10644407)
   (0.0377036   0.034945846 0.28327137  ~ 0.07359542  0.40399343  0.020138593))
  :facet :input
  :requires-grad NIL
  :backward &lt;Node: PROCEEDNODE-T (A[~] -&gt; A[~])&gt;}

;; Works at toplevel
(define-composite-function (Softmax-Model) !softmax-static)

;; No overheads of compiling, but there's a little overhead to dispatch the method.
(time (!softmax-static (randn `(10 10))))
Evaluation took:
  0.000 seconds of real time
  0.000301 seconds of total run time (0.000253 user, 0.000048 system)
  100.00% CPU
  691,808 processor cycles
  32,496 bytes consed

{CPUTENSOR[float] :shape (10 10) :named ChainTMP6195 
  ((0.042827643  0.13156936   0.06729175   ~ 0.059296332  0.17645036   0.04613843)                    
   (0.32095885   0.030778391  0.091331415  ~ 0.09311637   0.28322798   0.040707175)   
                 ...
   (0.045369238  0.045168925  0.12002338   ~ 0.2656273    0.01337298   0.41475114)
   (0.020064427  0.01839381   0.013036524  ~ 0.20158055   0.3377756    0.061546378))
  :facet :input
  :requires-grad NIL
  :backward NIL}
</code></pre>
<h3 id="proceed-vs-composite-function">Proceed vs Composite-function</h3>
<p>Compared to <code>Proceed</code>, <code>Composite-function</code> and codes which consisted of it have a small overhead in calling a function, but it becomes negligible as the matrix size increases.</p>
<pre><code class="language-lisp">;; Proceed
;; 1 * 1 Matrix
(let ((a (ax+b `(1 1) 0 1)))
    (proceed-time (!sin a)))
Proceed-Time: First Trying
Evaluation took:
  0.000 seconds of real time
  0.000077 seconds of total run time (0.000054 user, 0.000023 system)
  100.00% CPU
  141,818 processor cycles
  0 bytes consed

Proceed-Time: Second Trying
Evaluation took:
  0.000 seconds of real time
  0.000007 seconds of total run time (0.000006 user, 0.000001 system)
  100.00% CPU
  11,790 processor cycles
  0 bytes consed

;; Proceed
;; 1000 * 1000 Matrix

(let ((a (ax+b `(1000 1000) 0 1)))
    (proceed-time (!sin a)))
Proceed-Time: First Trying
Evaluation took:
  0.019 seconds of real time
  0.019118 seconds of total run time (0.017191 user, 0.001927 system)
  100.00% CPU
  44,118,196 processor cycles
  8,000,032 bytes consed

Proceed-Time: Second Trying
Evaluation took:
  0.015 seconds of real time
  0.015628 seconds of total run time (0.015613 user, 0.000015 system)
  106.67% CPU
  36,025,586 processor cycles
  0 bytes consed
</code></pre>
<pre><code class="language-lisp">;; Composite-Function
;; 1 * 1 Matrix
(let ((a (ax+b `(1 1) 0 1)))
     (time (!sin-inline a)))
Evaluation took:
  0.000 seconds of real time
  0.000103 seconds of total run time (0.000098 user, 0.000005 system)
  100.00% CPU
  231,840 processor cycles
  0 bytes consed

;; Composite-Function
;; 1000 * 1000 Matrix
(let ((a (ax+b `(1000 1000) 0 1)))
     (time (!sin-inline a)))
Evaluation took:
  0.015 seconds of real time
  0.015862 seconds of total run time (0.015813 user, 0.000049 system)
  106.67% CPU
  36,632,326 processor cycles
  0 bytes consed
</code></pre>
<p>(Tips: the <code>call</code> method is designed to invoke <code>Composite</code>, but it is also applicatable into <code>AbstractNode</code>, that is, <code>call</code> is a general-purpose method to invoke nodes.)</p>
<h3 id="sequence-model">Sequence Model</h3>
<p>Since the shape of matrices is declared everywhere operation, cl-waffe2 can trace the structure of neural networks lazily, and being checked before the execution.</p>
<p>In the code below, <code>defsequence</code> is a macro to define <code>Composite</code> sequentially, <code>(asnode function)</code> is a macro which coerce function into <code>Composite</code>.</p>
<pre><code class="language-lisp">(defsequence MLP-Sequence (in-features hidden-dim out-features
               &amp;key (activation #'!tanh))
         &quot;3 Layers MLP&quot;
         (LinearLayer in-features hidden-dim)
         (asnode activation)
         (LinearLayer hidden-dim hidden-dim)
         (asnode activation)
         (LinearLayer hidden-dim out-features)
         (asnode #'!softmax))
</code></pre>
<p>All composites/nodes that used to define <code>MLP-Sequence</code> has also a definition of shape.</p>
<pre><code class="language-lisp">(MLP-Sequence 784 512 256)

&lt;Composite: MLP-SEQUENCE{W23852}(
    &lt;&lt;6 Layers Sequence&gt;&gt;

[1/6]          ↓ 
&lt;Composite: LINEARLAYER{W23682}(
    &lt;Input : ((~ BATCH-SIZE 784)) -&gt; Output: ((~ BATCH-SIZE 512))&gt;

    WEIGHTS -&gt; (512 784)
    BIAS    -&gt; (512)
)&gt;
[2/6]          ↓ 
&lt;Composite: ENCAPSULATED-NODE{W23680}(
    #&lt;FUNCTION !TANH&gt;
)&gt;
[3/6]          ↓ 
&lt;Composite: LINEARLAYER{W23510}(
    &lt;Input : ((~ BATCH-SIZE 512)) -&gt; Output: ((~ BATCH-SIZE 512))&gt;

    WEIGHTS -&gt; (512 512)
    BIAS    -&gt; (512)
)&gt;
[4/6]          ↓ 
&lt;Composite: ENCAPSULATED-NODE{W23508}(
    #&lt;FUNCTION !TANH&gt;
)&gt;
[5/6]          ↓ 
&lt;Composite: LINEARLAYER{W23338}(
    &lt;Input : ((~ BATCH-SIZE 512)) -&gt; Output: ((~ BATCH-SIZE 256))&gt;

    WEIGHTS -&gt; (256 512)
    BIAS    -&gt; (256)
)&gt;
[6/6]          ↓ 
&lt;Composite: ENCAPSULATED-NODE{W23336}(
    #&lt;FUNCTION CL-WAFFE2/NN:!SOFTMAX&gt;
)&gt;)&gt;
</code></pre>
<p>Not an operation is performed, nor a matrix is allocated at the moment <code>MLP-Sequence</code> is initialized, but done when compiling/invoking the computation node.</p>
<h2 id="shaping-api-with-dsl">Shaping API with DSL</h2>
<p>See also: <a href="../nodes/#introducing-subscript-dsl">Introducing Subscript DSL</a></p>
<h2 id="view-apisbroadcasting">View APIs/Broadcasting</h2>
<p><code>%transform</code> <code>!flexible</code> for Optinal Broadcasting. <code>!view</code> for view apis.</p>
<p>See also: <a href="../base-impl/#macro-transform">%transform</a>, <a href="../base-impl/#function-flexible">!flexible</a>, and <a href="../base-impl/#function-view">!view</a></p>
<p>(TODO)</p>
<p>defoptimizer</p>
<p>deftrainer</p>
<p>parameter</p>
<p>Tutorials Over!</p>
<p>(TO ADD: ./Examples, training MNIST, Image processing, NLP etc...)</p>
<p>I'll keep my finger crossed.</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../install/" class="btn btn-neutral float-left" title="Install"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../Tips/" class="btn btn-neutral float-right" title="Tips">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../install/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../Tips/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme_extra.js" defer></script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
