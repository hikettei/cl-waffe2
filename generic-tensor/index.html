<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>cl-waffe2/vm.generic-tensor - cl-waffe2 Documentation</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "cl-waffe2/vm.generic-tensor";
        var mkdocs_page_input_path = "generic-tensor.md";
        var mkdocs_page_url = null;
      </script>
    
    <script src="../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> cl-waffe2 Documentation
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../install/">Install</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../overview/">Tutorials</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Tips/">Tips</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../utils/">cl-waffe2</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../nodes/">cl-waffe2/vm.nodes</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="./">cl-waffe2/vm.generic-tensor</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#class-abstracttensor">[class] AbstractTensor</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#creating-a-new-backend">Creating a new backend.</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#function-shape">[function] shape</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#function-dims">[function] dims</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#function-total">[function] total</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-orig-shape-list">[slot] orig-shape (List)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-stride-list">[slot] stride (list)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-visible-shape-list">[slot] visible-shape (list)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-view-list">[slot] view (list)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-projected-p-boolean">[slot] projected-p (boolean)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-scalar-p">[slot] scalar-p</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-detach-p">[slot] detach-p</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-state">[slot] state</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-variables">[slot] variables</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-tensor-id-symbol">[slot] tensor-id (symbol)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-grad-abstracttensor">[slot] grad (AbstractTensor)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-backward-abstractnode">[slot] backward (AbstractNode)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-requires-grad-boolean">[slot] requires-grad (Boolean)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-ancestor-param-p-boolean">[slot] ancestor-param-p (Boolean)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-flexible-p-boolean">[slot] flexible-p (Boolean)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-facet-keyword">[slot] facet (keyword)</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#function-tensor-vec">[function] tensor-vec</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#function-mref">[function] mref</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#generic-vref">[generic] vref</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#example">Example</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#an-form-of-tensors-in-cl-waffe2">An form of tensors in cl-waffe2</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#existtensor">ExistTensor</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#inputtensor">InputTensor</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#function-make-tensor">[function] make-tensor</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#input">Input</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#example_1">Example</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#function-make-input">[function] make-input</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#inputs">Inputs</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#example_2">Example</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#class-compiled-composite">[class] Compiled-Composite</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#examples">Examples</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#function-build">[function] build</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#the-constraints-of-toplevel-tensor">The constraints of toplevel tensor.</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#inputs_1">Inputs</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#example_3">Example</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#method-set-input">[method] set-input</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#method-get-input">[method] get-input</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#parameter-no-grad">[parameter] *no-grad*</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#macro-with-no-grad">[macro] with-no-grad</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#function-parameter">[function] parameter</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#example_4">Example</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#function-call-with-view">[function] call-with-view</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#function-shape-equal">[function] shape-equal</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#compiling-options">Compiling Options</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#dtypes">Dtypes</a>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../base-impl/">[Functions] cl-waffe2/base-impl</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../base-impl-nodes/">[Nodes] cl-waffe2/base-impl</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../distributions/">cl-waffe2/distributions</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../nn/">cl-waffe2/nn</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../optimizer/">cl-waffe2/optimizers</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">cl-waffe2 Documentation</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a> &raquo;</li>
      <li>cl-waffe2/vm.generic-tensor</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="abstracttensor">AbstractTensor</h1>
<h2 id="class-abstracttensor">[class] AbstractTensor</h2>
<p>[class] <code>AbstractTensor</code></p>
<p>AbstractTensor is a primal class for all devices. Each devices (e.g.: <code>ScalarTensor</code> <code>LispTensor</code> <code>CPUTensor</code> etc...) is a subclass of this.</p>
<p>The class provides the fundamental and necessary features for tensors.</p>
<ol>
<li>
<p>Lazy-Evaluated and Multi-Dimensional APIs, stride computations.</p>
</li>
<li>
<p><code>View APIs</code> multi-dimensional offsets</p>
</li>
<li>
<p>To construct backward, AbstractTensor records variables called with.</p>
</li>
<li>
<p><code>vec</code> container.</p>
</li>
<li>
<p>an space for saving gradients, copies for backward.</p>
</li>
<li>
<p>Lazy-Evaluated Shapings</p>
</li>
<li>
<p>Trace Informations for JIT to create well-optimized computation node.</p>
</li>
</ol>
<h3 id="creating-a-new-backend">Creating a new backend.</h3>
<p>Users can create a new backend by extending this abstract class.</p>
<pre><code class="language-lisp">(defclass MyBackend (AbstractNode) nil)
</code></pre>
<p>To use the <code>MyBackend</code> as a tensor, users also has to override these methods:</p>
<ol>
<li>
<p><code>initialize-instance</code> ... An allocator for tensor's vec.</p>
</li>
<li>
<p><code>vref</code> <code>(setf vref)</code> ... an generic function to access/write tensor's vec.</p>
</li>
</ol>
<pre><code class="language-lisp">;; TODO: Establish a common API for initargs
(defmethod initialize-instance :before ((tensor MyBackend)
                    &amp;rest initargs
                    &amp;key &amp;allow-other-keys)
  ;; if projected-p -&gt; alloc new vec
  (let* ((shape (getf initargs :shape))
     (dtype (dtype-&gt;lisp-type (getf initargs :dtype)))
     (vec   (getf initargs :vec))
     (facet (getf initargs :facet))
     (initial-element (coerce (or (getf initargs :initial-element) 0) dtype)))
    (when (eql facet :exist)
      (if vec
      (setf (tensor-vec tensor) vec)
      (setf (tensor-vec tensor) ;; vec can be anything.
        (make-array
         (apply #'* shape)
         :element-type dtype
         :initial-element initial-element))))))
</code></pre>
<pre><code class="language-lisp">(defmethod vref ((tensor MyBackend) index)
  (aref (tensor-vec tensor) index))

(defmethod (setf vref) (new-value (tensor MyBackend) index)
  (setf (aref (tensor-vec tensor) index) new-value))
</code></pre>
<p>Now, the name <code>MyBackend</code> is available as a brand-new cl-waffe2 backend!</p>
<p>Users can define a new implementation following <code>(define-impl (Name :device MyBackend) ...)</code></p>
<p>(See the examples to understand how this could be achieved at ./source/backends/lisp/tensor.lisp. or ./source/backends/cpu.)</p>
<h3 id="function-shape">[function] shape</h3>
<p>Returns a visible shape of tensor</p>
<h3 id="function-dims">[function] dims</h3>
<p>Returns the number of axes of tensor</p>
<h3 id="function-total">[function] total</h3>
<p>Returns the number of total visible elements in tensor.</p>
<h3 id="slot-orig-shape-list">[slot] orig-shape (List)</h3>
<p>the original shape of <code>vec</code>. <code>(apply #'* orig-shape)</code> must correspond with the number of total elements of <code>vec</code>.</p>
<h3 id="slot-stride-list">[slot] stride (list)</h3>
<p>An stride of tensor, can be chosen from <code>:column</code> <code>:row</code>.</p>
<p>This slot can be accessed by <code>(tensor-stride object)</code>.</p>
<h3 id="slot-visible-shape-list">[slot] visible-shape (list)</h3>
<p>An shape of visible-area of tensor, visible-area is that an viewed size of tensor.</p>
<p>Can be accessed by <code>(shape object)</code></p>
<h3 id="slot-view-list">[slot] view (list)</h3>
<p>An list of multidimensional offsets, view.</p>
<p>Can be accessed by <code>(tensor-view object)</code></p>
<h3 id="slot-projected-p-boolean">[slot] projected-p (boolean)</h3>
<p>Set t if <code>(apply #'* orig-shape) == (apply #'* visible-shape)</code> otherwise set nil.</p>
<p>If t, the tensor is produced by <code>!view</code> or <code>view</code> functions.</p>
<h3 id="slot-scalar-p">[slot] scalar-p</h3>
<p>If t, the tensor is regarded as a Scalar.</p>
<h3 id="slot-detach-p">[slot] detach-p</h3>
<p>If t, JIT compilers stop tracing at the tensor.</p>
<h3 id="slot-state">[slot] state</h3>
<p>Stores a corresponding <code>StateContainer</code>.</p>
<h3 id="slot-variables">[slot] variables</h3>
<p><code>(tensor-variables object)</code></p>
<p>Records variables called with the tensor.</p>
<h3 id="slot-tensor-id-symbol">[slot] tensor-id (symbol)</h3>
<p>Corresponding variable name that used in JIT compiler.</p>
<h3 id="slot-grad-abstracttensor">[slot] grad (AbstractTensor)</h3>
<p>If the tensor is a parameter, (i.e.: requires-grad t) and backward propagation has called, the gradients has set to this slot.</p>
<p>Reader: <code>(grad object)</code>.  Writer: <code>(set-grad object value)</code></p>
<h3 id="slot-backward-abstractnode">[slot] backward (AbstractNode)</h3>
<p>the node called with.</p>
<h3 id="slot-requires-grad-boolean">[slot] requires-grad (Boolean)</h3>
<p>If t, the tensor become a <code>parameter</code> that gradients are saved.</p>
<h3 id="slot-ancestor-param-p-boolean">[slot] ancestor-param-p (Boolean)</h3>
<p>If t, the tensor has created by <code>parameter</code> or tensors whose ancestor-param-p=t.</p>
<h3 id="slot-flexible-p-boolean">[slot] flexible-p (Boolean)</h3>
<p>If t, the tensor is <code>broadcastable</code></p>
<h3 id="slot-facet-keyword">[slot] facet (keyword)</h3>
<p>Tensors has a two state:</p>
<ol>
<li>
<p>:input</p>
</li>
<li>
<p>:exist</p>
</li>
</ol>
<p><code>:exist</code> tensor is a just normal state, which <code>vec</code> is already allocated.</p>
<p><code>:input</code> tensor is a lazy-evaluated tensor, which allocation will be done until they're really needed. (often used as a cache, or training data.)
...</p>
<h2 id="function-tensor-vec">[function] tensor-vec</h2>
<p><code>(tensor-vec tensor)</code></p>
<p>Reading the <code>vec</code> of tensor.  Not until tensor-vec is called, the new area isn't allocated.</p>
<h2 id="function-mref">[function] mref</h2>
<p><code>(mref tensor &amp;rest subscripts)</code></p>
<p>The function mref is only used to print/initialize tensors, accessing the index of subscripts <strong>with</strong> considering views..</p>
<p>If you cares about performance, dont' use <code>mref</code>, but <code>!view</code>.</p>
<p>This function is setfable.</p>
<h2 id="generic-vref">[generic] vref</h2>
<p><code>(vref tensor index)</code></p>
<p><code>vref</code> is a generic-function to access the <code>vec</code> slot of specific backends tensor, and returns <code>index</code>th element on <code>vec</code> slot <strong>without</strong> considering views.</p>
<p>If you added a new backend with having different ptr-type (can't be accessed by aref), override this method and <code>(setf vref)</code>.</p>
<h3 id="example">Example</h3>
<pre><code class="language-lisp">(defmethod vref ((tensor YourBackend) index)
    (aref (tensor-vec tensor) index))

(defmethod (setf vref) (new-value (tensor YourBackend) index)
    (setf (aref (tensor-vec tensor) index) new-value))
</code></pre>
<h2 id="an-form-of-tensors-in-cl-waffe2">An form of tensors in cl-waffe2</h2>
<p>There's a two type of tensors in cl-waffe2: <code>InputTensor</code> and <code>ExistTensor</code>, each state is called <code>facet</code> and the keyword <code>:input</code> <code>:exist</code> is dispatched respectively.</p>
<h3 id="existtensor">ExistTensor</h3>
<p><code>ExistTensor</code> means a tensor with its vec <strong>allocated</strong> in the memory, that is, the same tensor as tensors you got when create a new tensor in <code>Numpy</code>, <code>PyTorch</code> or something.</p>
<p><code>ExistTensor</code> can be created by the function <code>make-tensor</code>.</p>
<h3 id="inputtensor">InputTensor</h3>
<p>On the other hand, <code>InputTensor</code> is a tensor with its vec <strong>unallocated</strong> in the memory, in other words, this can be a <code>Lazy-Evaluated Tensor</code>.</p>
<p><code>InputTensor</code> is created by the function <code>make-input</code>, and its shape can include a symbol.</p>
<p>In the network, <code>InputTensor</code> plays a role in being caches in the operation, or being a tensor that one may want to change its content later. (e.g.: training data).</p>
<h2 id="function-make-tensor">[function] make-tensor</h2>
<pre><code>(make-tensor shape-or-scalar
           &amp;key
              (requires-grad nil)
              (dtype *default-dtype*)
              (view nil)
              (order *default-order*)
              (initial-element))
</code></pre>
<p>Refering a first-priority of  <em>using-backends</em> (i.e.: <code>car</code> of <code>*using-backends*</code>) to know what device to use, the function <code>make-tensor</code> creates and allocate a new matrix instantly.</p>
<h3 id="input">Input</h3>
<ol>
<li>
<p><code>shape-or-scalar (Any)</code> set list (consisted of fixnum) here to create a matrix, otherwise the ScalarTensor is forcibly created.</p>
</li>
<li>
<p><code>requires-grad</code> (Boolean) Set t to create gradient. (e.g.: the tensor is needed to be optimized.)</p>
</li>
<li>
<p><code>dtype</code> (keyword) Set dtype you wanna use. See also: (Dtype API)</p>
</li>
<li>
<p><code>order</code> (member :column :row)</p>
</li>
<li>
<p><code>initial-element</code> (Optional)</p>
</li>
</ol>
<h3 id="example_1">Example</h3>
<pre><code class="language-lisp">(make-tensor `(10 10) :initial-element 1.0)

{CPUTENSOR[float] :shape (10 10)  
  ((1.0 1.0 1.0 ~ 1.0 1.0 1.0)           
   (1.0 1.0 1.0 ~ 1.0 1.0 1.0)   
        ...
   (1.0 1.0 1.0 ~ 1.0 1.0 1.0)
   (1.0 1.0 1.0 ~ 1.0 1.0 1.0))
  :facet :exist
  :requires-grad NIL
  :backward NIL}
</code></pre>
<h2 id="function-make-input">[function] make-input</h2>
<p>Referring a first-priority of <code>*using-backend*</code> (i.e.: car part), the function make-input creates a InputTensor.</p>
<p>In contrast to <code>make-tensor</code>, allocation of <code>vec</code> is lazy-evaluated, and <code>shape</code> can include symbols. (Lazy-Evaluated Shape).</p>
<p>For example, whichever <code>(make-input (list 256 256 256 ... 256 256 256) nil)</code> or <code>(make-input (list 256) nil)</code> is called, the memory-usage is the same until <code>(tensor-vec tensor)</code> is called but the moment <code>(tensor-vec tensor)</code> is called, the first one would cause <code>CUDA OUT OF MEMORY</code> or something :(.</p>
<h3 id="inputs">Inputs</h3>
<p><code>Shape</code> [list] consisted of fixnum or symbol. (e.g.: <code>(a 10)</code> is OK for make-input.)</p>
<p><code>Named</code> [keyword] the name of input. If nil, the tensor is regarded as just cache. If you want to change the content of inputs later (e.g.: training data), set an appropriate name to <code>InputTensor</code> (e.g.: <code>:training-data</code> <code>:train-x</code>).</p>
<p><code>scalar-p</code> [boolean] set t is the input is scalar.</p>
<p><code>dtype</code> [keyword] as it is.</p>
<p><code>order</code> [keyword] an member of :column :row</p>
<p><code>create-from[nil or AbstractTensor]</code> If you want to extend permute state/stride information, fill it.</p>
<h3 id="example_2">Example</h3>
<pre><code class="language-lisp">(make-input `(a 10) :train-x)

{CPUTENSOR[float] :shape (A 10) :named TRAIN-X 
  &lt;&lt;Not-Embodied (A 10) Tensor&gt;&gt;
  :facet :input
  :requires-grad NIL
  :backward NIL}
</code></pre>
<p>The InputTensor named with a keyword is called <code>not-embodied tensor</code>, and can be changed its <code>vec</code> with <code>embody-input</code></p>
<h2 id="class-compiled-composite">[class] Compiled-Composite</h2>
<p>Compiled-Composite is a <code>callable</code> CLOS class, and holds compiled forward/backward function of all the computation node to all the endpoints from the top of the models' neural network. Also, this class holds information of all variables used in the node.</p>
<p>It is NOT possible to construct a computation node after Compiled-Composite, If you need this, try consider using the function <code>cl-waffe2/base-impl:proceed</code>.</p>
<p>The class will appear in your project with calling the function <code>build</code>, set the toplevel node (e.g.: the result of criterion when the task is optimizing.) to the first argument. cl-waffe2 compiler will instantly construct an lambda function of forward/backward, which is invoked by calling <code>(forward compiled-composite)</code> or <code>(backward compiled-composite)</code> method.</p>
<p>See also: <code>build</code> <code>set-input</code> <code>get-input</code>.</p>
<h3 id="examples">Examples</h3>
<p>(TODO)</p>
<h2 id="function-build">[function] build</h2>
<pre><code class="language-lisp">(build toplevel
          &amp;key
        (construct-backward? (not *no-grad*))
        (compile-mode :fastest))
</code></pre>
<p>Receiving the toplevel node in the neural network, the function <code>build</code> constructs a optimal forward/backward function, returning <code>Compiled-Composite</code>.</p>
<h3 id="the-constraints-of-toplevel-tensor">The constraints of toplevel tensor.</h3>
<p>The shape of topleve mustn't include a <code>symbol</code>.</p>
<p>For example, this cl-waffe2 operation is invaild. because the function <code>(!sin x)</code> still returns <code>(A B)</code> tensor.</p>
<pre><code class="language-lisp">(build (!sin (make-input `(A B) :Input)))
</code></pre>
<p>In order to build this operation correctly, calling <code>criterion</code> (intrinsically, <code>!sum</code> or <code>!mean</code>) is a vaild option for neural network tasks.</p>
<pre><code class="language-lisp">(build (!sum (!sin (make-input `(A B) :input)))) ;; Passes Correctly!
</code></pre>
<p>After working with adjustable shape tensor, don't forget to embody the InputTensor!</p>
<pre><code class="language-lisp">(let ((compiled-model (build (!sum (!sin (make-input `(A B) :input))))))
    (set-input compiled-model :input (randn `(10 10)))
    (forward compiled-model))
</code></pre>
<h3 id="inputs_1">Inputs</h3>
<p><code>toplevel [AbstractTensor]</code> the end of nodes. for neural network tasks, this should be scalartensor or tensors with total elements is 1, but since cl-waffe2 is intended to be applied other tasks, cl-waffe2 never produce warning while other frameworks like PyTorch will return error if <code>&lt;&lt;(10 10)Tensor&gt;&gt;.backward()</code> is invoked for example.</p>
<p><code>construct-backward?</code> [boolean] If t, the backward construction won't be done.</p>
<p><code>compile-mode</code>[compile-mode-t] an keyword to indicate compiling option.</p>
<h3 id="example_3">Example</h3>
<p><strong>REPL:</strong></p>
<pre><code class="language-lisp">&gt; (setq out (!add (randn `(10 10)) (make-input `(a 10) :X)))
</code></pre>
<pre><code>{CPUTENSOR[float] :shape (10 10) :named ChainTMP1295 
  :vec-state [maybe-not-computed]
  &lt;&lt;Not-Embodied (10 10) Tensor&gt;&gt;
  :facet :input
  :requires-grad NIL
  :backward &lt;Node: ADDNODE-CPUTENSOR (A[~] B[~] -&gt; A[~])&gt;}
</code></pre>
<p><strong>REPL:</strong></p>
<pre><code class="language-lisp">&gt; (multiple-value-list (build out))
</code></pre>
<pre><code>(&lt;Compiled-Composite
    forward:  #&lt;FUNCTION (LAMBDA ()
                           :IN
                           &quot;/Users/hikettei/.cache/common-lisp/sbcl-2.3.3-macosx-x64/Users/hikettei/Desktop/cl-waffe-workspace/progs/develop/cl-waffe2/docs/apis/generic-tensor.fasl&quot;) {53A4361B}&gt;
    backward: #&lt;FUNCTION (LAMBDA ()
                           :IN
                           &quot;/Users/hikettei/.cache/common-lisp/sbcl-2.3.3-macosx-x64/Users/hikettei/Desktop/cl-waffe-workspace/progs/develop/cl-waffe2/docs/apis/generic-tensor.fasl&quot;) {53A4161B}&gt;

+= [Tensors in the computation node] =======+

Subscripts:
     [A -&gt; ?, max=?]


Variables:
 NAMES |   SIZE  | 
––––––––––––––––––
   X   |  (A 10) | 


 - The number of tmp variables : 6
 - The number of parameters    : 0
+========================================+
&gt;
 NIL)
</code></pre>
<h2 id="method-set-input">[method] set-input</h2>
<pre><code>(set-input (model Compiled-Composite) input-name actual-value)
</code></pre>
<p>Embodies an <code>InputTensor</code> in the model. All unembodied tensors in the model can be accessed by printing the model.</p>
<p><code>input-name</code> could be a keyword indicating input-tensor, <code>actual-value</code> is a <code>AbstractTensor</code> whose facet = <code>:exist</code> (created by <code>make-tensor</code>).</p>
<h2 id="method-get-input">[method] get-input</h2>
<pre><code>(get-input (model Compiled-Composite) input-name)
</code></pre>
<p>Reading all variables in the computation node, the method get-input returns an corresponding <code>InputTensor</code> of model.</p>
<h2 id="parameter-no-grad">[parameter] <code>*no-grad*</code></h2>
<p>[parameter] <code>*no-grad*</code></p>
<p>If t, no gradients are made for backwards.</p>
<h2 id="macro-with-no-grad">[macro] with-no-grad</h2>
<pre><code class="language-lisp">(with-no-grad &amp;body body)
</code></pre>
<p>Under the <code>body</code> execution, the macro sets <code>*no-grad*</code> = <code>t</code>, that is, the built nodes are regarded as: no gradients are made for backwards.</p>
<h2 id="function-parameter">[function] parameter</h2>
<pre><code>(parameter tensor)
</code></pre>
<p>The function parameter computes all the previous nodes of the given tensor if any, returning the new tensor with <code>requires-grad=t</code>.</p>
<h3 id="example_4">Example</h3>
<pre><code class="language-lisp">(parameter (randn `(3 3)))
</code></pre>
<h2 id="function-call-with-view">[function] call-with-view</h2>
<pre><code class="language-lisp">(call-with-view function tensors &amp;key (at-least-dim 1))
</code></pre>
<p><code>call-with-view</code> is a general-purpose interface to iterate multi-dimensional tensor with considering offsets.</p>
<p>(TODO: Example/Documents)</p>
<p><code>function</code> [lambda] an lambda function which receives <code>variable1.view variable2.view ...</code> as arguments, returning an list being compiled.</p>
<p><code>tensors</code> [list of abstracttensor] tensors to be called with.
<code>at-least-dim</code> [fixnum] ... kernel-size</p>
<p>See also:</p>
<p><code>size-of</code>
<code>stride-of</code>
<code>offset-of</code>
NILNILNIL</p>
<h2 id="function-shape-equal">[function] shape-equal</h2>
<p>a=1, b=k =&gt; T
a=1, b=2 =&gt; NIL</p>
<p>...Returns subscript-t if view is Subscript otherwise returns a view</p>
<h2 id="compiling-options">Compiling Options</h2>
<p>TODO</p>
<h2 id="dtypes">Dtypes</h2>
<p>TODO</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../nodes/" class="btn btn-neutral float-left" title="cl-waffe2/vm.nodes"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../base-impl/" class="btn btn-neutral float-right" title="[Functions] cl-waffe2/base-impl">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../nodes/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../base-impl/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme_extra.js" defer></script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
