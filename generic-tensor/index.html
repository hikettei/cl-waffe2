<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>cl-waffe2/vm.generic-tensor - cl-waffe2 Documentation</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "cl-waffe2/vm.generic-tensor";
        var mkdocs_page_input_path = "generic-tensor.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> cl-waffe2 Documentation
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Overview</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../install/">Install</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../examples/">Examples</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../overview/">Learn More</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../utils/">cl-waffe2</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../nodes/">cl-waffe2/vm.nodes</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">cl-waffe2/vm.generic-tensor</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#working-with-abstracttensor">Working with AbstractTensor</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#class-abstracttensor">[class] AbstractTensor</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#function-shape">[function] shape</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#function-dims">[function] dims</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#function-total">[function] total</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-orig-shape-list">[slot] orig-shape (List)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#accessor-initial-offset-fixnum">[accessor] initial-offset (fixnum)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-stride-list">[slot] stride (list)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-visible-shape-list">[slot] visible-shape (list)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-view-list">[slot] view (list)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-projected-p-boolean">[slot] projected-p (boolean)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-scalar-p">[slot] scalar-p</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-detach-p">[slot] detach-p</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-state">[slot] state</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-variables">[slot] variables</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-tensor-id-symbol">[slot] tensor-id (symbol)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-tensor-iid-symbol">[slot] tensor-iid (symbol)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-grad-abstracttensor">[slot] grad (AbstractTensor)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-backward-abstractnode">[slot] backward (AbstractNode)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-requires-grad-boolean">[slot] requires-grad (Boolean)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-ancestor-param-p-boolean">[slot] ancestor-param-p (Boolean)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-flexible-p-fixnum-or-null">[slot] flexible-p (Fixnum or Null)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slot-facet-keyword">[slot] facet (keyword)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#method-mref">[method] mref</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#accessor-tensor-state-dict-name">[accessor] tensor-state-dict-name</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#accessor-tensor-param-belongs-to">[accessor] tensor-param-belongs-to</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#function-hook-optimizer">[function] hook-optimizer!</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#inputs">Inputs</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#function-call-optimizer">[function] call-optimizer!</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#function-reset-grad">[function] reset-grad!</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#function-tensor-vec">[function] tensor-vec</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#function-make-tensor">[function] make-tensor</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#inputs_1">Inputs</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#example">Example</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#function-make-input">[function] make-input</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#inputs_2">Inputs</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#example_1">Example</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#manipulating-gradients">Manipulating Gradients</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#parameter-no-grad">[parameter] *no-grad*</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#macro-with-no-grad">[macro] with-no-grad</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#function-parameter">[function] parameter</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#example_2">Example</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#building-functions-from-abstracttensor">Building functions from AbstractTensor</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#class-compiled-composite">[class] Compiled-Composite</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#function-build">[function] build</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#inputs_3">Inputs</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#example_3">Example</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#method-set-input">[method] set-input</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#method-get-input">[method] get-input</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#optimized-and-ranked-tensor-iterators">Optimized and Ranked Tensor Iterators</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#function-call-with-view">[function] call-with-view</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#macro-do-compiled-loop">[macro] do-compiled-loop</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#inputs_4">Inputs</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#example_4">Example</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#save-and-restore-weights">Save and Restore Weights</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#struct-state-dict">[struct] State-Dict</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#state-dict-naming-convention">State-dict naming convention</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#parsing-a-state-dict-key">Parsing a state dict key</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#slots">Slots</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#example_5">Example</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#macro-define-model-format">[macro] define-model-format</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#inputs_5">Inputs</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#examples">Examples</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#function-save-weights">[function] save-weights</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#examples_1">Examples</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#inputs_6">Inputs</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#function-load-weights">[function] load-weights</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#examples_2">Examples</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#function-load-from-state-dict">[function] load-from-state-dict</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#inputs_7">Inputs</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#returns">Returns</a>
    </li>
        </ul>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../vm/">cl-waffe2/vm</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../base-impl/">[Functions] cl-waffe2/base-impl</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../base-impl-nodes/">[Nodes] cl-waffe2/base-impl</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../distributions/">cl-waffe2/distributions</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../nn/">cl-waffe2/nn</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../optimizer/">cl-waffe2/optimizers</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../lisp-tensor-backend/">cl-waffe2/backends.lisp</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../cpu-tensor-backend/">cl-waffe2/backends.cpu</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../cpu-jit-tensor-backend/">cl-waffe2/backends.jit.cpu</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">cl-waffe2 Documentation</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">cl-waffe2/vm.generic-tensor</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="abstracttensor">AbstractTensor</h1>
<ul>
<li>AbstractTensor<ul>
<li><a href="./#working-with-abstracttensor">AbstractTensor</a></li>
<li><a href="./#manipulating-gradients">Gradients</a></li>
</ul>
</li>
<li>Compiled Objects<ul>
<li><a href="./#building-functions-from-abstracttensor">Compiled-Composite</a></li>
</ul>
</li>
<li>Iterators<ul>
<li><a href="./#optimized-and-ranked-tensor-iterators">Multiple Dimensional Offsets</a></li>
</ul>
</li>
<li>Save and restore weights<ul>
<li><a href="./#save-and-restore-weights">file format</a></li>
</ul>
</li>
</ul>
<h2 id="working-with-abstracttensor">Working with AbstractTensor</h2>
<h2 id="class-abstracttensor">[class] AbstractTensor</h2>
<p>AbstractTensor is a CLOS class that Wraps existing data structures such as matrices in an abstract class in automatic differential programming using cl-waffe2, and further adds information about computation nodes, gradients, etc.</p>
<p>Tensors can be created by the <code>make-tensor</code> function.</p>
<pre><code class="language-lisp">(make-tensor `(3 3))
</code></pre>
<p>Plus, InputTensors (lazy-evaluated tensors), which is used to delay allocation timing, to use dynamic shaping, and to store the result, can be created by the <code>make-input</code> function.</p>
<pre><code class="language-lisp">(make-input `(3 3) :A) ;; Set :A=nil to register as a temporary space.
</code></pre>
<p>As an applied use, users can create new <code>AbstractTensor</code> that inherit from AbstractTensor. In addition, inheriting existing AbstractTensors (e.g.: <code>LispTensor</code> for CL Standard Array) allows reusing descriptions such as allocations.</p>
<pre><code class="language-lisp">(defclass MyOriginalTensor (AbstractTensor) nil)
(defclass MyCPUTensor      (LispTensor) nil)
</code></pre>
<p>Declare the priority of the device to be used with the with-devices macro.</p>
<pre><code class="language-lisp">;; Higher &lt;-&gt; Lower
(with-devices (MyCPUTensor MyOriginalTensor CPUTensor)
    (make-tensor `(10 10)))
</code></pre>
<p>All available devices can be accessed with the <code>(show-backends)</code> function, and they can only be used as devices together if they are shown to have an inheritance relationship.</p>
<p>If a completely new Tensor is defined from AbstractTensor, cl-waffe2 can handle it completely in a fast form by writing the following additional information.</p>
<ul>
<li>
<p>Allocator: <code>initialize-instance :before method</code></p>
</li>
<li>
<p>Storage Accessor: <code>vref</code> and <code>(setf vref)</code> method</p>
</li>
<li>
<p>Finalizer: <code>tensor-finalizer</code> method</p>
</li>
<li>
<p>(Optional) Backend State: <code>current-backend-state</code> method</p>
</li>
<li>
<p>(Optional) a <code>cl-waffe2/vm:defpath</code> macro to enable device-specific optimization.</p>
</li>
</ul>
<p>This is the simplest case of <code>MyTensor</code> which works on CL Standard Array.</p>
<pre><code class="language-lisp">(defclass MyTensor (AbstractTensor) nil)

;; Allocators satisfy the following properties
;; 1. When facet is not `:exist`, do nothing.
;; 2. If `vec` is specified as an argument, use this, and do not allocate any tensors.
;; 3. Otherwise, allocate the tensor with:
;;     1. Dtype -&gt; :dtype
;;     2. Size  -&gt; :shape (must be 1D on the memory)
;;     3. initial-element -&gt; :initial-element
(defmethod initialize-instance :before ((tensor MyTensor)
                    &amp;rest initargs
                    &amp;key &amp;allow-other-keys)
  (let* ((shape (getf initargs :shape))
     (dtype (dtype-&gt;lisp-type (getf initargs :dtype)))
     (vec   (getf initargs :vec))
     (facet (getf initargs :facet))
     (initial-element (coerce (or (getf initargs :initial-element) 0) dtype)))
    (when (eql facet :exist)
      (if vec
      (setf (tensor-vec tensor) vec)
      (setf (tensor-vec tensor)
        (make-array
         (apply #'* shape)
         :element-type dtype
         :initial-element initial-element))))))

;; vref reads the index th element of storage vec, this is must be a setfable.
;; Leave the annoying and complicated stride/offset computations to cl-waffe2!
(defmethod vref ((tensor MyTensor) index)
  (declare (type fixnum index))
  (aref (tensor-vec tensor) index))

(defmethod (setf vref) (new-value (tensor MyTensor) index)
  (declare (type fixnum index))
  (setf (aref (tensor-vec tensor) index) new-value))


;; The method should return a lambda function, if its storage vector isn't gc-reachable.
;; Finalizers are called when quitting (with-memory-pool ...) macro.
(defmethod tensor-finalizer ((tensor MyTensor))
    ;; Returning a dummy finalizer
    #'(lambda ()))

;; The function (show-backends) will display all devices and their information
;; If you want to put something, override this method and return a string.
(defmethod current-backend-state ((backend-name (eql 'MyTensor)))
  &quot;Hello This is an demo&quot;)

;; For FusionOp and defpath macro usage, see the :cl-waffe2/vm docs.
</code></pre>
<p><code>MyTensor</code> is now recognised as a usable device, so operations can be defined using the define-impl and define-impl-op macros.</p>
<h3 id="function-shape">[function] shape</h3>
<p><code>(shape tensor)</code> returns a visible shape of the given tensor.</p>
<h3 id="function-dims">[function] dims</h3>
<p><code>(dims tensor)</code> returns a rank of the given tensor.</p>
<h3 id="function-total">[function] total</h3>
<p><code>(total tensor)</code> returns the number of total visible elements of the giventensor.</p>
<h3 id="slot-orig-shape-list">[slot] orig-shape (List)</h3>
<p>stores the shape of storage vec.</p>
<h3 id="accessor-initial-offset-fixnum">[accessor] initial-offset (fixnum)</h3>
<p>stores the offset of the tensor. In default, set to 0. Shape testing, for example, does not work, so use with caution.</p>
<p><code>(tensor-initial-offset tensor)</code></p>
<h3 id="slot-stride-list">[slot] stride (list)</h3>
<p><code>(tensor-stride tensor)</code> stores the stride of tensor.</p>
<h3 id="slot-visible-shape-list">[slot] visible-shape (list)</h3>
<p><code>(shape tensor)</code></p>
<h3 id="slot-view-list">[slot] view (list)</h3>
<p>Returns a list of ViewInstruction, created by the function <code>(view tensor ...)</code> or <code>(!view tensor ...)</code> to create a backward.</p>
<p><code>(tensor-view tensor)</code></p>
<h3 id="slot-projected-p-boolean">[slot] projected-p (boolean)</h3>
<p>Set t if <code>(apply #'* orig-shape) == (apply #'* visible-shape)</code> otherwise set nil.</p>
<p>If t, the tensor is created by <code>!view</code> or <code>view</code> functions.</p>
<h3 id="slot-scalar-p">[slot] scalar-p</h3>
<p>Set t if the tensor should be represented as a scalar. In cl-waffe2, it's not a pretty thing but scalars are represented as a <code>(apply #'* shape)=1</code> tensors. ranks are anything but for the most case, returns 1.</p>
<h3 id="slot-detach-p">[slot] detach-p</h3>
<p>Set T to detach the tensor at a certain position.</p>
<h3 id="slot-state">[slot] state</h3>
<p>(tensor-state tensor) stores <code>StateContainer</code>.</p>
<h3 id="slot-variables">[slot] variables</h3>
<p><code>(tensor-variables tensor)</code> stores the previous variables if the tensor is created by any operation.</p>
<h3 id="slot-tensor-id-symbol">[slot] tensor-id (symbol)</h3>
<p>Indicates where the Tensor is stored, (e.g. in a virtual machine). In-place operations inherit tensor-id from variables called with, and should not be used for topological sorting.</p>
<h3 id="slot-tensor-iid-symbol">[slot] tensor-iid (symbol)</h3>
<p>It holds an ID that is guaranteed to be absolutely unique to the processing system generated by gensym. Used for topological sorting.</p>
<h3 id="slot-grad-abstracttensor">[slot] grad (AbstractTensor)</h3>
<p>If the tensor is created by (parameter ...) or with <code>:requires-grad=t</code>, <code>(grad tensor)</code> will return a gradient.</p>
<h3 id="slot-backward-abstractnode">[slot] backward (AbstractNode)</h3>
<p><code>(tensor-backward tensor)</code> returns a abstractnode if the tensor is created by any operation.</p>
<h3 id="slot-requires-grad-boolean">[slot] requires-grad (Boolean)</h3>
<p>Set T to hold the gradients.</p>
<h3 id="slot-ancestor-param-p-boolean">[slot] ancestor-param-p (Boolean)</h3>
<p>Set T if compilers can reach any tensors with <code>:requires-grad=t</code>, by tracing the tensor.</p>
<h3 id="slot-flexible-p-fixnum-or-null">[slot] flexible-p (Fixnum or Null)</h3>
<p>Indicates the position of broadcastable axis.</p>
<h3 id="slot-facet-keyword">[slot] facet (keyword)</h3>
<p>AbstractTensors in cl-waffe2 has a two state: <code>ExistTensor</code> and <code>InputTensor</code>. <code>ExistTensor</code> is a just tensor with allocated storage vec, made by make-tensor function. On the other hand InputTensor is a lazy-evaluated tensor, allocation won't be done until it is needed.</p>
<p>:exist to ExitTensor, :input to InputTensor.</p>
<h3 id="method-mref">[method] mref</h3>
<p><code>(mref tensor &amp;rest subscripts)</code> will reads a cetrain position of storage vec. This is setfable. In terms of performance, it is much faster way to edit a storage vec that using <code>(change-facet)</code> function and convert into other forms.</p>
<h3 id="accessor-tensor-state-dict-name">[accessor] tensor-state-dict-name</h3>
<p>If the tensor is created as a parameter and placed inside <code>defmodel</code>, it is filled with the name of slot where it is placed. The name is used to create state_dict.</p>
<h3 id="accessor-tensor-param-belongs-to">[accessor] tensor-param-belongs-to</h3>
<p>If the tensor is a parameter, this slot is set by a composite which the tensor belongs to.</p>
<h2 id="function-hook-optimizer">[function] hook-optimizer!</h2>
<pre><code class="language-lisp">(hook-optimizer! tensor optimizer)
</code></pre>
<p>Hooks the optimizer to the tensor.</p>
<h3 id="inputs">Inputs</h3>
<p>tensor[AbstractTensor]</p>
<p>optimizer[AbstractOptimizer]</p>
<h2 id="function-call-optimizer">[function] call-optimizer!</h2>
<pre><code class="language-lisp">(call-optimizer! tensor)
</code></pre>
<p>Reading the <code>(grad tensor)</code>, the function invokes the optimizer hooked to the tensor.</p>
<h2 id="function-reset-grad">[function] reset-grad!</h2>
<p>Resets the gradient of the tensor with zero with <code>retain-grad=t</code>.</p>
<h2 id="function-tensor-vec">[function] tensor-vec</h2>
<pre><code class="language-lisp">(tensor-vec tensor)
</code></pre>
<p>If the given tensor is a ExistTensor, returns its storage vec.</p>
<p>If the given tensor is a InputTensor, allocates the area for tensor and return its storage vec.</p>
<p>This function is setfable and inlined.</p>
<h2 id="function-make-tensor">[function] make-tensor</h2>
<pre><code>(make-tensor shape-or-scalar
           &amp;key
          (requires-grad nil)
          (dtype *default-dtype*)
          (view nil)
          (order *default-order*)
          (initial-element nil)
                  (device nil))
</code></pre>
<p>Created a new ExistTensor of a device of <code>(car *using-backend*)</code>.</p>
<h3 id="inputs_1">Inputs</h3>
<ol>
<li>
<p><code>shape-or-scalar</code>[Anything] If set to list, creates a new matrix. Otherwise (e.g.: set to fixnum), creates a ScalarTensor. In that case, cl-waffe2 uses the highest priority device from <code>*using-backends*</code> parameter that inherits from the <code>ScalarTensor</code> class.</p>
</li>
<li>
<p><code>requires-grad</code>[Boolean] Set t to holds a gradients. <code>(parameter tensor)</code> will also do the same work. Under <code>(with-no-grad ...)</code> macro. This is set to nil forcibly.</p>
</li>
<li>
<p><code>dtype</code>[keyword] Set keyword indicating a type of elements.</p>
</li>
<li>
<p><code>order</code>[keyword] set keyword indicating the order of elments from <code>:column</code> or <code>:row</code>. in default set to <code>:column</code>.</p>
</li>
<li>
<p><code>initial-element</code>[Anything] Set anything which you want to set as a initial element.</p>
</li>
<li>
<p><code>device[symbol or null]</code> If set to symbol, the function returns with making a tensor of device.</p>
</li>
</ol>
<h3 id="example">Example</h3>
<pre><code class="language-lisp">(make-tensor `(10 10) :initial-element 1.0)

{CPUTENSOR[float] :shape (10 10)  
  ((1.0 1.0 1.0 ~ 1.0 1.0 1.0)           
   (1.0 1.0 1.0 ~ 1.0 1.0 1.0)   
        ...
   (1.0 1.0 1.0 ~ 1.0 1.0 1.0)
   (1.0 1.0 1.0 ~ 1.0 1.0 1.0))
  :facet :exist
  :requires-grad NIL
  :backward NIL}
</code></pre>
<h2 id="function-make-input">[function] make-input</h2>
<pre><code class="language-lisp">(make-input shape named &amp;key (created-from nil) (scalar-p nil) (dtype *default-dtype*) (order *default-order*))
</code></pre>
<p>Creates a new InputTensor. The allocation won't be done until the function <code>(tensor-vec tensor)</code> is called. In cl-waffe2, InputTensors can be applied for various things, for example, tracing the structure of computation node, used as a temporary tensor which can be pruned later by a compiler, as an argument of the computation node compiled by the <code>build</code> function. </p>
<h3 id="inputs_2">Inputs</h3>
<p><code>Shape</code> [list] Set the shape of tensor. You can also use symbols if shapes can be changed later. The function <code>set-input</code> will update all symbols declared in the computation node, and accordingly, strides/shapes etc... will be also updated to minimise compiling-time overhead (use <code>build</code> and <code>forward</code> to do this). ScalarTensors aren't created by setting it=<code>&lt;&lt;Something but not a list&gt;&gt;</code>. Instead, set <code>scalar-p=t</code>.</p>
<p><code>Named</code> [keyword or null] Indicates the name of tensor. If set to keyword, This means the name of the argument when compiled into a function, which can be changed later. If set to nil, the name is filled with <code>gensym</code> indicating the index in the memory-pool.</p>
<p><code>scalar-p</code> [boolean] Set t to create a scalar.</p>
<p><code>dtype</code> [keyword] Set dtype.</p>
<p><code>order</code> [keyword] Set order.</p>
<p><code>create-from[nil or AbstractTensor]</code> The returned InputTensor will extend Permutions/Strides and so on from <code>create-from</code> if any.</p>
<h3 id="example_1">Example</h3>
<pre><code class="language-lisp">(make-input `(a 10) :train-x)

{CPUTENSOR[float] :shape (A 10) :named :TRAIN-X 
    &lt;&lt;Not allocated: size=(A 10)&gt;&gt;
  :facet :input
  :requires-grad NIL
  :backward NIL}
</code></pre>
<h2 id="manipulating-gradients">Manipulating Gradients</h2>
<h2 id="parameter-no-grad">[parameter] <code>*no-grad*</code></h2>
<p>Ensures that back-propagation is not invoked inside the scope for which this parameter is set to T, with the following effects:</p>
<ul>
<li>
<p>Save For Backward is forcibly ignored.</p>
</li>
<li>
<p>Computational nodes for back propagation are not compiled.</p>
</li>
</ul>
<p>In default, set to nil. See also the <code>with-no-grad</code> macro to explict this state.</p>
<h2 id="macro-with-no-grad">[macro] with-no-grad</h2>
<pre><code class="language-lisp">(with-no-grad &amp;body body)
</code></pre>
<p>Set T to <code>*no-grad*</code> during the execution of body.</p>
<h2 id="function-parameter">[function] parameter</h2>
<pre><code>(parameter tensor)
</code></pre>
<p>Creates a new tensor with :requires-grad=t from the given tensor. If the tensor is remained to be computed, parameter will use the result from <code>proceed</code>.</p>
<h3 id="example_2">Example</h3>
<pre><code class="language-lisp">(parameter (randn `(3 3)))
</code></pre>
<h2 id="building-functions-from-abstracttensor">Building functions from AbstractTensor</h2>
<h2 id="class-compiled-composite">[class] Compiled-Composite</h2>
<p>Stores information on computation nodes compiled by the build function. The user has to guarantee that this point is the end of the computation node. Therefore, it is not possible in principle to continue the computation node after this point. Forward and backward propagation can be invoked using the <code>forward</code> and <code>backward</code> methods respectively.</p>
<pre><code class="language-lisp">;; Example
(let ((model (build (!add 1 1))))
      (forward model)
      (backward model))
</code></pre>
<p>This class furthermore records information on lazy-evaluated tensors. The tensor is an argument to the function, which can change the input via the <code>set-input</code> method.</p>
<pre><code class="language-lisp">(let ((lazy-tensor (make-input `(10 10) :A)))
    (let ((model (build (!sum lazy-tensor))))
         (set-input model :A (randn `(10 10))) ;; :A = (randn `(10 10))
         (get-input model :A)
         (forward model)))
</code></pre>
<p>By passing argument information to the compiler at build time, arguments can be given together when the forward method is called.</p>
<pre><code class="language-lisp">(let ((a (make-input `(A B) :A))
      (b (make-input `(A B) :B)))
    (let ((model (build (!mul a b) :inputs `(:A :B))))
          (forward model (randn `(3 3)) (randn `(3 3)))))
</code></pre>
<p>All tensors with <code>:requires-grad=t</code>, can be accessed by the <code>(model-parameters model)</code> method.</p>
<h2 id="function-build">[function] build</h2>
<pre><code class="language-lisp">(build toplevel &amp;key (inputs nil) (construct-backward? (not *no-grad*)) (compile-mode :fastest) (fuse-ops t))
</code></pre>
<p>Compiles the given computation node starting from <code>toplevel</code>. The docstring of <code>Compiled-Composite</code> describes how this function are used in practical.</p>
<h3 id="inputs_3">Inputs</h3>
<p><code>toplevel [AbstractTensor]</code> The end of node. Any shapes could be OK even when constructing backward.</p>
<p><code>inputs[list]</code> Set a list of argument keywords here so that the method <code>forward</code> can receive arguments that have been lazily evaluated. The order is taken into account. (e.g.: Set to <code>(:A :B)</code> and forward can receive this: <code>(forward compiled-model (randn</code>(3 3)) (randn <code>(3 3)))</code>)</p>
<p><code>construct-backward?</code> [boolean] Set t to build backward.</p>
<p><code>compile-mode</code>[compile-mode-t] an keyword indicating the compiling option. (No significant impact on execution speed but compile speed. for any case <code>:fastest</code> is the best solution.)</p>
<p><code>fuse-ops[boolean]</code> Set to enable <code>FusionOps</code> declared by <code>defpath</code>.</p>
<h3 id="example_3">Example</h3>
<p><strong>REPL:</strong></p>
<pre><code class="language-lisp">&gt; (setq out (!add (make-input `(a 10) :X) (make-input `(a 10) :Y)))
</code></pre>
<pre><code>{CPUTENSOR[float] :shape (A 10) :id TID1622 
  :vec-state [maybe-not-computed]
    &lt;&lt;Not allocated: size=(A 10)&gt;&gt;
  :facet :input
  :belongs-to :memory-pool
  :requires-grad NIL
  :backward &lt;Node: ADDNODE-CPUTENSOR (A[~] B[~] -&gt; A[~])&gt;}
</code></pre>
<p><strong>REPL:</strong></p>
<pre><code class="language-lisp">&gt; (with-no-grad (build out :inputs `(:X :Y)))
</code></pre>
<pre><code>&lt;Compiled-Composite(allocated-p=NIL)
    forward     : forward(model X Y) -&gt; CPUTENSOR{FLOAT}(A 10)
    backward    : nil
    memory-pool : one tensor(s)
                   L {4.0e-5+((A) x 4.0e-6)}MB
    inputs:
        X -&gt; (A 10)
        Y -&gt; (A 10)
&gt;
</code></pre>
<h2 id="method-set-input">[method] set-input</h2>
<pre><code>(set-input (model Compiled-Composite) input-name actual-value)
</code></pre>
<p>Embodies an <code>InputTensor</code> in the model. All unembodied tensors in the model can be accessed by printing the model.</p>
<p><code>input-name</code> could be a keyword indicating input-tensor, <code>actual-value</code> is a <code>AbstractTensor</code> whose facet = <code>:exist</code> (created by <code>make-tensor</code>).</p>
<h2 id="method-get-input">[method] get-input</h2>
<pre><code>(get-input (model Compiled-Composite) input-name)
</code></pre>
<p>Reading all variables in the computation node, the method get-input returns an corresponding <code>InputTensor</code> of model.</p>
<h2 id="optimized-and-ranked-tensor-iterators">Optimized and Ranked Tensor Iterators</h2>
<h2 id="function-call-with-view">[function] call-with-view</h2>
<p>Inlined Loop Macro Generator for the extension to higher order foreign functions.</p>
<pre><code class="language-lisp">(call-with-view function tensors &amp;key (at-least-dim 1) (force-order nil) (lparallel nil))
</code></pre>
<p>The function <code>call-with-view</code> generates a lisp code calling BLAS-like function for nd-arrays with considered offsets produced by views. Plus, depending on the memory-layouts and permutation(offsets) the generated loop is also collapsed and shuffled to maximize the locality of memory and reduce the overheads produced by calling CFFI function.</p>
<p>In the simplest case, <code>call-with-view</code> first deploys <code>(loop for...)</code> until the rank of given tensors reaches the given <code>at-least-dim</code>. After reaching <code>at-least-dim</code>, the function places the result of calling the given <code>function</code>.</p>
<pre><code class="language-lisp">(call-with-view
      #'(lambda (x-view)
       `(+ 1 1))
       (list (randn `(100 100 100)))
       :at-least-dim 1)

(LET ((#:OFFSETS68091
       (MAKE-ARRAY 1 :ELEMENT-TYPE '(UNSIGNED-BYTE 64) :INITIAL-ELEMENT 0)))
  (DECLARE (TYPE (SIMPLE-ARRAY (UNSIGNED-BYTE 64) (*)) #:OFFSETS68091))
  (PROGN (+ 1 1)))
</code></pre>
<p>Here, the number of tensors corresponds with the number of arguments <code>function</code> receive. Usually, the function receives information on the view of the tensor at the corresponding position: <code>(size-of x-view)</code> to get the number of iteration, <code>(stride-of x-view)</code> to get the number of increment, and, <code>(offset-of x-view)</code> to get the offset of tensor. (Sometimes they return s-expression because the shapes of tensors are not necessary number, but symbols.)</p>
<p><code>function [function]</code> should return a list which corresponds with invoking user-defined operation given views.</p>
<p><code>tensors[a list of abstracttensor]</code> tensors to be called with.</p>
<p><code>at-least-dim [fixnum]</code> <code>at-least-dim is minimum rank value required by the operation. set 1 to define</code>element-wise<code>operation, set 2 to define</code>gemm` for example.</p>
<p><code>force-order[boolean]</code> On some conditions, <code>call-with-view</code> shuffles the order of ranks, or flattens given tensors (e.g.: <code>100x100</code> tensors is the equivalent to just <code>10000x1</code> tensor on the memory). If you want to disable this behaviour, set <code>force-order</code>=t.</p>
<p><code>lparallel[boolean]</code> Set t to use lparallel. This should be denoted that under lparallel execution, the parameter <code>cl-waffe2/threads:*under-multi-thread*</code> becomes t. Use this parameter for the lowest rank operation to decide whether to parallelise.</p>
<p>Return: <code>Expanded Lisp Codes</code></p>
<p>Note that <code>call-with-view</code> should be used at once or zero in the one <code>define-impl</code> forward. If you need twice times to call it, the general definition of <code>AbstractNode</code> should be split.</p>
<p>See also: <code>with-ranked-loop</code> to the more elegant wrapping macro.</p>
<h2 id="macro-do-compiled-loop">[macro] do-compiled-loop</h2>
<pre><code class="language-lisp">(do-compiled-loop tensor-list (&amp;key (kernel-size 1) (collapse t) (mode :runtime)) (&amp;rest views-bind) &amp;body body)
</code></pre>
<p>Iterates the given tensors in optimized order. The behavior is the same as the <code>call-with-view</code> function in that both is intended to call a ranked matrix function with considering multidimensional offsets. This macro, however, directly placed within functions. </p>
<h3 id="inputs_4">Inputs</h3>
<p><code>tensor-list[list]</code> an list of tensors. all of them must have the same rank and shape[&gt; kernel-size]. Must not include adjustable shape.</p>
<p><code>kernel-size[(unsigned-byte 32)]</code> Indicates the rank of operation, that is, indicates the same as <code>:at-least-dim</code> in the call-with-view function.</p>
<p><code>collapse[boolean]</code> Set T to enable <code>Loop Collapse</code>. (= (not :force-order) in call-with-view)</p>
<p><code>mode[:runtime or :heuristic]</code> indicates the algorithm of optimizing. <code>:heuristic</code> mode is still under experimental and not tested well. So set :runtime.</p>
<h3 id="example_4">Example</h3>
<pre><code class="language-lisp">(define-impl-op (Compare-Operation-Node :device LispTensor)
        :forward ((self tensor1 tensor2 out)
              (let ((kernel (compare-kernel (dtype tensor1))))
                (do-compiled-loop (list tensor1 tensor2 out) ()
                (x-view y-view o-view)
                  (funcall kernel
                       (tensor-vec tensor1)
                       (tensor-vec tensor2)
                       (tensor-vec out)                    
                       (logical-condition self)
                       (logical-true-then self)
                       (logical-false-then self)
                       (size-of x-view 0)
                       (offset-of x-view 0)
                       (offset-of y-view 0)
                       (offset-of o-view 0)
                       (stride-of x-view 0)
                       (stride-of y-view 0)
                       (stride-of o-view 0)))
                out)))
</code></pre>
<h2 id="save-and-restore-weights">Save and Restore Weights</h2>
<h2 id="struct-state-dict">[struct] State-Dict</h2>
<pre><code class="language-lisp">;; 1. Creating from existing hash-table
(from-state-dict state-dict-hash-table)
</code></pre>
<pre><code class="language-lisp">;; 2. Creating from compiled composite
(make-state-dict compiled-composite &amp;key (weights T) (optimizers nil))
</code></pre>
<p>A table structure obtained from tracing all parameters of Compiled-Composite which is used to save/restore all parameters in a compiled-composite.</p>
<p>To ensure reproducibility, state-dict collects following contents:</p>
<ul>
<li>
<p>If weights=T, AbstractTensor where requires-grad=T (the tensor must belong to any slots of Composite, otherwise cl-waffe2 can't determine the key name)</p>
</li>
<li>
<p>If optimizers=T, reading all slots of <code>AbstractOptimizer</code>, values that satisfies <code>(numberp value)</code> <code>(typep x AbstractTensor)</code> are saved.</p>
</li>
</ul>
<h3 id="state-dict-naming-convention">State-dict naming convention</h3>
<p>Basically Follows this rule:</p>
<p><code>STATE_DICT_NAME = {PREFIX}:{COMPOSITE_NAME}.{NTH?}.{SLOT_NAME}</code></p>
<p>where:</p>
<ul>
<li>
<p><code>PREFIX</code> is one of param, missing_param, optimizer, missing_optimizer. if the prefix has <code>missing</code>, the parameter didn't belong to any composite. the prefix param indicates the corresponding value is a trained weight. optimizer indicates the value is one of slot of AbstractOptimizer.</p>
</li>
<li>
<p><code>COMPOSITE_NAME</code> = the name of model the parameter belongs to (tensor-param-belongs-to ...)</p>
</li>
<li>
<p><code>SLOT_NAME</code> = the name of slot  (tensor-state-dict-name ...)</p>
</li>
<li>
<p><code>NTH</code> As {COMPOSITE_NAME}.{SLOT_NAME} naming conflicts in the dictionary, NTH is increased by 1. First=0.</p>
</li>
</ul>
<p>If the value to save is a slot of AbstractOptimizer, we use following naming in addition to STATE_DICT_NAME.</p>
<p><code>optimizer:{STATE_DICT_NAME}.{OPTIMIZER_NAME}.{TYPE_SPECIFIER}.{SLOT_NAME}</code></p>
<p>For example, LinearLayer has two parameters named as: <code>param:linearlayer.0.bias</code> <code>param:linearlayer.0.weights</code>. If adam optimizers are hooked to them, following keys are added: <code>optimizer:linearlayer.0.bias.adam.single-float.lr</code>, <code>optimizer:linearlayer.0.bias.adam.single-float.eps</code>, <code>optimizer:linearlayer.0.bias.adam.single-float.beta1</code>, <code>optimizer:linearlayer.0.bias.adam.single-float.beta2</code>, <code>optimizer:linearlayer.0.bias.adam.bit.n</code>, <code>optimizer:linearlayer.0.bias.adam.cputensor.m</code>, <code>optimizer:linearlayer.0.bias.adam.cputensor.v</code>, <code>param:linearlayer.0.weights</code>, <code>optimizer:linearlayer.0.weights.adam.single-float.lr</code> <code>optimizer:linearlayer.0.weights.adam.single-float.eps</code>, <code>optimizer:linearlayer.0.weights.adam.single-float.beta1</code>, <code>optimizer:linearlayer.0.weights.adam.single-float.beta2</code>, <code>optimizer:linearlayer.0.weights.adam.bit.n</code>, <code>optimizer:linearlayer.0.weights.adam.cputensor.m</code>, and <code>optimizer:linearlayer.0.weights.adam.cputensor.v</code>.</p>
<p>Note that all keys are stored as a string. all strings are downcased. The package to which the symbol belongs is ignored. (e.g.: cl-waffe2/nn:LinearLayer is saved as just linearlayer).</p>
<h3 id="parsing-a-state-dict-key">Parsing a state dict key</h3>
<p>In order to parse the state_dict key, the function <code>parse-state-dict-key</code> is available.</p>
<pre><code class="language-lisp">(parse-state-dict-key key)
;; -&gt; (values prefix rest-forms)
;; e.g.: (values :param &quot;linearlayer&quot; &quot;0&quot;  &quot;bias&quot;)
</code></pre>
<h3 id="slots">Slots</h3>
<p><code>(state-dict-table state-dict)[hash-table]</code> key -&gt; value hash table where :test is #'equal</p>
<h3 id="example_5">Example</h3>
<p><strong>REPL:</strong></p>
<pre><code class="language-lisp">&gt; (make-state-dict (build (call (LinearLayer 10 10) (randn `(10 10)))))
</code></pre>
<pre><code>#S(STATE-DICT :TABLE #&lt;HASH-TABLE :TEST EQUAL :COUNT 2 {1005210003}&gt;
 table-key-to-value:
    param:linearlayer.0.bias    -&gt; CPUTENSOR{FLOAT}(10)
    param:linearlayer.0.weights -&gt; CPUTENSOR{FLOAT}(10 10)

)
</code></pre>
<h2 id="macro-define-model-format">[macro] define-model-format</h2>
<pre><code class="language-lisp">(define-model-format ((format device) &amp;key (save-weights) (load-weights)))
</code></pre>
<p>Defines a format in which the compiled-composite is saved.</p>
<h3 id="inputs_5">Inputs</h3>
<p><code>format[keyword]</code> a keyword indicating the format. <code>save-weight</code> <code>load-weight</code> can find the format from this keyword.</p>
<p><code>device[symbol]</code> a symbol indicating a device to use. If parameters aren't given as <code>device</code>, automatically converts to device.</p>
<p><code>save-weights[form]</code> a form saving <code>state-dict</code> to <code>path</code>. This form must be: <code>((path state-dict) body)</code>.</p>
<p><code>load-weights[form]</code> a form restoring <code>state-dict</code> from path. This form also must be <code>((path state-dict) body)</code>. The form must return <code>State-dict</code> structure.</p>
<h3 id="examples">Examples</h3>
<pre><code class="language-lisp">(define-model-format
    (:my-format cl-waffe2/backends.lisp:LispTensor)
    :save-weights
    ((path state-dict)
     ;; Use (device-as tensor 'LispTensor) to always make storage-vec simple-array.
     ;; Writing the values of state-dict into path
     T)
    :load-weights
    ((path)
     ;; Restores the values of state-dict from path
     ;; Retuning a hash-table
     (from-state-dict (make-hash-table))))

(save-weights compiled-model path :my-format)
(load-weights compiled-model path :my-format)
</code></pre>
<h2 id="function-save-weights">[function] save-weights</h2>
<pre><code class="language-lisp">(save-weights compiled-composite save-dir format &amp;key (weights T) (optimizers NIL))
</code></pre>
<p>Saves compiled-composite as a <code>format</code> to <code>save-dir</code>.</p>
<h3 id="examples_1">Examples</h3>
<pre><code class="language-lisp">(save-weights model &quot;./model.wf2model&quot; :wf2model)
</code></pre>
<h3 id="inputs_6">Inputs</h3>
<p><code>weights[boolean]</code> Set T to save all trainable parameters in the compiled-composite</p>
<p><code>optimizers[boolean]</code> Set T to save all parameters of AbstractTensor</p>
<h2 id="function-load-weights">[function] load-weights</h2>
<pre><code class="language-lisp">(load-weights compiled-model save-dir format)
</code></pre>
<p>Restores all weights and states from <code>save-dir</code> as <code>format</code>.</p>
<h3 id="examples_2">Examples</h3>
<pre><code class="language-lisp">(load-weights model &quot;./model.wf2model&quot; :wf2model)
</code></pre>
<h2 id="function-load-from-state-dict">[function] load-from-state-dict</h2>
<pre><code class="language-lisp">(load-from-state-dict compiled-composite state-dict)
;; -&gt; (list failed-values)
</code></pre>
<p>This function restores the training status from state-dict, overwriting compiled-composite values.</p>
<h3 id="inputs_7">Inputs</h3>
<ul>
<li>
<p><code>compiled-composite[Compiled-Composite]</code></p>
</li>
<li>
<p><code>state-dict[State-Dict]</code></p>
</li>
</ul>
<h3 id="returns">Returns</h3>
<ul>
<li><code>failed-values[list]</code> an list of values existing in <code>state-dict</code> that couldn't loaded well.</li>
</ul>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../nodes/" class="btn btn-neutral float-left" title="cl-waffe2/vm.nodes"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../vm/" class="btn btn-neutral float-right" title="cl-waffe2/vm">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../nodes/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../vm/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
