{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Programmable Deep Learning Framework Repository \u00bb Issues \u00b7 Installing \u00b7 Tutorials \u26a0\ufe0f cl-waffe2 is still in the concept stage and has a limited feature. Also, things are subject to change. DO NOT USE CL-WAFFE2 IN YOUR PRODUCT. cl-waffe2 provides a set of differentiable matrix operations which is aimed to apply to build neural network models on Common Lisp. The primary concepts and goals of this project is following: Do not run until the node is optimized. , that is, Lazy-Evaluation first. Generic operations powered by multiple kinds of user-defined backends. defined-and-run, closer to defined-by-run APIs. Everything in this documentation is still incomplete! I guess should be much more polished! What's next? Install cl-waffe2 Learn the concepts with practical tutorials Visit some examples! (Not yet ready...) Benchmarks Coming soon ... Contribute Coming soon ... Acknowledgments Coming soon... LICENCE Coming soon...","title":"Home"},{"location":"#_1","text":"","title":""},{"location":"#whats-next","text":"Install cl-waffe2 Learn the concepts with practical tutorials Visit some examples! (Not yet ready...)","title":"What's next?"},{"location":"#benchmarks","text":"Coming soon ...","title":"Benchmarks"},{"location":"#contribute","text":"Coming soon ...","title":"Contribute"},{"location":"#acknowledgments","text":"Coming soon...","title":"Acknowledgments"},{"location":"#licence","text":"Coming soon...","title":"LICENCE"},{"location":"Tips/","text":"Tips -> Example? MNIST Kernel Abstraction Adding a new backend Adding a new operation Make a Input","title":"Tips"},{"location":"Tips/#tips-example","text":"","title":"Tips -&gt; Example?"},{"location":"Tips/#mnist","text":"","title":"MNIST"},{"location":"Tips/#kernel-abstraction","text":"","title":"Kernel Abstraction"},{"location":"Tips/#adding-a-new-backend","text":"","title":"Adding a new backend"},{"location":"Tips/#adding-a-new-operation","text":"","title":"Adding a new operation"},{"location":"Tips/#make-a-input","text":"","title":"Make a Input"},{"location":"base-impl-nodes/","text":".katex img { object-fit: fill; padding: unset; display: block; position: absolute; width: 100%; height: inherit; } Standard Nodes [node] ADDNODE (A[~] B[~] -> A[~]) Description AddNode is a node which computes following operation element-wise. Let X and Y be a given arguments and both are matrix. X \u2190 X + Y X\\gets{X + Y} X \u2190 X + Y Constructor (AddNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dx dy)) (values dout dout)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SUBNODE (A[~] B[~] -> A[~]) Description SubNode is a node which computes following operation element-wise. Let X and Y be a given arguments and both are matrix. X \u2190 X \u2212 Y X\\gets{X - Y} X \u2190 X \u2212 Y Constructor (SubNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dx dy)) (values dout (!mul -1 dout))) No need to implement backwards at define-impl . (they'd be ignored.) [node] MULNODE (A[~] B[~] -> A[~]) Description MulNode is a node which computes following operation element-wise. Let X and Y be a given arguments and both are matrix. X \u2190 X \u2217 Y X\\gets{X * Y} X \u2190 X \u2217 Y Constructor (MulNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx dy) (values (!mul dout dy) (!mul dout dx))) No need to implement backwards at define-impl . (they'd be ignored.) [node] DIVNODE (A[~] B[~] -> A[~]) Description DivNode is a node which computes following operation element-wise. Let X and Y be a given arguments and both are matrix. X \u2190 X / Y X\\gets{X / Y} X \u2190 X / Y Constructor (DivNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx dy) (values (!div dout dy) (!div (!mul dx (!mul -1 dout)) (!square dy)))) No need to implement backwards at define-impl . (they'd be ignored.) [node] INVERSETENSORNODE (A[~] -> A[~]) Description InverseTensorNode is a node which computes following operation element-wise A \u2190 1 / A A\\gets{1 / A} A \u2190 1/ A Constructor (InverseTensorNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx) (values (!div (!mul -1 dout) (!square dx)))) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALARADD (A[~] SCALAR[SCAL] -> A[~] WHERE SCAL = 1) Description ScalarAdd is a node which computes following operation element-wise. Let X be a given matrix and S be a given scalar. X \u2190 X + s c a l a r X\\gets{X + scalar} X \u2190 X + sc a l a r Constructor (ScalarAdd dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dx dy)) (values dout (->scal (!mean dout)))) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALARSUB (A[~] SCALAR[SCAL] -> A[~] WHERE SCAL = 1) Description ScalarSub is a node which computes following operation element-wise. Let X be a given matrix and S be a given scalar. X \u2190 X \u2212 s c a l a r X\\gets{X - scalar} X \u2190 X \u2212 sc a l a r Constructor (ScalarSub dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dx dy)) (values dout (->scal (!mul -1.0 (!mean dout))))) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALARMUL (A[~] SCALAR[SCAL] -> A[~] WHERE SCAL = 1) Description ScalarMul is a node which computes following operation element-wise. Let X be a given matrix and S be a given scalar. X \u2190 X \u2217 s c a l a r X\\gets{X * scalar} X \u2190 X \u2217 sc a l a r Constructor (ScalarMul dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx dy) (values (!mul dout dy) (->scal (!mean (!mul dx dout))))) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALARDIV (A[~] SCALAR[SCAL] -> A[~] WHERE SCAL = 1) Description ScalarDiv is a node which computes following operation element-wise. Let X be a given matrix and S be a given scalar. X \u2190 X / s c a l a r X\\gets{X / scalar} X \u2190 X / sc a l a r Constructor (ScalarDiv dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx dy) (values (!div dout dy) (->scal (!mean (!div (!mul dx (!mul -1 dout)) (!square dy)))))) No need to implement backwards at define-impl . (they'd be ignored.) [node] MOVETENSORNODE (A[~] B[~] -> A[~]) Description Moves all the visible elements of B into visible areas of A . A \u2190 B A\\gets{B} A \u2190 B Behaviour All cl-waffe2 operations follow this rule: Make a copy for now, disable later . (e.g.: the function (!add x y) makes an copy of x and y for now, but this copy operation is ignored, if they're concluded not to be needed, by tracing computation node.) In order to disable a useless copy operations, MoveTensorNode must follow this behaviour: Reading (movetensor-ignore-me self) in runtime, the forward makes a copy of given tensor only after the slot is nil . Otherwise, return B Don't worry the allocation won't be done until (tensor-vec A) is called. For practical example, my impls ( ./source/backends/lisp/arithmetic.lisp for example) would be helpful!. Constructor (MoveTensorNode dtype) dtype dtype to use. Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dx)) (let ((dy-out (if (and (eql (tensor-attribute dy) chain) (movetensor-ignore-me self)) dout (!copy dout force t)))) (values nil dy-out))) No need to implement backwards at define-impl . (they'd be ignored.) [node] ABSNODE (X[~] OUT[~] -> OUT[~]) Description The node ABSNODE takes X as an argument, applying a abs function into each element and writes the result into out. O U T \u2190 a b s ( X ) OUT\\gets{abs(X)} O U T \u2190 ab s ( X ) save-for-backward: (T NIL) See also: SCALAR-ABSNODE !abs Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dy)) (values (!mul dout (!sign dx)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-ABSNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-ABSNODE takes scalar X as an argument, applying a abs function into each element and writes the result into out. o u t \u2190 a b s ( x ) out\\gets{abs(x)} o u t \u2190 ab s ( x ) save-for-backward: (T NIL) See also: ABSNODE !abs Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dy)) (values (!mul dout (!sign dx)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SIGNNODE (X[~] OUT[~] -> OUT[~]) Description The node SIGNNODE takes X as an argument, applying a sign function into each element and writes the result into out. O U T \u2190 s i g n ( X ) OUT\\gets{sign(X)} O U T \u2190 s i g n ( X ) save-for-backward: (T NIL) See also: SCALAR-SIGNNODE !sign Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dout dy)) (values (!mul dx 0) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-SIGNNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-SIGNNODE takes scalar X as an argument, applying a sign function into each element and writes the result into out. o u t \u2190 s i g n ( x ) out\\gets{sign(x)} o u t \u2190 s i g n ( x ) save-for-backward: (T NIL) See also: SIGNNODE !sign Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dout dy)) (values (!mul dx 0) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SQRTNODE (X[~] OUT[~] -> OUT[~]) Description The node SQRTNODE takes X as an argument, applying a sqrt function into each element and writes the result into out. O U T \u2190 s q r t ( X ) OUT\\gets{sqrt(X)} O U T \u2190 s q r t ( X ) save-for-backward: (T NIL) See also: SCALAR-SQRTNODE !sqrt Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dy)) (values (!mul dout (!div 1 dx)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-SQRTNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-SQRTNODE takes scalar X as an argument, applying a sqrt function into each element and writes the result into out. o u t \u2190 s q r t ( x ) out\\gets{sqrt(x)} o u t \u2190 s q r t ( x ) save-for-backward: (T NIL) See also: SQRTNODE !sqrt Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dy)) (values (!mul dout (!div 1 dx)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SQUARENODE (X[~] OUT[~] -> OUT[~]) Description The node SQUARENODE takes X as an argument, applying a square function into each element and writes the result into out. O U T \u2190 s q u a r e ( X ) OUT\\gets{square(X)} O U T \u2190 s q u a re ( X ) save-for-backward: (T NIL) See also: SCALAR-SQUARENODE !square Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout x) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-SQUARENODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-SQUARENODE takes scalar X as an argument, applying a square function into each element and writes the result into out. o u t \u2190 s q u a r e ( x ) out\\gets{square(x)} o u t \u2190 s q u a re ( x ) save-for-backward: (T NIL) See also: SQUARENODE !square Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout x) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SINNODE (X[~] OUT[~] -> OUT[~]) Description The node SINNODE takes X as an argument, applying a sin function into each element and writes the result into out. O U T \u2190 s i n ( X ) OUT\\gets{sin(X)} O U T \u2190 s in ( X ) save-for-backward: (T NIL) See also: SCALAR-SINNODE !sin Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!cos x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-SINNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-SINNODE takes scalar X as an argument, applying a sin function into each element and writes the result into out. o u t \u2190 s i n ( x ) out\\gets{sin(x)} o u t \u2190 s in ( x ) save-for-backward: (T NIL) See also: SINNODE !sin Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!cos x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] COSNODE (X[~] OUT[~] -> OUT[~]) Description The node COSNODE takes X as an argument, applying a cos function into each element and writes the result into out. O U T \u2190 c o s ( X ) OUT\\gets{cos(X)} O U T \u2190 cos ( X ) save-for-backward: (T NIL) See also: SCALAR-COSNODE !cos Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!mul -1 (!sin x))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-COSNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-COSNODE takes scalar X as an argument, applying a cos function into each element and writes the result into out. o u t \u2190 c o s ( x ) out\\gets{cos(x)} o u t \u2190 cos ( x ) save-for-backward: (T NIL) See also: COSNODE !cos Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!mul -1 (!sin x))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] TANNODE (X[~] OUT[~] -> OUT[~]) Description The node TANNODE takes X as an argument, applying a tan function into each element and writes the result into out. O U T \u2190 t a n ( X ) OUT\\gets{tan(X)} O U T \u2190 t an ( X ) save-for-backward: (T NIL) See also: SCALAR-TANNODE !tan Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul (!cos x) (!cos x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-TANNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-TANNODE takes scalar X as an argument, applying a tan function into each element and writes the result into out. o u t \u2190 t a n ( x ) out\\gets{tan(x)} o u t \u2190 t an ( x ) save-for-backward: (T NIL) See also: TANNODE !tan Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul (!cos x) (!cos x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] ASINNODE (X[~] OUT[~] -> OUT[~]) Description The node ASINNODE takes X as an argument, applying a asin function into each element and writes the result into out. O U T \u2190 a s i n ( X ) OUT\\gets{asin(X)} O U T \u2190 a s in ( X ) save-for-backward: (T NIL) See also: SCALAR-ASINNODE !asin Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!sqrt (!sub 1 (!square x))))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-ASINNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-ASINNODE takes scalar X as an argument, applying a asin function into each element and writes the result into out. o u t \u2190 a s i n ( x ) out\\gets{asin(x)} o u t \u2190 a s in ( x ) save-for-backward: (T NIL) See also: ASINNODE !asin Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!sqrt (!sub 1 (!square x))))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] ACOSNODE (X[~] OUT[~] -> OUT[~]) Description The node ACOSNODE takes X as an argument, applying a acos function into each element and writes the result into out. O U T \u2190 a c o s ( X ) OUT\\gets{acos(X)} O U T \u2190 a cos ( X ) save-for-backward: (T NIL) See also: SCALAR-ACOSNODE !acos Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div -1 (!sqrt (!sub 1 (!square x))))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-ACOSNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-ACOSNODE takes scalar X as an argument, applying a acos function into each element and writes the result into out. o u t \u2190 a c o s ( x ) out\\gets{acos(x)} o u t \u2190 a cos ( x ) save-for-backward: (T NIL) See also: ACOSNODE !acos Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div -1 (!sqrt (!sub 1 (!square x))))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] ATANNODE (X[~] OUT[~] -> OUT[~]) Description The node ATANNODE takes X as an argument, applying a atan function into each element and writes the result into out. O U T \u2190 a t a n ( X ) OUT\\gets{atan(X)} O U T \u2190 a t an ( X ) save-for-backward: (T NIL) See also: SCALAR-ATANNODE !atan Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!add 1 (!square x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-ATANNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-ATANNODE takes scalar X as an argument, applying a atan function into each element and writes the result into out. o u t \u2190 a t a n ( x ) out\\gets{atan(x)} o u t \u2190 a t an ( x ) save-for-backward: (T NIL) See also: ATANNODE !atan Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!add 1 (!square x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SINHNODE (X[~] OUT[~] -> OUT[~]) Description The node SINHNODE takes X as an argument, applying a sinh function into each element and writes the result into out. O U T \u2190 s i n h ( X ) OUT\\gets{sinh(X)} O U T \u2190 s inh ( X ) save-for-backward: (T NIL) See also: SCALAR-SINHNODE !sinh Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!cosh x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-SINHNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-SINHNODE takes scalar X as an argument, applying a sinh function into each element and writes the result into out. o u t \u2190 s i n h ( x ) out\\gets{sinh(x)} o u t \u2190 s inh ( x ) save-for-backward: (T NIL) See also: SINHNODE !sinh Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!cosh x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] COSHNODE (X[~] OUT[~] -> OUT[~]) Description The node COSHNODE takes X as an argument, applying a cosh function into each element and writes the result into out. O U T \u2190 c o s h ( X ) OUT\\gets{cosh(X)} O U T \u2190 cos h ( X ) save-for-backward: (T NIL) See also: SCALAR-COSHNODE !cosh Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!mul -1 (!sinh x))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-COSHNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-COSHNODE takes scalar X as an argument, applying a cosh function into each element and writes the result into out. o u t \u2190 c o s h ( x ) out\\gets{cosh(x)} o u t \u2190 cos h ( x ) save-for-backward: (T NIL) See also: COSHNODE !cosh Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!mul -1 (!sinh x))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] TANHNODE (X[~] OUT[~] -> OUT[~]) Description The node TANHNODE takes X as an argument, applying a tanh function into each element and writes the result into out. O U T \u2190 t a n h ( X ) OUT\\gets{tanh(X)} O U T \u2190 t anh ( X ) save-for-backward: (T NIL) See also: SCALAR-TANHNODE !tanh Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul (!cosh x) (!cosh x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-TANHNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-TANHNODE takes scalar X as an argument, applying a tanh function into each element and writes the result into out. o u t \u2190 t a n h ( x ) out\\gets{tanh(x)} o u t \u2190 t anh ( x ) save-for-backward: (T NIL) See also: TANHNODE !tanh Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul (!cosh x) (!cosh x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] ASINHNODE (X[~] OUT[~] -> OUT[~]) Description The node ASINHNODE takes X as an argument, applying a asinh function into each element and writes the result into out. O U T \u2190 a s i n h ( X ) OUT\\gets{asinh(X)} O U T \u2190 a s inh ( X ) save-for-backward: NIL See also: SCALAR-ASINHNODE !asinh Backward \u274c Undefined. (To make it differentiable, must be defined with define-impl macro.) [node] SCALAR-ASINHNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-ASINHNODE takes scalar X as an argument, applying a asinh function into each element and writes the result into out. o u t \u2190 a s i n h ( x ) out\\gets{asinh(x)} o u t \u2190 a s inh ( x ) save-for-backward: NIL See also: ASINHNODE !asinh Backward \u274c Undefined. (To make it differentiable, must be defined with define-impl macro.) [node] ACOSHNODE (X[~] OUT[~] -> OUT[~]) Description The node ACOSHNODE takes X as an argument, applying a acosh function into each element and writes the result into out. O U T \u2190 a c o s h ( X ) OUT\\gets{acosh(X)} O U T \u2190 a cos h ( X ) save-for-backward: NIL See also: SCALAR-ACOSHNODE !acosh Backward \u274c Undefined. (To make it differentiable, must be defined with define-impl macro.) [node] SCALAR-ACOSHNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-ACOSHNODE takes scalar X as an argument, applying a acosh function into each element and writes the result into out. o u t \u2190 a c o s h ( x ) out\\gets{acosh(x)} o u t \u2190 a cos h ( x ) save-for-backward: NIL See also: ACOSHNODE !acosh Backward \u274c Undefined. (To make it differentiable, must be defined with define-impl macro.) [node] ATANHNODE (X[~] OUT[~] -> OUT[~]) Description The node ATANHNODE takes X as an argument, applying a atanh function into each element and writes the result into out. O U T \u2190 a t a n h ( X ) OUT\\gets{atanh(X)} O U T \u2190 a t anh ( X ) save-for-backward: NIL See also: SCALAR-ATANHNODE !atanh Backward \u274c Undefined. (To make it differentiable, must be defined with define-impl macro.) [node] SCALAR-ATANHNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-ATANHNODE takes scalar X as an argument, applying a atanh function into each element and writes the result into out. o u t \u2190 a t a n h ( x ) out\\gets{atanh(x)} o u t \u2190 a t anh ( x ) save-for-backward: NIL See also: ATANHNODE !atanh Backward \u274c Undefined. (To make it differentiable, must be defined with define-impl macro.) [node] EXPNODE (X[~] OUT[~] -> OUT[~]) Description The node EXPNODE takes X as an argument, applying a exp function into each element and writes the result into out. O U T \u2190 e x p ( X ) OUT\\gets{exp(X)} O U T \u2190 e x p ( X ) save-for-backward: (T NIL) See also: SCALAR-EXPNODE !exp Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!exp x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-EXPNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-EXPNODE takes scalar X as an argument, applying a exp function into each element and writes the result into out. o u t \u2190 e x p ( x ) out\\gets{exp(x)} o u t \u2190 e x p ( x ) save-for-backward: (T NIL) See also: EXPNODE !exp Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!exp x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] LOG2NODE (X[~] OUT[~] -> OUT[~]) Description The node LOG2NODE takes X as an argument, applying a log2 function into each element and writes the result into out. O U T \u2190 l o g 2 ( X ) OUT\\gets{log2(X)} O U T \u2190 l o g 2 ( X ) save-for-backward: (T NIL) See also: SCALAR-LOG2NODE !log2 Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul x (log 2)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-LOG2NODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-LOG2NODE takes scalar X as an argument, applying a log2 function into each element and writes the result into out. o u t \u2190 l o g 2 ( x ) out\\gets{log2(x)} o u t \u2190 l o g 2 ( x ) save-for-backward: (T NIL) See also: LOG2NODE !log2 Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul x (log 2)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] LOG10NODE (X[~] OUT[~] -> OUT[~]) Description The node LOG10NODE takes X as an argument, applying a log10 function into each element and writes the result into out. O U T \u2190 l o g 10 ( X ) OUT\\gets{log10(X)} O U T \u2190 l o g 10 ( X ) save-for-backward: (T NIL) See also: SCALAR-LOG10NODE !log10 Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul x (log 10)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-LOG10NODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-LOG10NODE takes scalar X as an argument, applying a log10 function into each element and writes the result into out. o u t \u2190 l o g 10 ( x ) out\\gets{log10(x)} o u t \u2190 l o g 10 ( x ) save-for-backward: (T NIL) See also: LOG10NODE !log10 Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul x (log 10)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] LOGENODE (X[~] OUT[~] -> OUT[~]) Description The node LOGENODE takes X as an argument, applying a loge function into each element and writes the result into out. O U T \u2190 l o g e ( X ) OUT\\gets{loge(X)} O U T \u2190 l o g e ( X ) save-for-backward: (T NIL) See also: SCALAR-LOGENODE !loge Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-LOGENODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-LOGENODE takes scalar X as an argument, applying a loge function into each element and writes the result into out. o u t \u2190 l o g e ( x ) out\\gets{loge(x)} o u t \u2190 l o g e ( x ) save-for-backward: (T NIL) See also: LOGENODE !loge Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] LAZYTRANSPOSENODE (A[~ I J] -> A[~ J I]) Description LazyTransposeNode is the matmul-dedicated node which supplies the lazy-transpose feature. Internally, This Node Returns The Given A itself but taking transpose of A's shape. If the computation node is like: [LazyTransposeNode] -> [MatmulNode], then transpose will be done with NO overhead. Backward \u2705 Already defined. ((self dout dx) (declare (ignore dx)) (values (!t dout))) No need to implement backwards at define-impl . (they'd be ignored.) [node] ARGMAX-NODE (A[~] OUT[OUT-SIZE] -> OUT[OUT-SIZE]) Description ArgMax-Node finds a maximum value of all elements in A. OUT is overwritten with the result. A is a target to find a maximum value, and OUT is a place to set the index. Constructor (ArgMax-Node out-size) out-size the reducted shape of out . Backward \u2705 Already defined. ((self dout da do) (declare (ignore dout da do)) (values nil nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] ARGMIN-NODE (A[~] OUT[OUT-SIZE] -> OUT[OUT-SIZE]) Description ArgMin-Node finds a minimum value of all elements in A. OUT is overwritten with the result. A is a target to find a minimum value, and OUT is a place to set the index. Constructor (ArgMin-Node out-size) out-size the reducted shape of out . Backward \u2705 Already defined. ((self dout da do) (declare (ignore dout da do)) (values nil nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] MATMULNODE (A[~ I J] B[~ J K] C[~ I K] -> C[~ I K]) Description MatmulNode Computes a matrix multiplication of given A and B, set the result to C. C \u2190 g e m m ( 1.0 , A , B , 0.0 , C ) C\\gets{gemm(1.0, A, B, 0.0, C)} C \u2190 g e mm ( 1.0 , A , B , 0.0 , C ) Constructor (MatMulNode dtype &key transpose-a transpose-b) dtype dtype to use. transpose-a transpose-b set t to call with transposing (reversing the last two axes the matrix). Backward \u2705 Already defined. ((self dout da db do) (declare (ignore do)) (values (!matmul dout (!t db)) (!matmul (!t da) dout) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] WHERE-OPERATION-NODE (A[~] OUT[~] -> OUT[~]) Description Where-Operation-Node is a node which set true-then , if the result of calling condition with each element of A, is t and if it is NIL, set false-then at corresponding position. Constructor (Where-Operation-Node condition true-then false-then) true-then and false-then is a number. condition a single argument function, each element of A is argument. (e.g.: this could be #'evenp #'oddp etc...) Backward \u2705 Already defined. ((self dout da do) (declare (ignore dout da do)) ;; todo: :no-grad t (values nil nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] COMPARE-OPERATION-NODE (A[~] B[~] OUT[~] -> OUT[~]) Description Compare-Operation-Node is a node which set true-then , if the result of calling condition with each element of A and B, if it is NIl set false-then at corresponding position. Constructor (Compare-Operation-Node condition true-then false-then) true-then and false-then is a number. condition a two arguments function, each element of A and B is argument. (e.g.: this could be #'> or #'< etc...) Backward \u2705 Already defined. ((self dout da db do) (declare (ignore dout da db do)) ;; todo: :no-grad t (values nil nil nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"[Nodes] cl-waffe2/base-impl"},{"location":"base-impl-nodes/#standard-nodes","text":"","title":"Standard Nodes"},{"location":"base-impl-nodes/#node-addnode","text":"(A[~] B[~] -> A[~])","title":"[node] ADDNODE"},{"location":"base-impl-nodes/#description","text":"AddNode is a node which computes following operation element-wise. Let X and Y be a given arguments and both are matrix. X \u2190 X + Y X\\gets{X + Y} X \u2190 X + Y","title":"Description"},{"location":"base-impl-nodes/#constructor","text":"(AddNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dx dy)) (values dout dout)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-subnode","text":"(A[~] B[~] -> A[~])","title":"[node] SUBNODE"},{"location":"base-impl-nodes/#description_1","text":"SubNode is a node which computes following operation element-wise. Let X and Y be a given arguments and both are matrix. X \u2190 X \u2212 Y X\\gets{X - Y} X \u2190 X \u2212 Y","title":"Description"},{"location":"base-impl-nodes/#constructor_1","text":"(SubNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward_1","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dx dy)) (values dout (!mul -1 dout))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-mulnode","text":"(A[~] B[~] -> A[~])","title":"[node] MULNODE"},{"location":"base-impl-nodes/#description_2","text":"MulNode is a node which computes following operation element-wise. Let X and Y be a given arguments and both are matrix. X \u2190 X \u2217 Y X\\gets{X * Y} X \u2190 X \u2217 Y","title":"Description"},{"location":"base-impl-nodes/#constructor_2","text":"(MulNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward_2","text":"\u2705 Already defined. ((self dout dx dy) (values (!mul dout dy) (!mul dout dx))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-divnode","text":"(A[~] B[~] -> A[~])","title":"[node] DIVNODE"},{"location":"base-impl-nodes/#description_3","text":"DivNode is a node which computes following operation element-wise. Let X and Y be a given arguments and both are matrix. X \u2190 X / Y X\\gets{X / Y} X \u2190 X / Y","title":"Description"},{"location":"base-impl-nodes/#constructor_3","text":"(DivNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward_3","text":"\u2705 Already defined. ((self dout dx dy) (values (!div dout dy) (!div (!mul dx (!mul -1 dout)) (!square dy)))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-inversetensornode","text":"(A[~] -> A[~])","title":"[node] INVERSETENSORNODE"},{"location":"base-impl-nodes/#description_4","text":"InverseTensorNode is a node which computes following operation element-wise A \u2190 1 / A A\\gets{1 / A} A \u2190 1/ A","title":"Description"},{"location":"base-impl-nodes/#constructor_4","text":"(InverseTensorNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward_4","text":"\u2705 Already defined. ((self dout dx) (values (!div (!mul -1 dout) (!square dx)))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalaradd","text":"(A[~] SCALAR[SCAL] -> A[~] WHERE SCAL = 1)","title":"[node] SCALARADD"},{"location":"base-impl-nodes/#description_5","text":"ScalarAdd is a node which computes following operation element-wise. Let X be a given matrix and S be a given scalar. X \u2190 X + s c a l a r X\\gets{X + scalar} X \u2190 X + sc a l a r","title":"Description"},{"location":"base-impl-nodes/#constructor_5","text":"(ScalarAdd dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward_5","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dx dy)) (values dout (->scal (!mean dout)))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalarsub","text":"(A[~] SCALAR[SCAL] -> A[~] WHERE SCAL = 1)","title":"[node] SCALARSUB"},{"location":"base-impl-nodes/#description_6","text":"ScalarSub is a node which computes following operation element-wise. Let X be a given matrix and S be a given scalar. X \u2190 X \u2212 s c a l a r X\\gets{X - scalar} X \u2190 X \u2212 sc a l a r","title":"Description"},{"location":"base-impl-nodes/#constructor_6","text":"(ScalarSub dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward_6","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dx dy)) (values dout (->scal (!mul -1.0 (!mean dout))))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalarmul","text":"(A[~] SCALAR[SCAL] -> A[~] WHERE SCAL = 1)","title":"[node] SCALARMUL"},{"location":"base-impl-nodes/#description_7","text":"ScalarMul is a node which computes following operation element-wise. Let X be a given matrix and S be a given scalar. X \u2190 X \u2217 s c a l a r X\\gets{X * scalar} X \u2190 X \u2217 sc a l a r","title":"Description"},{"location":"base-impl-nodes/#constructor_7","text":"(ScalarMul dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward_7","text":"\u2705 Already defined. ((self dout dx dy) (values (!mul dout dy) (->scal (!mean (!mul dx dout))))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalardiv","text":"(A[~] SCALAR[SCAL] -> A[~] WHERE SCAL = 1)","title":"[node] SCALARDIV"},{"location":"base-impl-nodes/#description_8","text":"ScalarDiv is a node which computes following operation element-wise. Let X be a given matrix and S be a given scalar. X \u2190 X / s c a l a r X\\gets{X / scalar} X \u2190 X / sc a l a r","title":"Description"},{"location":"base-impl-nodes/#constructor_8","text":"(ScalarDiv dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward_8","text":"\u2705 Already defined. ((self dout dx dy) (values (!div dout dy) (->scal (!mean (!div (!mul dx (!mul -1 dout)) (!square dy)))))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-movetensornode","text":"(A[~] B[~] -> A[~])","title":"[node] MOVETENSORNODE"},{"location":"base-impl-nodes/#description_9","text":"Moves all the visible elements of B into visible areas of A . A \u2190 B A\\gets{B} A \u2190 B","title":"Description"},{"location":"base-impl-nodes/#behaviour","text":"All cl-waffe2 operations follow this rule: Make a copy for now, disable later . (e.g.: the function (!add x y) makes an copy of x and y for now, but this copy operation is ignored, if they're concluded not to be needed, by tracing computation node.) In order to disable a useless copy operations, MoveTensorNode must follow this behaviour: Reading (movetensor-ignore-me self) in runtime, the forward makes a copy of given tensor only after the slot is nil . Otherwise, return B Don't worry the allocation won't be done until (tensor-vec A) is called. For practical example, my impls ( ./source/backends/lisp/arithmetic.lisp for example) would be helpful!.","title":"Behaviour"},{"location":"base-impl-nodes/#constructor_9","text":"(MoveTensorNode dtype) dtype dtype to use.","title":"Constructor"},{"location":"base-impl-nodes/#backward_9","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dx)) (let ((dy-out (if (and (eql (tensor-attribute dy) chain) (movetensor-ignore-me self)) dout (!copy dout force t)))) (values nil dy-out))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-absnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] ABSNODE"},{"location":"base-impl-nodes/#description_10","text":"The node ABSNODE takes X as an argument, applying a abs function into each element and writes the result into out. O U T \u2190 a b s ( X ) OUT\\gets{abs(X)} O U T \u2190 ab s ( X ) save-for-backward: (T NIL) See also: SCALAR-ABSNODE !abs","title":"Description"},{"location":"base-impl-nodes/#backward_10","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dy)) (values (!mul dout (!sign dx)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-absnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-ABSNODE"},{"location":"base-impl-nodes/#description_11","text":"The node SCALAR-ABSNODE takes scalar X as an argument, applying a abs function into each element and writes the result into out. o u t \u2190 a b s ( x ) out\\gets{abs(x)} o u t \u2190 ab s ( x ) save-for-backward: (T NIL) See also: ABSNODE !abs","title":"Description"},{"location":"base-impl-nodes/#backward_11","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dy)) (values (!mul dout (!sign dx)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-signnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SIGNNODE"},{"location":"base-impl-nodes/#description_12","text":"The node SIGNNODE takes X as an argument, applying a sign function into each element and writes the result into out. O U T \u2190 s i g n ( X ) OUT\\gets{sign(X)} O U T \u2190 s i g n ( X ) save-for-backward: (T NIL) See also: SCALAR-SIGNNODE !sign","title":"Description"},{"location":"base-impl-nodes/#backward_12","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dout dy)) (values (!mul dx 0) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-signnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-SIGNNODE"},{"location":"base-impl-nodes/#description_13","text":"The node SCALAR-SIGNNODE takes scalar X as an argument, applying a sign function into each element and writes the result into out. o u t \u2190 s i g n ( x ) out\\gets{sign(x)} o u t \u2190 s i g n ( x ) save-for-backward: (T NIL) See also: SIGNNODE !sign","title":"Description"},{"location":"base-impl-nodes/#backward_13","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dout dy)) (values (!mul dx 0) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-sqrtnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SQRTNODE"},{"location":"base-impl-nodes/#description_14","text":"The node SQRTNODE takes X as an argument, applying a sqrt function into each element and writes the result into out. O U T \u2190 s q r t ( X ) OUT\\gets{sqrt(X)} O U T \u2190 s q r t ( X ) save-for-backward: (T NIL) See also: SCALAR-SQRTNODE !sqrt","title":"Description"},{"location":"base-impl-nodes/#backward_14","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dy)) (values (!mul dout (!div 1 dx)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-sqrtnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-SQRTNODE"},{"location":"base-impl-nodes/#description_15","text":"The node SCALAR-SQRTNODE takes scalar X as an argument, applying a sqrt function into each element and writes the result into out. o u t \u2190 s q r t ( x ) out\\gets{sqrt(x)} o u t \u2190 s q r t ( x ) save-for-backward: (T NIL) See also: SQRTNODE !sqrt","title":"Description"},{"location":"base-impl-nodes/#backward_15","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dy)) (values (!mul dout (!div 1 dx)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-squarenode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SQUARENODE"},{"location":"base-impl-nodes/#description_16","text":"The node SQUARENODE takes X as an argument, applying a square function into each element and writes the result into out. O U T \u2190 s q u a r e ( X ) OUT\\gets{square(X)} O U T \u2190 s q u a re ( X ) save-for-backward: (T NIL) See also: SCALAR-SQUARENODE !square","title":"Description"},{"location":"base-impl-nodes/#backward_16","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout x) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-squarenode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-SQUARENODE"},{"location":"base-impl-nodes/#description_17","text":"The node SCALAR-SQUARENODE takes scalar X as an argument, applying a square function into each element and writes the result into out. o u t \u2190 s q u a r e ( x ) out\\gets{square(x)} o u t \u2190 s q u a re ( x ) save-for-backward: (T NIL) See also: SQUARENODE !square","title":"Description"},{"location":"base-impl-nodes/#backward_17","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout x) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-sinnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SINNODE"},{"location":"base-impl-nodes/#description_18","text":"The node SINNODE takes X as an argument, applying a sin function into each element and writes the result into out. O U T \u2190 s i n ( X ) OUT\\gets{sin(X)} O U T \u2190 s in ( X ) save-for-backward: (T NIL) See also: SCALAR-SINNODE !sin","title":"Description"},{"location":"base-impl-nodes/#backward_18","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!cos x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-sinnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-SINNODE"},{"location":"base-impl-nodes/#description_19","text":"The node SCALAR-SINNODE takes scalar X as an argument, applying a sin function into each element and writes the result into out. o u t \u2190 s i n ( x ) out\\gets{sin(x)} o u t \u2190 s in ( x ) save-for-backward: (T NIL) See also: SINNODE !sin","title":"Description"},{"location":"base-impl-nodes/#backward_19","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!cos x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-cosnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] COSNODE"},{"location":"base-impl-nodes/#description_20","text":"The node COSNODE takes X as an argument, applying a cos function into each element and writes the result into out. O U T \u2190 c o s ( X ) OUT\\gets{cos(X)} O U T \u2190 cos ( X ) save-for-backward: (T NIL) See also: SCALAR-COSNODE !cos","title":"Description"},{"location":"base-impl-nodes/#backward_20","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!mul -1 (!sin x))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-cosnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-COSNODE"},{"location":"base-impl-nodes/#description_21","text":"The node SCALAR-COSNODE takes scalar X as an argument, applying a cos function into each element and writes the result into out. o u t \u2190 c o s ( x ) out\\gets{cos(x)} o u t \u2190 cos ( x ) save-for-backward: (T NIL) See also: COSNODE !cos","title":"Description"},{"location":"base-impl-nodes/#backward_21","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!mul -1 (!sin x))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-tannode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] TANNODE"},{"location":"base-impl-nodes/#description_22","text":"The node TANNODE takes X as an argument, applying a tan function into each element and writes the result into out. O U T \u2190 t a n ( X ) OUT\\gets{tan(X)} O U T \u2190 t an ( X ) save-for-backward: (T NIL) See also: SCALAR-TANNODE !tan","title":"Description"},{"location":"base-impl-nodes/#backward_22","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul (!cos x) (!cos x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-tannode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-TANNODE"},{"location":"base-impl-nodes/#description_23","text":"The node SCALAR-TANNODE takes scalar X as an argument, applying a tan function into each element and writes the result into out. o u t \u2190 t a n ( x ) out\\gets{tan(x)} o u t \u2190 t an ( x ) save-for-backward: (T NIL) See also: TANNODE !tan","title":"Description"},{"location":"base-impl-nodes/#backward_23","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul (!cos x) (!cos x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-asinnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] ASINNODE"},{"location":"base-impl-nodes/#description_24","text":"The node ASINNODE takes X as an argument, applying a asin function into each element and writes the result into out. O U T \u2190 a s i n ( X ) OUT\\gets{asin(X)} O U T \u2190 a s in ( X ) save-for-backward: (T NIL) See also: SCALAR-ASINNODE !asin","title":"Description"},{"location":"base-impl-nodes/#backward_24","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!sqrt (!sub 1 (!square x))))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-asinnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-ASINNODE"},{"location":"base-impl-nodes/#description_25","text":"The node SCALAR-ASINNODE takes scalar X as an argument, applying a asin function into each element and writes the result into out. o u t \u2190 a s i n ( x ) out\\gets{asin(x)} o u t \u2190 a s in ( x ) save-for-backward: (T NIL) See also: ASINNODE !asin","title":"Description"},{"location":"base-impl-nodes/#backward_25","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!sqrt (!sub 1 (!square x))))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-acosnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] ACOSNODE"},{"location":"base-impl-nodes/#description_26","text":"The node ACOSNODE takes X as an argument, applying a acos function into each element and writes the result into out. O U T \u2190 a c o s ( X ) OUT\\gets{acos(X)} O U T \u2190 a cos ( X ) save-for-backward: (T NIL) See also: SCALAR-ACOSNODE !acos","title":"Description"},{"location":"base-impl-nodes/#backward_26","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div -1 (!sqrt (!sub 1 (!square x))))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-acosnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-ACOSNODE"},{"location":"base-impl-nodes/#description_27","text":"The node SCALAR-ACOSNODE takes scalar X as an argument, applying a acos function into each element and writes the result into out. o u t \u2190 a c o s ( x ) out\\gets{acos(x)} o u t \u2190 a cos ( x ) save-for-backward: (T NIL) See also: ACOSNODE !acos","title":"Description"},{"location":"base-impl-nodes/#backward_27","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div -1 (!sqrt (!sub 1 (!square x))))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-atannode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] ATANNODE"},{"location":"base-impl-nodes/#description_28","text":"The node ATANNODE takes X as an argument, applying a atan function into each element and writes the result into out. O U T \u2190 a t a n ( X ) OUT\\gets{atan(X)} O U T \u2190 a t an ( X ) save-for-backward: (T NIL) See also: SCALAR-ATANNODE !atan","title":"Description"},{"location":"base-impl-nodes/#backward_28","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!add 1 (!square x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-atannode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-ATANNODE"},{"location":"base-impl-nodes/#description_29","text":"The node SCALAR-ATANNODE takes scalar X as an argument, applying a atan function into each element and writes the result into out. o u t \u2190 a t a n ( x ) out\\gets{atan(x)} o u t \u2190 a t an ( x ) save-for-backward: (T NIL) See also: ATANNODE !atan","title":"Description"},{"location":"base-impl-nodes/#backward_29","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!add 1 (!square x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-sinhnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SINHNODE"},{"location":"base-impl-nodes/#description_30","text":"The node SINHNODE takes X as an argument, applying a sinh function into each element and writes the result into out. O U T \u2190 s i n h ( X ) OUT\\gets{sinh(X)} O U T \u2190 s inh ( X ) save-for-backward: (T NIL) See also: SCALAR-SINHNODE !sinh","title":"Description"},{"location":"base-impl-nodes/#backward_30","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!cosh x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-sinhnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-SINHNODE"},{"location":"base-impl-nodes/#description_31","text":"The node SCALAR-SINHNODE takes scalar X as an argument, applying a sinh function into each element and writes the result into out. o u t \u2190 s i n h ( x ) out\\gets{sinh(x)} o u t \u2190 s inh ( x ) save-for-backward: (T NIL) See also: SINHNODE !sinh","title":"Description"},{"location":"base-impl-nodes/#backward_31","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!cosh x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-coshnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] COSHNODE"},{"location":"base-impl-nodes/#description_32","text":"The node COSHNODE takes X as an argument, applying a cosh function into each element and writes the result into out. O U T \u2190 c o s h ( X ) OUT\\gets{cosh(X)} O U T \u2190 cos h ( X ) save-for-backward: (T NIL) See also: SCALAR-COSHNODE !cosh","title":"Description"},{"location":"base-impl-nodes/#backward_32","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!mul -1 (!sinh x))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-coshnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-COSHNODE"},{"location":"base-impl-nodes/#description_33","text":"The node SCALAR-COSHNODE takes scalar X as an argument, applying a cosh function into each element and writes the result into out. o u t \u2190 c o s h ( x ) out\\gets{cosh(x)} o u t \u2190 cos h ( x ) save-for-backward: (T NIL) See also: COSHNODE !cosh","title":"Description"},{"location":"base-impl-nodes/#backward_33","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!mul -1 (!sinh x))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-tanhnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] TANHNODE"},{"location":"base-impl-nodes/#description_34","text":"The node TANHNODE takes X as an argument, applying a tanh function into each element and writes the result into out. O U T \u2190 t a n h ( X ) OUT\\gets{tanh(X)} O U T \u2190 t anh ( X ) save-for-backward: (T NIL) See also: SCALAR-TANHNODE !tanh","title":"Description"},{"location":"base-impl-nodes/#backward_34","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul (!cosh x) (!cosh x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-tanhnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-TANHNODE"},{"location":"base-impl-nodes/#description_35","text":"The node SCALAR-TANHNODE takes scalar X as an argument, applying a tanh function into each element and writes the result into out. o u t \u2190 t a n h ( x ) out\\gets{tanh(x)} o u t \u2190 t anh ( x ) save-for-backward: (T NIL) See also: TANHNODE !tanh","title":"Description"},{"location":"base-impl-nodes/#backward_35","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul (!cosh x) (!cosh x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-asinhnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] ASINHNODE"},{"location":"base-impl-nodes/#description_36","text":"The node ASINHNODE takes X as an argument, applying a asinh function into each element and writes the result into out. O U T \u2190 a s i n h ( X ) OUT\\gets{asinh(X)} O U T \u2190 a s inh ( X ) save-for-backward: NIL See also: SCALAR-ASINHNODE !asinh","title":"Description"},{"location":"base-impl-nodes/#backward_36","text":"\u274c Undefined. (To make it differentiable, must be defined with define-impl macro.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-asinhnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-ASINHNODE"},{"location":"base-impl-nodes/#description_37","text":"The node SCALAR-ASINHNODE takes scalar X as an argument, applying a asinh function into each element and writes the result into out. o u t \u2190 a s i n h ( x ) out\\gets{asinh(x)} o u t \u2190 a s inh ( x ) save-for-backward: NIL See also: ASINHNODE !asinh","title":"Description"},{"location":"base-impl-nodes/#backward_37","text":"\u274c Undefined. (To make it differentiable, must be defined with define-impl macro.)","title":"Backward"},{"location":"base-impl-nodes/#node-acoshnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] ACOSHNODE"},{"location":"base-impl-nodes/#description_38","text":"The node ACOSHNODE takes X as an argument, applying a acosh function into each element and writes the result into out. O U T \u2190 a c o s h ( X ) OUT\\gets{acosh(X)} O U T \u2190 a cos h ( X ) save-for-backward: NIL See also: SCALAR-ACOSHNODE !acosh","title":"Description"},{"location":"base-impl-nodes/#backward_38","text":"\u274c Undefined. (To make it differentiable, must be defined with define-impl macro.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-acoshnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-ACOSHNODE"},{"location":"base-impl-nodes/#description_39","text":"The node SCALAR-ACOSHNODE takes scalar X as an argument, applying a acosh function into each element and writes the result into out. o u t \u2190 a c o s h ( x ) out\\gets{acosh(x)} o u t \u2190 a cos h ( x ) save-for-backward: NIL See also: ACOSHNODE !acosh","title":"Description"},{"location":"base-impl-nodes/#backward_39","text":"\u274c Undefined. (To make it differentiable, must be defined with define-impl macro.)","title":"Backward"},{"location":"base-impl-nodes/#node-atanhnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] ATANHNODE"},{"location":"base-impl-nodes/#description_40","text":"The node ATANHNODE takes X as an argument, applying a atanh function into each element and writes the result into out. O U T \u2190 a t a n h ( X ) OUT\\gets{atanh(X)} O U T \u2190 a t anh ( X ) save-for-backward: NIL See also: SCALAR-ATANHNODE !atanh","title":"Description"},{"location":"base-impl-nodes/#backward_40","text":"\u274c Undefined. (To make it differentiable, must be defined with define-impl macro.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-atanhnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-ATANHNODE"},{"location":"base-impl-nodes/#description_41","text":"The node SCALAR-ATANHNODE takes scalar X as an argument, applying a atanh function into each element and writes the result into out. o u t \u2190 a t a n h ( x ) out\\gets{atanh(x)} o u t \u2190 a t anh ( x ) save-for-backward: NIL See also: ATANHNODE !atanh","title":"Description"},{"location":"base-impl-nodes/#backward_41","text":"\u274c Undefined. (To make it differentiable, must be defined with define-impl macro.)","title":"Backward"},{"location":"base-impl-nodes/#node-expnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] EXPNODE"},{"location":"base-impl-nodes/#description_42","text":"The node EXPNODE takes X as an argument, applying a exp function into each element and writes the result into out. O U T \u2190 e x p ( X ) OUT\\gets{exp(X)} O U T \u2190 e x p ( X ) save-for-backward: (T NIL) See also: SCALAR-EXPNODE !exp","title":"Description"},{"location":"base-impl-nodes/#backward_42","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!exp x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-expnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-EXPNODE"},{"location":"base-impl-nodes/#description_43","text":"The node SCALAR-EXPNODE takes scalar X as an argument, applying a exp function into each element and writes the result into out. o u t \u2190 e x p ( x ) out\\gets{exp(x)} o u t \u2190 e x p ( x ) save-for-backward: (T NIL) See also: EXPNODE !exp","title":"Description"},{"location":"base-impl-nodes/#backward_43","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!exp x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-log2node","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] LOG2NODE"},{"location":"base-impl-nodes/#description_44","text":"The node LOG2NODE takes X as an argument, applying a log2 function into each element and writes the result into out. O U T \u2190 l o g 2 ( X ) OUT\\gets{log2(X)} O U T \u2190 l o g 2 ( X ) save-for-backward: (T NIL) See also: SCALAR-LOG2NODE !log2","title":"Description"},{"location":"base-impl-nodes/#backward_44","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul x (log 2)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-log2node","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-LOG2NODE"},{"location":"base-impl-nodes/#description_45","text":"The node SCALAR-LOG2NODE takes scalar X as an argument, applying a log2 function into each element and writes the result into out. o u t \u2190 l o g 2 ( x ) out\\gets{log2(x)} o u t \u2190 l o g 2 ( x ) save-for-backward: (T NIL) See also: LOG2NODE !log2","title":"Description"},{"location":"base-impl-nodes/#backward_45","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul x (log 2)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-log10node","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] LOG10NODE"},{"location":"base-impl-nodes/#description_46","text":"The node LOG10NODE takes X as an argument, applying a log10 function into each element and writes the result into out. O U T \u2190 l o g 10 ( X ) OUT\\gets{log10(X)} O U T \u2190 l o g 10 ( X ) save-for-backward: (T NIL) See also: SCALAR-LOG10NODE !log10","title":"Description"},{"location":"base-impl-nodes/#backward_46","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul x (log 10)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-log10node","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-LOG10NODE"},{"location":"base-impl-nodes/#description_47","text":"The node SCALAR-LOG10NODE takes scalar X as an argument, applying a log10 function into each element and writes the result into out. o u t \u2190 l o g 10 ( x ) out\\gets{log10(x)} o u t \u2190 l o g 10 ( x ) save-for-backward: (T NIL) See also: LOG10NODE !log10","title":"Description"},{"location":"base-impl-nodes/#backward_47","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul x (log 10)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-logenode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] LOGENODE"},{"location":"base-impl-nodes/#description_48","text":"The node LOGENODE takes X as an argument, applying a loge function into each element and writes the result into out. O U T \u2190 l o g e ( X ) OUT\\gets{loge(X)} O U T \u2190 l o g e ( X ) save-for-backward: (T NIL) See also: SCALAR-LOGENODE !loge","title":"Description"},{"location":"base-impl-nodes/#backward_48","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-logenode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-LOGENODE"},{"location":"base-impl-nodes/#description_49","text":"The node SCALAR-LOGENODE takes scalar X as an argument, applying a loge function into each element and writes the result into out. o u t \u2190 l o g e ( x ) out\\gets{loge(x)} o u t \u2190 l o g e ( x ) save-for-backward: (T NIL) See also: LOGENODE !loge","title":"Description"},{"location":"base-impl-nodes/#backward_49","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-lazytransposenode","text":"(A[~ I J] -> A[~ J I])","title":"[node] LAZYTRANSPOSENODE"},{"location":"base-impl-nodes/#description_50","text":"LazyTransposeNode is the matmul-dedicated node which supplies the lazy-transpose feature. Internally, This Node Returns The Given A itself but taking transpose of A's shape. If the computation node is like: [LazyTransposeNode] -> [MatmulNode], then transpose will be done with NO overhead.","title":"Description"},{"location":"base-impl-nodes/#backward_50","text":"\u2705 Already defined. ((self dout dx) (declare (ignore dx)) (values (!t dout))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-argmax-node","text":"(A[~] OUT[OUT-SIZE] -> OUT[OUT-SIZE])","title":"[node] ARGMAX-NODE"},{"location":"base-impl-nodes/#description_51","text":"ArgMax-Node finds a maximum value of all elements in A. OUT is overwritten with the result. A is a target to find a maximum value, and OUT is a place to set the index.","title":"Description"},{"location":"base-impl-nodes/#constructor_10","text":"(ArgMax-Node out-size) out-size the reducted shape of out .","title":"Constructor"},{"location":"base-impl-nodes/#backward_51","text":"\u2705 Already defined. ((self dout da do) (declare (ignore dout da do)) (values nil nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-argmin-node","text":"(A[~] OUT[OUT-SIZE] -> OUT[OUT-SIZE])","title":"[node] ARGMIN-NODE"},{"location":"base-impl-nodes/#description_52","text":"ArgMin-Node finds a minimum value of all elements in A. OUT is overwritten with the result. A is a target to find a minimum value, and OUT is a place to set the index.","title":"Description"},{"location":"base-impl-nodes/#constructor_11","text":"(ArgMin-Node out-size) out-size the reducted shape of out .","title":"Constructor"},{"location":"base-impl-nodes/#backward_52","text":"\u2705 Already defined. ((self dout da do) (declare (ignore dout da do)) (values nil nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-matmulnode","text":"(A[~ I J] B[~ J K] C[~ I K] -> C[~ I K])","title":"[node] MATMULNODE"},{"location":"base-impl-nodes/#description_53","text":"MatmulNode Computes a matrix multiplication of given A and B, set the result to C. C \u2190 g e m m ( 1.0 , A , B , 0.0 , C ) C\\gets{gemm(1.0, A, B, 0.0, C)} C \u2190 g e mm ( 1.0 , A , B , 0.0 , C )","title":"Description"},{"location":"base-impl-nodes/#constructor_12","text":"(MatMulNode dtype &key transpose-a transpose-b) dtype dtype to use. transpose-a transpose-b set t to call with transposing (reversing the last two axes the matrix).","title":"Constructor"},{"location":"base-impl-nodes/#backward_53","text":"\u2705 Already defined. ((self dout da db do) (declare (ignore do)) (values (!matmul dout (!t db)) (!matmul (!t da) dout) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-where-operation-node","text":"(A[~] OUT[~] -> OUT[~])","title":"[node] WHERE-OPERATION-NODE"},{"location":"base-impl-nodes/#description_54","text":"Where-Operation-Node is a node which set true-then , if the result of calling condition with each element of A, is t and if it is NIL, set false-then at corresponding position.","title":"Description"},{"location":"base-impl-nodes/#constructor_13","text":"(Where-Operation-Node condition true-then false-then) true-then and false-then is a number. condition a single argument function, each element of A is argument. (e.g.: this could be #'evenp #'oddp etc...)","title":"Constructor"},{"location":"base-impl-nodes/#backward_54","text":"\u2705 Already defined. ((self dout da do) (declare (ignore dout da do)) ;; todo: :no-grad t (values nil nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-compare-operation-node","text":"(A[~] B[~] OUT[~] -> OUT[~])","title":"[node] COMPARE-OPERATION-NODE"},{"location":"base-impl-nodes/#description_55","text":"Compare-Operation-Node is a node which set true-then , if the result of calling condition with each element of A and B, if it is NIl set false-then at corresponding position.","title":"Description"},{"location":"base-impl-nodes/#constructor_14","text":"(Compare-Operation-Node condition true-then false-then) true-then and false-then is a number. condition a two arguments function, each element of A and B is argument. (e.g.: this could be #'> or #'< etc...)","title":"Constructor"},{"location":"base-impl-nodes/#backward_55","text":"\u2705 Already defined. ((self dout da db do) (declare (ignore dout da db do)) ;; todo: :no-grad t (values nil nil nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl/","text":".katex img { object-fit: fill; padding: unset; display: block; position: absolute; width: 100%; height: inherit; } Basic APIs [function] !matrix-add (!matrix-add x y) The function !matrix-add calls ADDNODE and adds X and Y element-wise, returning a new tensor. X c o p y \u2190 X + Y X_{copy}\\gets{X + Y} X co p y \u200b \u2190 X + Y Inputs X and Y must be a AbstractTensor (not a ScalarTensor), with the same shape. SideEffects None. [function] !matrix-sub (!matrix-sub x y) The function !matrix-sub calls SUBNODE and substracts X by Y element-wise, returning a new tensor. X c o p y \u2190 X \u2212 Y X_{copy}\\gets{X - Y} X co p y \u200b \u2190 X \u2212 Y Inputs X and Y must be a AbstractTensor (not a ScalarTensor), with the same shape. SideEffects None. [function] !matrix-mul (!matrix-mul x y) The function !matrix-mul calls MULNODE and multiplies X and Y element-wise, returning a new tensor. X c o p y \u2190 X \u2217 Y X_{copy}\\gets{X * Y} X co p y \u200b \u2190 X \u2217 Y Inputs X and Y must be a AbstractTensor (not a ScalarTensor), with the same shape. SideEffects None. [function] !matrix-div (!matrix-div x y) The function !matrix-div calls DIVNODE and divides X by Y element-wise, returning a new tensor. X c o p y \u2190 X / Y X_{copy}\\gets{X / Y} X co p y \u200b \u2190 X / Y Inputs X and Y must be a AbstractTensor (not a ScalarTensor), with the same shape. SideEffects None. [function] !inverse (!inverse tensor) The function !inverse calls InverseTensorNode , and finds the inverse of the received Tensor/Scalar, returning a new tensor. X c o p y \u2190 1 / X X_{copy}\\gets{1 / X} X co p y \u200b \u2190 1/ X Inputs tensor[ScalarTensor/AbstractTensor/Number] [function] !scalar-add (!scalar-add scalar x) The function !SCALAR-ADD computes following operation with calling SCALARADD , returning a new tensor. X c o p y \u2190 X + s c a l a r X_{copy}\\gets{X + scalar} X co p y \u200b \u2190 X + sc a l a r Inputs scalar could be one of ScalarTensor or number tensor AbstractTensor (should not be a scalar) [function] !scalar-sub (!scalar-sub scalar x) The function !SCALAR-SUB computes following operation with calling SCALARSUB , returning a new tensor. X c o p y \u2190 X \u2212 s c a l a r X_{copy}\\gets{X - scalar} X co p y \u200b \u2190 X \u2212 sc a l a r Inputs scalar could be one of ScalarTensor or number tensor AbstractTensor (should not be a scalar) [function] !scalar-mul (!scalar-mul scalar x) The function !SCALAR-MUL computes following operation with calling SCALARMUL , returning a new tensor. X c o p y \u2190 X \u2217 s c a l a r X_{copy}\\gets{X * scalar} X co p y \u200b \u2190 X \u2217 sc a l a r Inputs scalar could be one of ScalarTensor or number tensor AbstractTensor (should not be a scalar) [function] !scalar-div (!scalar-div scalar x) The function !SCALAR-DIV computes following operation with calling SCALARDIV , returning a new tensor. X c o p y \u2190 X / s c a l a r X_{copy}\\gets{X / scalar} X co p y \u200b \u2190 X / sc a l a r Inputs scalar could be one of ScalarTensor or number tensor AbstractTensor (should not be a scalar) [function] !sas-add The function !sas-add provides differentiable scalar-and-scalar operation. Calling a node SCALARANDSCALARADD , the function performs following operation: x c o p y \u2190 x + y x_{copy}\\gets{x + y} x co p y \u200b \u2190 x + y Inputs x y could be one of: ScalarTensor or number [function] !sas-sub The function !sas-sub provides differentiable scalar-and-scalar operation. Calling a node SCALARANDSCALARSUB , the function performs following operation: x c o p y \u2190 x \u2212 y x_{copy}\\gets{x - y} x co p y \u200b \u2190 x \u2212 y Inputs x y could be one of: ScalarTensor or number [function] !sas-mul The function !sas-mul provides differentiable scalar-and-scalar operation. Calling a node SCALARANDSCALARMUL , the function performs following operation: x c o p y \u2190 x \u2217 y x_{copy}\\gets{x * y} x co p y \u200b \u2190 x \u2217 y Inputs x y could be one of: ScalarTensor or number [function] !sas-div The function !sas-div provides differentiable scalar-and-scalar operation. Calling a node SCALARANDSCALARDIV , the function performs following operation: x c o p y \u2190 x / y x_{copy}\\gets{x / y} x co p y \u200b \u2190 x / y Inputs x y could be one of: ScalarTensor or number [function] !add (!add x y) The function provides general-purpose arithmetic operation. Given type of tensors, this function dispatches these functions automatically: !sas-add !scalar-add !matrix-add Inputs x y could be one of AbstractTensor number ScalarTensor SideEffects None [function] !sub (!sub x y) The function provides general-purpose arithmetic operation. Given type of tensors, this function dispatches these functions automatically: !sas-sub !scalar-sub !matrix-sub Inputs x y could be one of AbstractTensor number ScalarTensor SideEffects None [function] !mul (!mul x y) The function provides general-purpose arithmetic operation. Given type of tensors, this function dispatches these functions automatically: !sas-mul !scalar-mul !matrix-mul Inputs x y could be one of AbstractTensor number ScalarTensor SideEffects None [function] !div (!div x y) The function provides general-purpose arithmetic operation. Given type of tensors, this function dispatches these functions automatically: !sas-div !scalar-div !matrix-div Inputs x y could be one of AbstractTensor number ScalarTensor SideEffects None [function] !move (!move place tensor) A \u2190 B A\\gets{B} A \u2190 B The function !move returns a node which moves tensor's visible elements into place's visible elements. nodes one of: MoveTensorNode ScalarTensorNode Inputs place[AbstractTensor] tensor to be overwritten. tensor[AbstractTensor] tensor to be referred. force[boolean] If t, the pruning of operation by cl-waffe2 will never done. Output Unevaluated Copied Tensor. [function] !copy (!copy tensor) The function !copy returns a node which makes a copy the tensor's visible area. Note that: the function !copy never creates a new tensor larger than (tensor-vec tensor) has, (i.e.: copying broadcasted tensor will return broadcasted and copied tensor). !copy is used to make a cache before calling destructive operation to avoid side effects, therefore if the copy is included to be useless by compiler, this operations is being ignored without changing its behaviour. And this is why !copy returns InputTensor , not AbstractTensor . See also: !copy-force never being ignored by compiler, and broadcasted axes will be padded. Input: Tensor[AbstractTensor] Output: Tensor[AbstractTensor] [function] !copy-force (!copy-force (tensor)) The function !copy-force returns a node which copies the given tensor forcibly while the function !copy sometimes ignored. This function is also used to adjust memory alignment of tensor. [function] !reshape (!reshape tensor &rest shapes) Changes the shape of given tensor. Before and after the operation, the total elements of tensors must correspond. Inputs tensor AbstractTensor but must not includes symbol in the shape. shapes could be one of: fixnum t . t can be used at one, but the value of t is automatically inferenced. [function] !view (!view tensor &rest subscripts) The function !view returns a tensor which is applied lazy-evaluated view. For Example, let A be a 4x8 Matrix, and we gonna create a view of A that portrays A[:, 2] . (!view A 2 t) A B 0 ++++++++ -------- 1 ++++++++ -------- 2 ++++++++ -> [make a view] -> ++++++++ 3 ++++++++ -------- Here, A and B shares the pointer. Calling (shape B) returns (1 8) . Subscripts Subscripts are following: t all elements in the axis. fixnum points out the specified index. (start end) slices the area. (start end step-by) slices the area by step-by . step-by can be a negative-fixnum. (Not tested) (:broadcast N-times) broadcasts the axis for N-times, the axis to be broadcasted must be 1 or broadcasted-axis. (:tflist ...) (TODO) (:indices ...) (TODO) Return (values sliced-tensor broadcast-reverser) Tips: Applying !view again to the returned sliced-tensor with broadcast-reverser will remove broadcasts from the tensor. [function] !flatten (!flatten tensor) equivalent to the (!reshape tensor t) [function] !rankup (!rankup tensor ntimes) The function !rankup appends/reduces 1 into the given tensor's shape for ntimes. If ntimes > 0, appends 1 If ntimes < 0, reduces 1, if the axis=1, otherwise returns error. [function] ->scal (->scal matrix-tensor) The function ->scal receives matrix-tensor with total-size = 1, returning a ScalarTensor. [function] ->mat (->mat scalar-tensor &key (dims 1)) The function ->mat receives ScalarTensor , returning a matrix with the number of axis=dims. [function] proceed (proceed tensor &key (measure-time nil)) The function proceed invokes special node, ProceedNode , which takes all the previous computation node before tensor, returning the result of it. The backward is created with the previous node. This function will be useful especially when debugging on REPL. Inputs If measure-time =t, ProceedNode wraps with time macro when calling COMPILED forward and backward propagation. Compiling time isn't included to the displayed time while (time (proceed tensor)) includes. compile-mode is a keyword, type of compile-mode-t . [function] proceed-time (proceed-time tensor) An alias for (proceed tensor :measure-time t) Note that: the proceed-time function invokes forward function twice times, in order for processing system to trace compiled lisp code, and ignoring allocation time. [function] proceed-backward (proceed-backward tensor) The function proceed-backward calls forward and backwrd of the tensor. Output T (which indicates backward is succeed) [function] !flexible (!flexible tensor) The function !flexible returns a node which adds 1 (which is broadcastable) to the head of the shape of tensor. That is: Tensor = (10 10) -> [!flexible] -> Tensor' = (1 ... 1 10 10) ^ <1 x N> Note that added axes could be broadcasted automatically when the operation called with multiple arguments. [function] !abs (!abs x &key (-> nil)) The function !abs takes x as an argument, applying a abs function into each element and writes the result into -> . O U T c o p y \u2190 a b s ( X ) OUT_{copy}\\gets{abs(X)} O U T co p y \u200b \u2190 ab s ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-ABSNODE ABSNODE SideEffects -> is destructed. [function] !sign (!sign x &key (-> nil)) The function !sign takes x as an argument, applying a sign function into each element and writes the result into -> . O U T c o p y \u2190 s i g n ( X ) OUT_{copy}\\gets{sign(X)} O U T co p y \u200b \u2190 s i g n ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-SIGNNODE SIGNNODE SideEffects -> is destructed. [function] !sqrt (!sqrt x &key (-> nil)) The function !sqrt takes x as an argument, applying a sqrt function into each element and writes the result into -> . O U T c o p y \u2190 s q r t ( X ) OUT_{copy}\\gets{sqrt(X)} O U T co p y \u200b \u2190 s q r t ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-SQRTNODE SQRTNODE SideEffects -> is destructed. [function] !square (!square x &key (-> nil)) The function !square takes x as an argument, applying a square function into each element and writes the result into -> . O U T c o p y \u2190 s q u a r e ( X ) OUT_{copy}\\gets{square(X)} O U T co p y \u200b \u2190 s q u a re ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-SQUARENODE SQUARENODE SideEffects -> is destructed. [function] !sin (!sin x &key (-> nil)) The function !sin takes x as an argument, applying a sin function into each element and writes the result into -> . O U T c o p y \u2190 s i n ( X ) OUT_{copy}\\gets{sin(X)} O U T co p y \u200b \u2190 s in ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-SINNODE SINNODE SideEffects -> is destructed. [function] !cos (!cos x &key (-> nil)) The function !cos takes x as an argument, applying a cos function into each element and writes the result into -> . O U T c o p y \u2190 c o s ( X ) OUT_{copy}\\gets{cos(X)} O U T co p y \u200b \u2190 cos ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-COSNODE COSNODE SideEffects -> is destructed. [function] !tan (!tan x &key (-> nil)) The function !tan takes x as an argument, applying a tan function into each element and writes the result into -> . O U T c o p y \u2190 t a n ( X ) OUT_{copy}\\gets{tan(X)} O U T co p y \u200b \u2190 t an ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-TANNODE TANNODE SideEffects -> is destructed. [function] !asin (!asin x &key (-> nil)) The function !asin takes x as an argument, applying a asin function into each element and writes the result into -> . O U T c o p y \u2190 a s i n ( X ) OUT_{copy}\\gets{asin(X)} O U T co p y \u200b \u2190 a s in ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-ASINNODE ASINNODE SideEffects -> is destructed. [function] !acos (!acos x &key (-> nil)) The function !acos takes x as an argument, applying a acos function into each element and writes the result into -> . O U T c o p y \u2190 a c o s ( X ) OUT_{copy}\\gets{acos(X)} O U T co p y \u200b \u2190 a cos ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-ACOSNODE ACOSNODE SideEffects -> is destructed. [function] !atan (!atan x &key (-> nil)) The function !atan takes x as an argument, applying a atan function into each element and writes the result into -> . O U T c o p y \u2190 a t a n ( X ) OUT_{copy}\\gets{atan(X)} O U T co p y \u200b \u2190 a t an ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-ATANNODE ATANNODE SideEffects -> is destructed. [function] !sinh (!sinh x &key (-> nil)) The function !sinh takes x as an argument, applying a sinh function into each element and writes the result into -> . O U T c o p y \u2190 s i n h ( X ) OUT_{copy}\\gets{sinh(X)} O U T co p y \u200b \u2190 s inh ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-SINHNODE SINHNODE SideEffects -> is destructed. [function] !cosh (!cosh x &key (-> nil)) The function !cosh takes x as an argument, applying a cosh function into each element and writes the result into -> . O U T c o p y \u2190 c o s h ( X ) OUT_{copy}\\gets{cosh(X)} O U T co p y \u200b \u2190 cos h ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-COSHNODE COSHNODE SideEffects -> is destructed. [function] !tanh (!tanh x &key (-> nil)) The function !tanh takes x as an argument, applying a tanh function into each element and writes the result into -> . O U T c o p y \u2190 t a n h ( X ) OUT_{copy}\\gets{tanh(X)} O U T co p y \u200b \u2190 t anh ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-TANHNODE TANHNODE SideEffects -> is destructed. [function] !asinh (!asinh x &key (-> nil)) The function !asinh takes x as an argument, applying a asinh function into each element and writes the result into -> . O U T c o p y \u2190 a s i n h ( X ) OUT_{copy}\\gets{asinh(X)} O U T co p y \u200b \u2190 a s inh ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-ASINHNODE ASINHNODE SideEffects -> is destructed. [function] !acosh (!acosh x &key (-> nil)) The function !acosh takes x as an argument, applying a acosh function into each element and writes the result into -> . O U T c o p y \u2190 a c o s h ( X ) OUT_{copy}\\gets{acosh(X)} O U T co p y \u200b \u2190 a cos h ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-ACOSHNODE ACOSHNODE SideEffects -> is destructed. [function] !atanh (!atanh x &key (-> nil)) The function !atanh takes x as an argument, applying a atanh function into each element and writes the result into -> . O U T c o p y \u2190 a t a n h ( X ) OUT_{copy}\\gets{atanh(X)} O U T co p y \u200b \u2190 a t anh ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-ATANHNODE ATANHNODE SideEffects -> is destructed. [function] !exp (!exp x &key (-> nil)) The function !exp takes x as an argument, applying a exp function into each element and writes the result into -> . O U T c o p y \u2190 e x p ( X ) OUT_{copy}\\gets{exp(X)} O U T co p y \u200b \u2190 e x p ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-EXPNODE EXPNODE SideEffects -> is destructed. [function] !log2 (!log2 x &key (-> nil)) The function !log2 takes x as an argument, applying a log2 function into each element and writes the result into -> . O U T c o p y \u2190 l o g 2 ( X ) OUT_{copy}\\gets{log2(X)} O U T co p y \u200b \u2190 l o g 2 ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-LOG2NODE LOG2NODE SideEffects -> is destructed. [function] !log10 (!log10 x &key (-> nil)) The function !log10 takes x as an argument, applying a log10 function into each element and writes the result into -> . O U T c o p y \u2190 l o g 10 ( X ) OUT_{copy}\\gets{log10(X)} O U T co p y \u200b \u2190 l o g 10 ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-LOG10NODE LOG10NODE SideEffects -> is destructed. [function] !loge (!loge x &key (-> nil)) The function !loge takes x as an argument, applying a loge function into each element and writes the result into -> . O U T c o p y \u2190 l o g e ( X ) OUT_{copy}\\gets{loge(X)} O U T co p y \u200b \u2190 l o g e ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-LOGENODE LOGENODE SideEffects -> is destructed. [function] !sum (!sum tensor &key (axis t) (-> nil) (keepdims nil)) The function !sum return a node which computes the sum of tensor along the given axis. Inputs tensor , a tensor to be reducted. axis [t or fixnum or list] the axis to be reducted. (-1, -2... is ok) -> [AbstractTensor or nil] the place to set the result. If nil, creates a new tensor. dims [boolean] If t, the axis reducted is broadcasted. Return: -> [AbstractTensor] the result. [function] !mean (!mean tensor &key (axis t) (-> nil) (keepdims nil)) The function !mean return a node which computes the average of tensor along the given axis. Inputs tensor , a tensor to be reducted. axis [t or fixnum or list] the axis to be reducted. (-1, -2... is ok) -> [AbstractTensor or nil] the place to set the result. If nil, creates a new tensor. keepdims [boolean] If t, the axis reducted is broadcasted. Return -> [AbstractTensor] the result. [function] !argmax (!argmax tensor &key (axis -1) (out nil)) The function !argmax computes the indices of maximum values of all elements below the axis dimension in the given tensor. Inputs tensor axis out Returns AbstractTensor[uint32] with dimensions behind axis is replaced with 1.## [function] !argmin (!argmin tensor &key (axis -1) (out nil)) The function !argmin computes the indices of minimum values of all elements below the axis dimension in the given tensor. Inputs tensor axis out Returns AbstractTensor[uint32] with dimensions behind axis is replaced with 1. [function] !t (!t tensor) Applies Lazy-Transpose to the given tensor. The function is matmul-dedicated, so cooperationg with other operations (e.g.: !add) will cause the wrong result. (Internally, it is the equivalent to calling !reshape ) Current Problem Inconsistency of operations: !flexible(!t(x)).is_transposed? = NIL !t(!flexible(x)).is_flexible? = T [function] !matmul (!matmul x y &key (out nil) (transpose-x nil) (transpose-y nil)) Computing a matrix multiplication of X and Y, the function set the result into out. o u t \u2190 g e m m ( 1.0 , x , y , 0.0 , o u t ) out\\gets{gemm(1.0, x, y, 0.0, out)} o u t \u2190 g e mm ( 1.0 , x , y , 0.0 , o u t ) Inputs transpose-x transpose-y If t, the tensor is called with (!t tensor) Lazy-Transpose Call the function (!t tensor) in advance to transpose the tensor without overheads. (!matmul (!t (randn `(5 3))) (randn `(5 3))) [function] !dot (!dot x y) Finds a dot product of x and y. Unlike numpy.dot , !dot intentionally only supports computing the dot product of two 1D tensors with the same number of elements. (proceed (!dot (randn `(100)) (randn `(10 10)))) {CPUTENSOR[float] :shape (1) -> :view (<0>) -> :visible-shape (1) :named ChainTMP115880 :vec-state [computed] (21.594929) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} [function] !where (!where tensor condition &key (true-then 1) (false-then 0) (out nil)) The function !where returns a elements selected-from true-then or false-then , depending on condition. The operation is defined as: o u t i = { true-then c o n d i t i o n ( X i ) false-then otherwise \\begin{equation} out_i= \\begin{cases} \\text{true-then} & condition(X_i) \\\\ \\text{false-then} & \\text{otherwise} \\end{cases} \\end{equation} o u t i \u200b = { true-then false-then \u200b co n d i t i o n ( X i \u200b ) otherwise \u200b \u200b \u200b (where X = tensor) Inputs out place to set the result condition an funcallable function. (e.g.: #'evenp #'oddp etc...) [function] !where (!compare tensor1 tensor2 condition &key (true-then 1) (false-then 0) (out nil)) The function !compare returns a elements selected-from true-then or false-then , depending on condition. The operation is defined as: o u t i = { true-then c o n d i t i o n ( X i , Y i ) false-then otherwise \\begin{equation} out_i= \\begin{cases} \\text{true-then} & condition(X_i, Y_i) \\\\ \\text{false-then} & \\text{otherwise} \\end{cases} \\end{equation} o u t i \u200b = { true-then false-then \u200b co n d i t i o n ( X i \u200b , Y i \u200b ) otherwise \u200b \u200b \u200b (where X = tensor1, Y=tensor2) Inputs out place to set the result condition an funcallable function. (e.g.: #'> #'< etc...) [function] a>scal (a>scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a>scal sets true-then if the equation: element > scal is t, otherwise set false-then at the corresponding positions. Inputs A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as scal) [function] a<scal (a<scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a<scal sets true-then if the equation: element < scal is t, otherwise set false-then at the corresponding positions. Inputs A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as scal) [function] a>=scal (a>=scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a>=scal sets true-then if the equation: element >= scal is t, otherwise set false-then at the corresponding positions. Inputs A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as scal) [function] a<=scal (a<=scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a<=scal sets true-then if the equation: element <= scal is t, otherwise set false-then at the corresponding positions. Inputs A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as scal) [function] a>b (a>b A B &key (out nil) (true-then 1) (false-then 0)) The function a>b sets true-then if the equation: A > B is t, otherwise set false-then at the corresponding positions. Inputs A B AbstractTensor to be compared. [function] a<b (a<b A B &key (out nil) (true-then 1) (false-then 0)) The function a<b sets true-then if the equation: A < B is t, otherwise set false-then at the corresponding positions. Inputs A B AbstractTensor to be compared. [function] a>=b (a>=b A B &key (out nil) (true-then 1) (false-then 0)) The function a>=b sets true-then if the equation: A >= B is t, otherwise set false-then at the corresponding positions. Inputs A B AbstractTensor to be compared. [function] a<=b (a<=b A B &key (out nil) (true-then 1) (false-then 0)) The function a<=b sets true-then if the equation: A <= B is t, otherwise set false-then at the corresponding positions. Inputs A B AbstractTensor to be compared.","title":"[Functions] cl-waffe2/base-impl"},{"location":"base-impl/#basic-apis","text":"","title":"Basic APIs"},{"location":"base-impl/#function-matrix-add","text":"(!matrix-add x y) The function !matrix-add calls ADDNODE and adds X and Y element-wise, returning a new tensor. X c o p y \u2190 X + Y X_{copy}\\gets{X + Y} X co p y \u200b \u2190 X + Y","title":"[function] !matrix-add"},{"location":"base-impl/#inputs","text":"X and Y must be a AbstractTensor (not a ScalarTensor), with the same shape.","title":"Inputs"},{"location":"base-impl/#sideeffects","text":"None.","title":"SideEffects"},{"location":"base-impl/#function-matrix-sub","text":"(!matrix-sub x y) The function !matrix-sub calls SUBNODE and substracts X by Y element-wise, returning a new tensor. X c o p y \u2190 X \u2212 Y X_{copy}\\gets{X - Y} X co p y \u200b \u2190 X \u2212 Y","title":"[function] !matrix-sub"},{"location":"base-impl/#inputs_1","text":"X and Y must be a AbstractTensor (not a ScalarTensor), with the same shape.","title":"Inputs"},{"location":"base-impl/#sideeffects_1","text":"None.","title":"SideEffects"},{"location":"base-impl/#function-matrix-mul","text":"(!matrix-mul x y) The function !matrix-mul calls MULNODE and multiplies X and Y element-wise, returning a new tensor. X c o p y \u2190 X \u2217 Y X_{copy}\\gets{X * Y} X co p y \u200b \u2190 X \u2217 Y","title":"[function] !matrix-mul"},{"location":"base-impl/#inputs_2","text":"X and Y must be a AbstractTensor (not a ScalarTensor), with the same shape.","title":"Inputs"},{"location":"base-impl/#sideeffects_2","text":"None.","title":"SideEffects"},{"location":"base-impl/#function-matrix-div","text":"(!matrix-div x y) The function !matrix-div calls DIVNODE and divides X by Y element-wise, returning a new tensor. X c o p y \u2190 X / Y X_{copy}\\gets{X / Y} X co p y \u200b \u2190 X / Y","title":"[function] !matrix-div"},{"location":"base-impl/#inputs_3","text":"X and Y must be a AbstractTensor (not a ScalarTensor), with the same shape.","title":"Inputs"},{"location":"base-impl/#sideeffects_3","text":"None.","title":"SideEffects"},{"location":"base-impl/#function-inverse","text":"(!inverse tensor) The function !inverse calls InverseTensorNode , and finds the inverse of the received Tensor/Scalar, returning a new tensor. X c o p y \u2190 1 / X X_{copy}\\gets{1 / X} X co p y \u200b \u2190 1/ X","title":"[function] !inverse"},{"location":"base-impl/#inputs_4","text":"tensor[ScalarTensor/AbstractTensor/Number]","title":"Inputs"},{"location":"base-impl/#function-scalar-add","text":"(!scalar-add scalar x) The function !SCALAR-ADD computes following operation with calling SCALARADD , returning a new tensor. X c o p y \u2190 X + s c a l a r X_{copy}\\gets{X + scalar} X co p y \u200b \u2190 X + sc a l a r","title":"[function] !scalar-add"},{"location":"base-impl/#inputs_5","text":"scalar could be one of ScalarTensor or number tensor AbstractTensor (should not be a scalar)","title":"Inputs"},{"location":"base-impl/#function-scalar-sub","text":"(!scalar-sub scalar x) The function !SCALAR-SUB computes following operation with calling SCALARSUB , returning a new tensor. X c o p y \u2190 X \u2212 s c a l a r X_{copy}\\gets{X - scalar} X co p y \u200b \u2190 X \u2212 sc a l a r","title":"[function] !scalar-sub"},{"location":"base-impl/#inputs_6","text":"scalar could be one of ScalarTensor or number tensor AbstractTensor (should not be a scalar)","title":"Inputs"},{"location":"base-impl/#function-scalar-mul","text":"(!scalar-mul scalar x) The function !SCALAR-MUL computes following operation with calling SCALARMUL , returning a new tensor. X c o p y \u2190 X \u2217 s c a l a r X_{copy}\\gets{X * scalar} X co p y \u200b \u2190 X \u2217 sc a l a r","title":"[function] !scalar-mul"},{"location":"base-impl/#inputs_7","text":"scalar could be one of ScalarTensor or number tensor AbstractTensor (should not be a scalar)","title":"Inputs"},{"location":"base-impl/#function-scalar-div","text":"(!scalar-div scalar x) The function !SCALAR-DIV computes following operation with calling SCALARDIV , returning a new tensor. X c o p y \u2190 X / s c a l a r X_{copy}\\gets{X / scalar} X co p y \u200b \u2190 X / sc a l a r","title":"[function] !scalar-div"},{"location":"base-impl/#inputs_8","text":"scalar could be one of ScalarTensor or number tensor AbstractTensor (should not be a scalar)","title":"Inputs"},{"location":"base-impl/#function-sas-add","text":"The function !sas-add provides differentiable scalar-and-scalar operation. Calling a node SCALARANDSCALARADD , the function performs following operation: x c o p y \u2190 x + y x_{copy}\\gets{x + y} x co p y \u200b \u2190 x + y","title":"[function] !sas-add"},{"location":"base-impl/#inputs_9","text":"x y could be one of: ScalarTensor or number","title":"Inputs"},{"location":"base-impl/#function-sas-sub","text":"The function !sas-sub provides differentiable scalar-and-scalar operation. Calling a node SCALARANDSCALARSUB , the function performs following operation: x c o p y \u2190 x \u2212 y x_{copy}\\gets{x - y} x co p y \u200b \u2190 x \u2212 y","title":"[function] !sas-sub"},{"location":"base-impl/#inputs_10","text":"x y could be one of: ScalarTensor or number","title":"Inputs"},{"location":"base-impl/#function-sas-mul","text":"The function !sas-mul provides differentiable scalar-and-scalar operation. Calling a node SCALARANDSCALARMUL , the function performs following operation: x c o p y \u2190 x \u2217 y x_{copy}\\gets{x * y} x co p y \u200b \u2190 x \u2217 y","title":"[function] !sas-mul"},{"location":"base-impl/#inputs_11","text":"x y could be one of: ScalarTensor or number","title":"Inputs"},{"location":"base-impl/#function-sas-div","text":"The function !sas-div provides differentiable scalar-and-scalar operation. Calling a node SCALARANDSCALARDIV , the function performs following operation: x c o p y \u2190 x / y x_{copy}\\gets{x / y} x co p y \u200b \u2190 x / y","title":"[function] !sas-div"},{"location":"base-impl/#inputs_12","text":"x y could be one of: ScalarTensor or number","title":"Inputs"},{"location":"base-impl/#function-add","text":"(!add x y) The function provides general-purpose arithmetic operation. Given type of tensors, this function dispatches these functions automatically: !sas-add !scalar-add !matrix-add","title":"[function] !add"},{"location":"base-impl/#inputs_13","text":"x y could be one of AbstractTensor number ScalarTensor","title":"Inputs"},{"location":"base-impl/#sideeffects_4","text":"None","title":"SideEffects"},{"location":"base-impl/#function-sub","text":"(!sub x y) The function provides general-purpose arithmetic operation. Given type of tensors, this function dispatches these functions automatically: !sas-sub !scalar-sub !matrix-sub","title":"[function] !sub"},{"location":"base-impl/#inputs_14","text":"x y could be one of AbstractTensor number ScalarTensor","title":"Inputs"},{"location":"base-impl/#sideeffects_5","text":"None","title":"SideEffects"},{"location":"base-impl/#function-mul","text":"(!mul x y) The function provides general-purpose arithmetic operation. Given type of tensors, this function dispatches these functions automatically: !sas-mul !scalar-mul !matrix-mul","title":"[function] !mul"},{"location":"base-impl/#inputs_15","text":"x y could be one of AbstractTensor number ScalarTensor","title":"Inputs"},{"location":"base-impl/#sideeffects_6","text":"None","title":"SideEffects"},{"location":"base-impl/#function-div","text":"(!div x y) The function provides general-purpose arithmetic operation. Given type of tensors, this function dispatches these functions automatically: !sas-div !scalar-div !matrix-div","title":"[function] !div"},{"location":"base-impl/#inputs_16","text":"x y could be one of AbstractTensor number ScalarTensor","title":"Inputs"},{"location":"base-impl/#sideeffects_7","text":"None","title":"SideEffects"},{"location":"base-impl/#function-move","text":"(!move place tensor) A \u2190 B A\\gets{B} A \u2190 B The function !move returns a node which moves tensor's visible elements into place's visible elements.","title":"[function] !move"},{"location":"base-impl/#nodes","text":"one of: MoveTensorNode ScalarTensorNode","title":"nodes"},{"location":"base-impl/#inputs_17","text":"place[AbstractTensor] tensor to be overwritten. tensor[AbstractTensor] tensor to be referred. force[boolean] If t, the pruning of operation by cl-waffe2 will never done.","title":"Inputs"},{"location":"base-impl/#output","text":"Unevaluated Copied Tensor.","title":"Output"},{"location":"base-impl/#function-copy","text":"(!copy tensor) The function !copy returns a node which makes a copy the tensor's visible area. Note that: the function !copy never creates a new tensor larger than (tensor-vec tensor) has, (i.e.: copying broadcasted tensor will return broadcasted and copied tensor). !copy is used to make a cache before calling destructive operation to avoid side effects, therefore if the copy is included to be useless by compiler, this operations is being ignored without changing its behaviour. And this is why !copy returns InputTensor , not AbstractTensor . See also: !copy-force never being ignored by compiler, and broadcasted axes will be padded. Input: Tensor[AbstractTensor] Output: Tensor[AbstractTensor]","title":"[function] !copy"},{"location":"base-impl/#function-copy-force","text":"(!copy-force (tensor)) The function !copy-force returns a node which copies the given tensor forcibly while the function !copy sometimes ignored. This function is also used to adjust memory alignment of tensor.","title":"[function] !copy-force"},{"location":"base-impl/#function-reshape","text":"(!reshape tensor &rest shapes) Changes the shape of given tensor. Before and after the operation, the total elements of tensors must correspond.","title":"[function] !reshape"},{"location":"base-impl/#inputs_18","text":"tensor AbstractTensor but must not includes symbol in the shape. shapes could be one of: fixnum t . t can be used at one, but the value of t is automatically inferenced.","title":"Inputs"},{"location":"base-impl/#function-view","text":"(!view tensor &rest subscripts) The function !view returns a tensor which is applied lazy-evaluated view. For Example, let A be a 4x8 Matrix, and we gonna create a view of A that portrays A[:, 2] . (!view A 2 t) A B 0 ++++++++ -------- 1 ++++++++ -------- 2 ++++++++ -> [make a view] -> ++++++++ 3 ++++++++ -------- Here, A and B shares the pointer. Calling (shape B) returns (1 8) .","title":"[function] !view"},{"location":"base-impl/#subscripts","text":"Subscripts are following: t all elements in the axis. fixnum points out the specified index. (start end) slices the area. (start end step-by) slices the area by step-by . step-by can be a negative-fixnum. (Not tested) (:broadcast N-times) broadcasts the axis for N-times, the axis to be broadcasted must be 1 or broadcasted-axis. (:tflist ...) (TODO) (:indices ...) (TODO)","title":"Subscripts"},{"location":"base-impl/#return","text":"(values sliced-tensor broadcast-reverser) Tips: Applying !view again to the returned sliced-tensor with broadcast-reverser will remove broadcasts from the tensor.","title":"Return"},{"location":"base-impl/#function-flatten","text":"(!flatten tensor) equivalent to the (!reshape tensor t)","title":"[function] !flatten"},{"location":"base-impl/#function-rankup","text":"(!rankup tensor ntimes) The function !rankup appends/reduces 1 into the given tensor's shape for ntimes. If ntimes > 0, appends 1 If ntimes < 0, reduces 1, if the axis=1, otherwise returns error.","title":"[function] !rankup"},{"location":"base-impl/#function-scal","text":"(->scal matrix-tensor) The function ->scal receives matrix-tensor with total-size = 1, returning a ScalarTensor.","title":"[function] -&gt;scal"},{"location":"base-impl/#function-mat","text":"(->mat scalar-tensor &key (dims 1)) The function ->mat receives ScalarTensor , returning a matrix with the number of axis=dims.","title":"[function] -&gt;mat"},{"location":"base-impl/#function-proceed","text":"(proceed tensor &key (measure-time nil)) The function proceed invokes special node, ProceedNode , which takes all the previous computation node before tensor, returning the result of it. The backward is created with the previous node. This function will be useful especially when debugging on REPL.","title":"[function] proceed"},{"location":"base-impl/#inputs_19","text":"If measure-time =t, ProceedNode wraps with time macro when calling COMPILED forward and backward propagation. Compiling time isn't included to the displayed time while (time (proceed tensor)) includes. compile-mode is a keyword, type of compile-mode-t .","title":"Inputs"},{"location":"base-impl/#function-proceed-time","text":"(proceed-time tensor) An alias for (proceed tensor :measure-time t) Note that: the proceed-time function invokes forward function twice times, in order for processing system to trace compiled lisp code, and ignoring allocation time.","title":"[function] proceed-time"},{"location":"base-impl/#function-proceed-backward","text":"(proceed-backward tensor) The function proceed-backward calls forward and backwrd of the tensor.","title":"[function] proceed-backward"},{"location":"base-impl/#output_1","text":"T (which indicates backward is succeed)","title":"Output"},{"location":"base-impl/#function-flexible","text":"(!flexible tensor) The function !flexible returns a node which adds 1 (which is broadcastable) to the head of the shape of tensor. That is: Tensor = (10 10) -> [!flexible] -> Tensor' = (1 ... 1 10 10) ^ <1 x N> Note that added axes could be broadcasted automatically when the operation called with multiple arguments.","title":"[function] !flexible"},{"location":"base-impl/#function-abs","text":"(!abs x &key (-> nil)) The function !abs takes x as an argument, applying a abs function into each element and writes the result into -> . O U T c o p y \u2190 a b s ( X ) OUT_{copy}\\gets{abs(X)} O U T co p y \u200b \u2190 ab s ( X ) (where OUT = -> )","title":"[function] !abs"},{"location":"base-impl/#inputs_20","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns","text":"->","title":"Returns"},{"location":"base-impl/#nodes_1","text":"SCALAR-ABSNODE ABSNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_8","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-sign","text":"(!sign x &key (-> nil)) The function !sign takes x as an argument, applying a sign function into each element and writes the result into -> . O U T c o p y \u2190 s i g n ( X ) OUT_{copy}\\gets{sign(X)} O U T co p y \u200b \u2190 s i g n ( X ) (where OUT = -> )","title":"[function] !sign"},{"location":"base-impl/#inputs_21","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_1","text":"->","title":"Returns"},{"location":"base-impl/#nodes_2","text":"SCALAR-SIGNNODE SIGNNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_9","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-sqrt","text":"(!sqrt x &key (-> nil)) The function !sqrt takes x as an argument, applying a sqrt function into each element and writes the result into -> . O U T c o p y \u2190 s q r t ( X ) OUT_{copy}\\gets{sqrt(X)} O U T co p y \u200b \u2190 s q r t ( X ) (where OUT = -> )","title":"[function] !sqrt"},{"location":"base-impl/#inputs_22","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_2","text":"->","title":"Returns"},{"location":"base-impl/#nodes_3","text":"SCALAR-SQRTNODE SQRTNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_10","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-square","text":"(!square x &key (-> nil)) The function !square takes x as an argument, applying a square function into each element and writes the result into -> . O U T c o p y \u2190 s q u a r e ( X ) OUT_{copy}\\gets{square(X)} O U T co p y \u200b \u2190 s q u a re ( X ) (where OUT = -> )","title":"[function] !square"},{"location":"base-impl/#inputs_23","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_3","text":"->","title":"Returns"},{"location":"base-impl/#nodes_4","text":"SCALAR-SQUARENODE SQUARENODE","title":"Nodes"},{"location":"base-impl/#sideeffects_11","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-sin","text":"(!sin x &key (-> nil)) The function !sin takes x as an argument, applying a sin function into each element and writes the result into -> . O U T c o p y \u2190 s i n ( X ) OUT_{copy}\\gets{sin(X)} O U T co p y \u200b \u2190 s in ( X ) (where OUT = -> )","title":"[function] !sin"},{"location":"base-impl/#inputs_24","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_4","text":"->","title":"Returns"},{"location":"base-impl/#nodes_5","text":"SCALAR-SINNODE SINNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_12","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-cos","text":"(!cos x &key (-> nil)) The function !cos takes x as an argument, applying a cos function into each element and writes the result into -> . O U T c o p y \u2190 c o s ( X ) OUT_{copy}\\gets{cos(X)} O U T co p y \u200b \u2190 cos ( X ) (where OUT = -> )","title":"[function] !cos"},{"location":"base-impl/#inputs_25","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_5","text":"->","title":"Returns"},{"location":"base-impl/#nodes_6","text":"SCALAR-COSNODE COSNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_13","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-tan","text":"(!tan x &key (-> nil)) The function !tan takes x as an argument, applying a tan function into each element and writes the result into -> . O U T c o p y \u2190 t a n ( X ) OUT_{copy}\\gets{tan(X)} O U T co p y \u200b \u2190 t an ( X ) (where OUT = -> )","title":"[function] !tan"},{"location":"base-impl/#inputs_26","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_6","text":"->","title":"Returns"},{"location":"base-impl/#nodes_7","text":"SCALAR-TANNODE TANNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_14","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-asin","text":"(!asin x &key (-> nil)) The function !asin takes x as an argument, applying a asin function into each element and writes the result into -> . O U T c o p y \u2190 a s i n ( X ) OUT_{copy}\\gets{asin(X)} O U T co p y \u200b \u2190 a s in ( X ) (where OUT = -> )","title":"[function] !asin"},{"location":"base-impl/#inputs_27","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_7","text":"->","title":"Returns"},{"location":"base-impl/#nodes_8","text":"SCALAR-ASINNODE ASINNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_15","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-acos","text":"(!acos x &key (-> nil)) The function !acos takes x as an argument, applying a acos function into each element and writes the result into -> . O U T c o p y \u2190 a c o s ( X ) OUT_{copy}\\gets{acos(X)} O U T co p y \u200b \u2190 a cos ( X ) (where OUT = -> )","title":"[function] !acos"},{"location":"base-impl/#inputs_28","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_8","text":"->","title":"Returns"},{"location":"base-impl/#nodes_9","text":"SCALAR-ACOSNODE ACOSNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_16","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-atan","text":"(!atan x &key (-> nil)) The function !atan takes x as an argument, applying a atan function into each element and writes the result into -> . O U T c o p y \u2190 a t a n ( X ) OUT_{copy}\\gets{atan(X)} O U T co p y \u200b \u2190 a t an ( X ) (where OUT = -> )","title":"[function] !atan"},{"location":"base-impl/#inputs_29","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_9","text":"->","title":"Returns"},{"location":"base-impl/#nodes_10","text":"SCALAR-ATANNODE ATANNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_17","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-sinh","text":"(!sinh x &key (-> nil)) The function !sinh takes x as an argument, applying a sinh function into each element and writes the result into -> . O U T c o p y \u2190 s i n h ( X ) OUT_{copy}\\gets{sinh(X)} O U T co p y \u200b \u2190 s inh ( X ) (where OUT = -> )","title":"[function] !sinh"},{"location":"base-impl/#inputs_30","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_10","text":"->","title":"Returns"},{"location":"base-impl/#nodes_11","text":"SCALAR-SINHNODE SINHNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_18","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-cosh","text":"(!cosh x &key (-> nil)) The function !cosh takes x as an argument, applying a cosh function into each element and writes the result into -> . O U T c o p y \u2190 c o s h ( X ) OUT_{copy}\\gets{cosh(X)} O U T co p y \u200b \u2190 cos h ( X ) (where OUT = -> )","title":"[function] !cosh"},{"location":"base-impl/#inputs_31","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_11","text":"->","title":"Returns"},{"location":"base-impl/#nodes_12","text":"SCALAR-COSHNODE COSHNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_19","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-tanh","text":"(!tanh x &key (-> nil)) The function !tanh takes x as an argument, applying a tanh function into each element and writes the result into -> . O U T c o p y \u2190 t a n h ( X ) OUT_{copy}\\gets{tanh(X)} O U T co p y \u200b \u2190 t anh ( X ) (where OUT = -> )","title":"[function] !tanh"},{"location":"base-impl/#inputs_32","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_12","text":"->","title":"Returns"},{"location":"base-impl/#nodes_13","text":"SCALAR-TANHNODE TANHNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_20","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-asinh","text":"(!asinh x &key (-> nil)) The function !asinh takes x as an argument, applying a asinh function into each element and writes the result into -> . O U T c o p y \u2190 a s i n h ( X ) OUT_{copy}\\gets{asinh(X)} O U T co p y \u200b \u2190 a s inh ( X ) (where OUT = -> )","title":"[function] !asinh"},{"location":"base-impl/#inputs_33","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_13","text":"->","title":"Returns"},{"location":"base-impl/#nodes_14","text":"SCALAR-ASINHNODE ASINHNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_21","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-acosh","text":"(!acosh x &key (-> nil)) The function !acosh takes x as an argument, applying a acosh function into each element and writes the result into -> . O U T c o p y \u2190 a c o s h ( X ) OUT_{copy}\\gets{acosh(X)} O U T co p y \u200b \u2190 a cos h ( X ) (where OUT = -> )","title":"[function] !acosh"},{"location":"base-impl/#inputs_34","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_14","text":"->","title":"Returns"},{"location":"base-impl/#nodes_15","text":"SCALAR-ACOSHNODE ACOSHNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_22","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-atanh","text":"(!atanh x &key (-> nil)) The function !atanh takes x as an argument, applying a atanh function into each element and writes the result into -> . O U T c o p y \u2190 a t a n h ( X ) OUT_{copy}\\gets{atanh(X)} O U T co p y \u200b \u2190 a t anh ( X ) (where OUT = -> )","title":"[function] !atanh"},{"location":"base-impl/#inputs_35","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_15","text":"->","title":"Returns"},{"location":"base-impl/#nodes_16","text":"SCALAR-ATANHNODE ATANHNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_23","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-exp","text":"(!exp x &key (-> nil)) The function !exp takes x as an argument, applying a exp function into each element and writes the result into -> . O U T c o p y \u2190 e x p ( X ) OUT_{copy}\\gets{exp(X)} O U T co p y \u200b \u2190 e x p ( X ) (where OUT = -> )","title":"[function] !exp"},{"location":"base-impl/#inputs_36","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_16","text":"->","title":"Returns"},{"location":"base-impl/#nodes_17","text":"SCALAR-EXPNODE EXPNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_24","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-log2","text":"(!log2 x &key (-> nil)) The function !log2 takes x as an argument, applying a log2 function into each element and writes the result into -> . O U T c o p y \u2190 l o g 2 ( X ) OUT_{copy}\\gets{log2(X)} O U T co p y \u200b \u2190 l o g 2 ( X ) (where OUT = -> )","title":"[function] !log2"},{"location":"base-impl/#inputs_37","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_17","text":"->","title":"Returns"},{"location":"base-impl/#nodes_18","text":"SCALAR-LOG2NODE LOG2NODE","title":"Nodes"},{"location":"base-impl/#sideeffects_25","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-log10","text":"(!log10 x &key (-> nil)) The function !log10 takes x as an argument, applying a log10 function into each element and writes the result into -> . O U T c o p y \u2190 l o g 10 ( X ) OUT_{copy}\\gets{log10(X)} O U T co p y \u200b \u2190 l o g 10 ( X ) (where OUT = -> )","title":"[function] !log10"},{"location":"base-impl/#inputs_38","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_18","text":"->","title":"Returns"},{"location":"base-impl/#nodes_19","text":"SCALAR-LOG10NODE LOG10NODE","title":"Nodes"},{"location":"base-impl/#sideeffects_26","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-loge","text":"(!loge x &key (-> nil)) The function !loge takes x as an argument, applying a loge function into each element and writes the result into -> . O U T c o p y \u2190 l o g e ( X ) OUT_{copy}\\gets{loge(X)} O U T co p y \u200b \u2190 l o g e ( X ) (where OUT = -> )","title":"[function] !loge"},{"location":"base-impl/#inputs_39","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_19","text":"->","title":"Returns"},{"location":"base-impl/#nodes_20","text":"SCALAR-LOGENODE LOGENODE","title":"Nodes"},{"location":"base-impl/#sideeffects_27","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-sum","text":"(!sum tensor &key (axis t) (-> nil) (keepdims nil)) The function !sum return a node which computes the sum of tensor along the given axis.","title":"[function] !sum"},{"location":"base-impl/#inputs_40","text":"tensor , a tensor to be reducted. axis [t or fixnum or list] the axis to be reducted. (-1, -2... is ok) -> [AbstractTensor or nil] the place to set the result. If nil, creates a new tensor. dims [boolean] If t, the axis reducted is broadcasted. Return: -> [AbstractTensor] the result.","title":"Inputs"},{"location":"base-impl/#function-mean","text":"(!mean tensor &key (axis t) (-> nil) (keepdims nil)) The function !mean return a node which computes the average of tensor along the given axis.","title":"[function] !mean"},{"location":"base-impl/#inputs_41","text":"tensor , a tensor to be reducted. axis [t or fixnum or list] the axis to be reducted. (-1, -2... is ok) -> [AbstractTensor or nil] the place to set the result. If nil, creates a new tensor. keepdims [boolean] If t, the axis reducted is broadcasted.","title":"Inputs"},{"location":"base-impl/#return_1","text":"-> [AbstractTensor] the result.","title":"Return"},{"location":"base-impl/#function-argmax","text":"(!argmax tensor &key (axis -1) (out nil)) The function !argmax computes the indices of maximum values of all elements below the axis dimension in the given tensor.","title":"[function] !argmax"},{"location":"base-impl/#inputs_42","text":"tensor axis out","title":"Inputs"},{"location":"base-impl/#returns_20","text":"AbstractTensor[uint32] with dimensions behind axis is replaced with 1.## [function] !argmin (!argmin tensor &key (axis -1) (out nil)) The function !argmin computes the indices of minimum values of all elements below the axis dimension in the given tensor.","title":"Returns"},{"location":"base-impl/#inputs_43","text":"tensor axis out","title":"Inputs"},{"location":"base-impl/#returns_21","text":"AbstractTensor[uint32] with dimensions behind axis is replaced with 1.","title":"Returns"},{"location":"base-impl/#function-t","text":"(!t tensor) Applies Lazy-Transpose to the given tensor. The function is matmul-dedicated, so cooperationg with other operations (e.g.: !add) will cause the wrong result. (Internally, it is the equivalent to calling !reshape )","title":"[function] !t"},{"location":"base-impl/#current-problem","text":"Inconsistency of operations: !flexible(!t(x)).is_transposed? = NIL !t(!flexible(x)).is_flexible? = T","title":"Current Problem"},{"location":"base-impl/#function-matmul","text":"(!matmul x y &key (out nil) (transpose-x nil) (transpose-y nil)) Computing a matrix multiplication of X and Y, the function set the result into out. o u t \u2190 g e m m ( 1.0 , x , y , 0.0 , o u t ) out\\gets{gemm(1.0, x, y, 0.0, out)} o u t \u2190 g e mm ( 1.0 , x , y , 0.0 , o u t )","title":"[function] !matmul"},{"location":"base-impl/#inputs_44","text":"transpose-x transpose-y If t, the tensor is called with (!t tensor)","title":"Inputs"},{"location":"base-impl/#lazy-transpose","text":"Call the function (!t tensor) in advance to transpose the tensor without overheads. (!matmul (!t (randn `(5 3))) (randn `(5 3)))","title":"Lazy-Transpose"},{"location":"base-impl/#function-dot","text":"(!dot x y) Finds a dot product of x and y. Unlike numpy.dot , !dot intentionally only supports computing the dot product of two 1D tensors with the same number of elements. (proceed (!dot (randn `(100)) (randn `(10 10)))) {CPUTENSOR[float] :shape (1) -> :view (<0>) -> :visible-shape (1) :named ChainTMP115880 :vec-state [computed] (21.594929) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>}","title":"[function] !dot"},{"location":"base-impl/#function-where","text":"(!where tensor condition &key (true-then 1) (false-then 0) (out nil)) The function !where returns a elements selected-from true-then or false-then , depending on condition. The operation is defined as: o u t i = { true-then c o n d i t i o n ( X i ) false-then otherwise \\begin{equation} out_i= \\begin{cases} \\text{true-then} & condition(X_i) \\\\ \\text{false-then} & \\text{otherwise} \\end{cases} \\end{equation} o u t i \u200b = { true-then false-then \u200b co n d i t i o n ( X i \u200b ) otherwise \u200b \u200b \u200b (where X = tensor)","title":"[function] !where"},{"location":"base-impl/#inputs_45","text":"out place to set the result condition an funcallable function. (e.g.: #'evenp #'oddp etc...)","title":"Inputs"},{"location":"base-impl/#function-where_1","text":"(!compare tensor1 tensor2 condition &key (true-then 1) (false-then 0) (out nil)) The function !compare returns a elements selected-from true-then or false-then , depending on condition. The operation is defined as: o u t i = { true-then c o n d i t i o n ( X i , Y i ) false-then otherwise \\begin{equation} out_i= \\begin{cases} \\text{true-then} & condition(X_i, Y_i) \\\\ \\text{false-then} & \\text{otherwise} \\end{cases} \\end{equation} o u t i \u200b = { true-then false-then \u200b co n d i t i o n ( X i \u200b , Y i \u200b ) otherwise \u200b \u200b \u200b (where X = tensor1, Y=tensor2)","title":"[function] !where"},{"location":"base-impl/#inputs_46","text":"out place to set the result condition an funcallable function. (e.g.: #'> #'< etc...)","title":"Inputs"},{"location":"base-impl/#function-ascal","text":"(a>scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a>scal sets true-then if the equation: element > scal is t, otherwise set false-then at the corresponding positions.","title":"[function] a&gt;scal"},{"location":"base-impl/#inputs_47","text":"A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as scal)","title":"Inputs"},{"location":"base-impl/#function-ascal_1","text":"(a<scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a<scal sets true-then if the equation: element < scal is t, otherwise set false-then at the corresponding positions.","title":"[function] a&lt;scal"},{"location":"base-impl/#inputs_48","text":"A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as scal)","title":"Inputs"},{"location":"base-impl/#function-ascal_2","text":"(a>=scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a>=scal sets true-then if the equation: element >= scal is t, otherwise set false-then at the corresponding positions.","title":"[function] a&gt;=scal"},{"location":"base-impl/#inputs_49","text":"A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as scal)","title":"Inputs"},{"location":"base-impl/#function-ascal_3","text":"(a<=scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a<=scal sets true-then if the equation: element <= scal is t, otherwise set false-then at the corresponding positions.","title":"[function] a&lt;=scal"},{"location":"base-impl/#inputs_50","text":"A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as scal)","title":"Inputs"},{"location":"base-impl/#function-ab","text":"(a>b A B &key (out nil) (true-then 1) (false-then 0)) The function a>b sets true-then if the equation: A > B is t, otherwise set false-then at the corresponding positions.","title":"[function] a&gt;b"},{"location":"base-impl/#inputs_51","text":"A B AbstractTensor to be compared.","title":"Inputs"},{"location":"base-impl/#function-ab_1","text":"(a<b A B &key (out nil) (true-then 1) (false-then 0)) The function a<b sets true-then if the equation: A < B is t, otherwise set false-then at the corresponding positions.","title":"[function] a&lt;b"},{"location":"base-impl/#inputs_52","text":"A B AbstractTensor to be compared.","title":"Inputs"},{"location":"base-impl/#function-ab_2","text":"(a>=b A B &key (out nil) (true-then 1) (false-then 0)) The function a>=b sets true-then if the equation: A >= B is t, otherwise set false-then at the corresponding positions.","title":"[function] a&gt;=b"},{"location":"base-impl/#inputs_53","text":"A B AbstractTensor to be compared.","title":"Inputs"},{"location":"base-impl/#function-ab_3","text":"(a<=b A B &key (out nil) (true-then 1) (false-then 0)) The function a<=b sets true-then if the equation: A <= B is t, otherwise set false-then at the corresponding positions.","title":"[function] a&lt;=b"},{"location":"base-impl/#inputs_54","text":"A B AbstractTensor to be compared.","title":"Inputs"},{"location":"distributions/","text":".katex img { object-fit: fill; padding: unset; display: block; position: absolute; width: 100%; height: inherit; } Distributions Sampling matrices from distribution cl-waffe2 provides a package :cl-waffe2/distributions which is used to sample matrices from the distributions. Common Format to the APIs All sampling functions are defined in the following format via define-tensor-initializer macro. (function-name shape [Optional Arguments] &rest args &keys &allow-other-keys) That is, arguments passed to the make-tensor function can also be passed directly to the initializer functions. Example (normal `(10 10) 0.0 1.0 :requires-grad t) {CPUTENSOR[float] :shape (10 10) ((-2.039073 2.1363842 0.4856452 ~ 1.3490102 -0.750389 -0.5306339) (1.1781243 0.24081734 0.2659924 ~ -0.83129555 -1.0675454 -0.40438387) ... (-0.8311747 -0.4938327 -1.8763269 ~ -0.64829946 -0.4458745 -0.6646508) (-2.18276 0.7794045 -0.5697511 ~ -0.7563723 -0.12596954 -0.472876)) :facet :exist :requires-grad T :backward NIL} Example (ax+b `(10 10) 1 0 :dtype :uint8) {CPUTENSOR[uint8] :shape (10 10) ((0 1 2 ~ 7 8 9) (10 11 12 ~ 17 18 19) ... (80 81 82 ~ 87 88 89) (90 91 92 ~ 97 98 99)) :facet :exist :requires-grad NIL :backward NIL} define-tensor-initializer (define-tensor-initializer (function-name (&rest args) initializer-lambda document &key (keep-order? nil))) define-tensor-initializer is a macro which is used to define a initializer function. Initializer function is a function whose arguments follow this format: (function-name shape <Initializer's Arguments> &rest initargs &key &allow-other-keys) Input: function-name - the function is defined after this argument args - Initializer's Arguments initializer-lambda - A form to be expanded as the sampling function, which must return a function of #'(lambda (i) ...) where i is the index of element. keep-order? - set t if the index is needed to sampling matrices. Example: (define-initializer-function uniform-random (upfrom below) (let ((upfrom (coerce upfrom (dtype->lisp-type (dtype tensor)))) (below (coerce below (dtype->lisp-type (dtype tensor))))) #'(lambda (i) (declare (ignore i)) (sample-uniform-random upfrom below))) \"\") (uniform-random `(10 10) 0.1 0.3 :requires-grad t) {CPUTENSOR[float] :shape (10 10) ((0.13149574 0.15135926 0.1569588 ~ 0.103781514 0.20610212 0.19365484) (0.2638953 0.12672275 0.21630599 ~ 0.16542184 0.10228193 0.12928057) ... (0.20429519 0.12252951 0.17538154 ~ 0.22072719 0.18642941 0.11027551) (0.14372297 0.11097031 0.25514898 ~ 0.28739202 0.18398522 0.15176433)) :facet :exist :requires-grad T :backward NIL} (Note that new tensor is binded to tensor, being used to determined dtype etc...) ax+b (ax+b shape a b &rest initargs &key &allow-other-keys) The function ax+b is a family of initializer functions, and samples matrices from arithmetic progression. o u t n = a n + b out_n = an + b o u t n \u200b = an + b Inputs: a, b - Coefficients of the above formula. Example (ax+b `(3 3) 1.0 0.0) {CPUTENSOR[float] :shape (3 3) ((0.0 1.0 2.0) (3.0 4.0 5.0) (6.0 7.0 8.0)) :facet :exist :requires-grad NIL :backward NIL} beta (beta shape alpha beta &rest initargs &key &allow-other-keys) The function beta is a family of initializer functions, and sample matrices from beta distribution. Reference Generating Beta Variates with Nonintegral Shape Parameters (R. C. H. Cheng University of Wales Institute of Science and Technology) https://dl.acm.org/doi/pdf/10.1145/359460.359482 Note: My implementation is unstable, being occurs floating-overflow constantly..., especially when min(alpha, beta) < 1.0 (i.e.: beta-bc) Example (beta `(3 3) 5.0 1.0) {CPUTENSOR[float] :shape (3 3) ((0.8364021 0.8976208 0.99033064) (0.8865608 0.99239224 0.8848019) (0.7562379 0.97992814 0.7644238)) :facet :exist :requires-grad NIL :backward NIL} bernoulli (bernoulli shape p &rest initargs &key &allow-other-keys) The bernoulli is a family of initializer functions, and samples matrices from bernoulli distribution. Inputs p - Takes 1 with probability p and 0 with probalibity (1-p). Example (bernoulli `(3 3) 0.3) {CPUTENSOR[float] :shape (3 3) ((1.0 0.0 1.0) (1.0 0.0 1.0) (0.0 0.0 0.0)) :facet :exist :requires-grad NIL :backward NIL} chisquare (chisquare shape df &rest initargs &key &allow-other-keys) The function chisquare is a family of initializer functions, and samples matrices from chisquare distributions. Inputs df - degree of freedom. References https://github.com/lvaruzza/cl-randist Example (chisquare `(3 3) 1.0) {CPUTENSOR[float] :shape (3 3) ((0.050981984 0.0045932024 1.2810057) (0.50474 0.73581934 0.28017002) (0.28480044 0.3380389 0.00970952)) :facet :exist :requires-grad NIL :backward NIL} expotential (expotential shape &rest initargs &key &allow-other-keys) The function expotential is a family of initializer functions, and samples the expotential distribution using ziggurat algorithm with table-size=256. References https://andantesoft.hatenablog.com/entry/2023/04/30/183032 Marsaglia, G., & Tsang, W. W. (2000). The ziggurat method for generating random variables. Journal of statistical software. https://marui.hatenablog.com/entry/2023/01/23/194507 Example (expotential `(3 3)) {CPUTENSOR[float] :shape (3 3) ((0.47353485 0.5842454 1.7064404) (0.85115165 0.53042275 0.031387772) (0.25639144 0.91021144 0.16937655)) :facet :exist :requires-grad NIL :backward NIL} gamma (gamma shape k &rest initargs &key &allow-other-keys) The function gamma is a family of initializer functions, and samples matrices from the gamma distribution. References https://github.com/lvaruzza/cl-randist Example (gamma `(3 3) 1.0) {CPUTENSOR[float] :shape (3 3) ((0.06623174 0.9723331 1.2478018) (0.103160195 1.1820387 0.6591184) (0.21935773 1.273132 0.96095526)) :facet :exist :requires-grad NIL :backward NIL} normal (normal shape mean stddev &rest initargs &key &allow-other-keys) The function normal is a family of initializer functions, and samples matrices from normal distribution. Reference https://github.com/lvaruzza/cl-randist (seems to create ziggurat table with size=128) Inputs mean stddev - Standard Deviation, \u03c3. Example (normal `(3 3) 1.0 0.0) {CPUTENSOR[float] :shape (3 3) ((1.0 1.0 1.0) (1.0 1.0 1.0) (1.0 1.0 1.0)) :facet :exist :requires-grad NIL :backward NIL} uniform-random (uniform-random shape upfrom below &rest initargs &key &allow-other-keys) The function uniform-random is a family of initializer funtions, and samples matrices from uniform random distribution using Common Lisp's standard function, (random arg) . Input: upfrom, below. Each elements of returned tensor is in the range of: `[upfrom, below)` Example (uniform-random `(3 3) 2 4) {CPUTENSOR[float] :shape (3 3) ((2.1255891 3.5020173 2.346451) (3.1401393 3.01733 2.1304102) (3.8687963 2.4499538 3.027814)) :facet :exist :requires-grad NIL :backward NIL} randn (randn shape &rest initargs &key &allow-other-keys) The function randn is a family of initializer functions, and samples the gaussian distributions using ziggurat algorithm with table-size=256. References https://andantesoft.hatenablog.com/entry/2023/04/30/183032 Marsaglia, G., & Tsang, W. W. (2000). The ziggurat method for generating random variables. Journal of statistical software. https://marui.hatenablog.com/entry/2023/01/23/194507 Example (randn `(3 3)) {CPUTENSOR[float] :shape (3 3) ((-1.4259218 0.5869355 2.229303) (0.5977518 -0.13385284 -0.2503691) (-0.41290623 0.35953173 -0.764124)) :facet :exist :requires-grad NIL :backward NIL}","title":"cl-waffe2/distributions"},{"location":"distributions/#distributions","text":"","title":"Distributions"},{"location":"distributions/#sampling-matrices-from-distribution","text":"cl-waffe2 provides a package :cl-waffe2/distributions which is used to sample matrices from the distributions.","title":"Sampling matrices from distribution"},{"location":"distributions/#common-format-to-the-apis","text":"All sampling functions are defined in the following format via define-tensor-initializer macro. (function-name shape [Optional Arguments] &rest args &keys &allow-other-keys) That is, arguments passed to the make-tensor function can also be passed directly to the initializer functions.","title":"Common Format to the APIs"},{"location":"distributions/#example","text":"(normal `(10 10) 0.0 1.0 :requires-grad t) {CPUTENSOR[float] :shape (10 10) ((-2.039073 2.1363842 0.4856452 ~ 1.3490102 -0.750389 -0.5306339) (1.1781243 0.24081734 0.2659924 ~ -0.83129555 -1.0675454 -0.40438387) ... (-0.8311747 -0.4938327 -1.8763269 ~ -0.64829946 -0.4458745 -0.6646508) (-2.18276 0.7794045 -0.5697511 ~ -0.7563723 -0.12596954 -0.472876)) :facet :exist :requires-grad T :backward NIL}","title":"Example"},{"location":"distributions/#example_1","text":"(ax+b `(10 10) 1 0 :dtype :uint8) {CPUTENSOR[uint8] :shape (10 10) ((0 1 2 ~ 7 8 9) (10 11 12 ~ 17 18 19) ... (80 81 82 ~ 87 88 89) (90 91 92 ~ 97 98 99)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#define-tensor-initializer","text":"(define-tensor-initializer (function-name (&rest args) initializer-lambda document &key (keep-order? nil))) define-tensor-initializer is a macro which is used to define a initializer function. Initializer function is a function whose arguments follow this format: (function-name shape <Initializer's Arguments> &rest initargs &key &allow-other-keys) Input: function-name - the function is defined after this argument args - Initializer's Arguments initializer-lambda - A form to be expanded as the sampling function, which must return a function of #'(lambda (i) ...) where i is the index of element. keep-order? - set t if the index is needed to sampling matrices. Example: (define-initializer-function uniform-random (upfrom below) (let ((upfrom (coerce upfrom (dtype->lisp-type (dtype tensor)))) (below (coerce below (dtype->lisp-type (dtype tensor))))) #'(lambda (i) (declare (ignore i)) (sample-uniform-random upfrom below))) \"\") (uniform-random `(10 10) 0.1 0.3 :requires-grad t) {CPUTENSOR[float] :shape (10 10) ((0.13149574 0.15135926 0.1569588 ~ 0.103781514 0.20610212 0.19365484) (0.2638953 0.12672275 0.21630599 ~ 0.16542184 0.10228193 0.12928057) ... (0.20429519 0.12252951 0.17538154 ~ 0.22072719 0.18642941 0.11027551) (0.14372297 0.11097031 0.25514898 ~ 0.28739202 0.18398522 0.15176433)) :facet :exist :requires-grad T :backward NIL} (Note that new tensor is binded to tensor, being used to determined dtype etc...)","title":"define-tensor-initializer"},{"location":"distributions/#axb","text":"(ax+b shape a b &rest initargs &key &allow-other-keys) The function ax+b is a family of initializer functions, and samples matrices from arithmetic progression. o u t n = a n + b out_n = an + b o u t n \u200b = an + b Inputs: a, b - Coefficients of the above formula.","title":"ax+b"},{"location":"distributions/#example_2","text":"(ax+b `(3 3) 1.0 0.0) {CPUTENSOR[float] :shape (3 3) ((0.0 1.0 2.0) (3.0 4.0 5.0) (6.0 7.0 8.0)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#beta","text":"(beta shape alpha beta &rest initargs &key &allow-other-keys) The function beta is a family of initializer functions, and sample matrices from beta distribution.","title":"beta"},{"location":"distributions/#reference","text":"Generating Beta Variates with Nonintegral Shape Parameters (R. C. H. Cheng University of Wales Institute of Science and Technology) https://dl.acm.org/doi/pdf/10.1145/359460.359482 Note: My implementation is unstable, being occurs floating-overflow constantly..., especially when min(alpha, beta) < 1.0 (i.e.: beta-bc)","title":"Reference"},{"location":"distributions/#example_3","text":"(beta `(3 3) 5.0 1.0) {CPUTENSOR[float] :shape (3 3) ((0.8364021 0.8976208 0.99033064) (0.8865608 0.99239224 0.8848019) (0.7562379 0.97992814 0.7644238)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#bernoulli","text":"(bernoulli shape p &rest initargs &key &allow-other-keys) The bernoulli is a family of initializer functions, and samples matrices from bernoulli distribution.","title":"bernoulli"},{"location":"distributions/#inputs","text":"p - Takes 1 with probability p and 0 with probalibity (1-p).","title":"Inputs"},{"location":"distributions/#example_4","text":"(bernoulli `(3 3) 0.3) {CPUTENSOR[float] :shape (3 3) ((1.0 0.0 1.0) (1.0 0.0 1.0) (0.0 0.0 0.0)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#chisquare","text":"(chisquare shape df &rest initargs &key &allow-other-keys) The function chisquare is a family of initializer functions, and samples matrices from chisquare distributions.","title":"chisquare"},{"location":"distributions/#inputs_1","text":"df - degree of freedom.","title":"Inputs"},{"location":"distributions/#references","text":"https://github.com/lvaruzza/cl-randist","title":"References"},{"location":"distributions/#example_5","text":"(chisquare `(3 3) 1.0) {CPUTENSOR[float] :shape (3 3) ((0.050981984 0.0045932024 1.2810057) (0.50474 0.73581934 0.28017002) (0.28480044 0.3380389 0.00970952)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#expotential","text":"(expotential shape &rest initargs &key &allow-other-keys) The function expotential is a family of initializer functions, and samples the expotential distribution using ziggurat algorithm with table-size=256.","title":"expotential"},{"location":"distributions/#references_1","text":"https://andantesoft.hatenablog.com/entry/2023/04/30/183032 Marsaglia, G., & Tsang, W. W. (2000). The ziggurat method for generating random variables. Journal of statistical software. https://marui.hatenablog.com/entry/2023/01/23/194507","title":"References"},{"location":"distributions/#example_6","text":"(expotential `(3 3)) {CPUTENSOR[float] :shape (3 3) ((0.47353485 0.5842454 1.7064404) (0.85115165 0.53042275 0.031387772) (0.25639144 0.91021144 0.16937655)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#gamma","text":"(gamma shape k &rest initargs &key &allow-other-keys) The function gamma is a family of initializer functions, and samples matrices from the gamma distribution.","title":"gamma"},{"location":"distributions/#references_2","text":"https://github.com/lvaruzza/cl-randist","title":"References"},{"location":"distributions/#example_7","text":"(gamma `(3 3) 1.0) {CPUTENSOR[float] :shape (3 3) ((0.06623174 0.9723331 1.2478018) (0.103160195 1.1820387 0.6591184) (0.21935773 1.273132 0.96095526)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#normal","text":"(normal shape mean stddev &rest initargs &key &allow-other-keys) The function normal is a family of initializer functions, and samples matrices from normal distribution.","title":"normal"},{"location":"distributions/#reference_1","text":"https://github.com/lvaruzza/cl-randist (seems to create ziggurat table with size=128)","title":"Reference"},{"location":"distributions/#inputs_2","text":"mean stddev - Standard Deviation, \u03c3.","title":"Inputs"},{"location":"distributions/#example_8","text":"(normal `(3 3) 1.0 0.0) {CPUTENSOR[float] :shape (3 3) ((1.0 1.0 1.0) (1.0 1.0 1.0) (1.0 1.0 1.0)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#uniform-random","text":"(uniform-random shape upfrom below &rest initargs &key &allow-other-keys) The function uniform-random is a family of initializer funtions, and samples matrices from uniform random distribution using Common Lisp's standard function, (random arg) . Input: upfrom, below. Each elements of returned tensor is in the range of: `[upfrom, below)`","title":"uniform-random"},{"location":"distributions/#example_9","text":"(uniform-random `(3 3) 2 4) {CPUTENSOR[float] :shape (3 3) ((2.1255891 3.5020173 2.346451) (3.1401393 3.01733 2.1304102) (3.8687963 2.4499538 3.027814)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#randn","text":"(randn shape &rest initargs &key &allow-other-keys) The function randn is a family of initializer functions, and samples the gaussian distributions using ziggurat algorithm with table-size=256.","title":"randn"},{"location":"distributions/#references_3","text":"https://andantesoft.hatenablog.com/entry/2023/04/30/183032 Marsaglia, G., & Tsang, W. W. (2000). The ziggurat method for generating random variables. Journal of statistical software. https://marui.hatenablog.com/entry/2023/01/23/194507","title":"References"},{"location":"distributions/#example_10","text":"(randn `(3 3)) {CPUTENSOR[float] :shape (3 3) ((-1.4259218 0.5869355 2.229303) (0.5977518 -0.13385284 -0.2503691) (-0.41290623 0.35953173 -0.764124)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"generic-tensor/","text":"AbstractTensor [class] AbstractTensor [class] AbstractTensor AbstractTensor is a primal class for all devices. Each devices (e.g.: ScalarTensor LispTensor CPUTensor etc...) is a subclass of this. The class provides the fundamental and necessary features for tensors. Lazy-Evaluated and Multi-Dimensional APIs, stride computations. View APIs multi-dimensional offsets To construct backward, AbstractTensor records variables called with. vec container. an space for saving gradients, copies for backward. Lazy-Evaluated Shapings Trace Informations for JIT to create well-optimized computation node. Creating a new backend. Users can create a new backend by extending this abstract class. (defclass MyBackend (AbstractNode) nil) To use the MyBackend as a tensor, users also has to override these methods: initialize-instance ... An allocator for tensor's vec. vref (setf vref) ... an generic function to access/write tensor's vec. ;; TODO: Establish a common API for initargs (defmethod initialize-instance :before ((tensor MyBackend) &rest initargs &key &allow-other-keys) ;; if projected-p -> alloc new vec (let* ((shape (getf initargs :shape)) (dtype (dtype->lisp-type (getf initargs :dtype))) (vec (getf initargs :vec)) (facet (getf initargs :facet)) (initial-element (coerce (or (getf initargs :initial-element) 0) dtype))) (when (eql facet :exist) (if vec (setf (tensor-vec tensor) vec) (setf (tensor-vec tensor) ;; vec can be anything. (make-array (apply #'* shape) :element-type dtype :initial-element initial-element)))))) (defmethod vref ((tensor MyBackend) index) (aref (tensor-vec tensor) index)) (defmethod (setf vref) (new-value (tensor MyBackend) index) (setf (aref (tensor-vec tensor) index) new-value)) Now, the name MyBackend is available as a brand-new cl-waffe2 backend! Users can define a new implementation following (define-impl (Name :device MyBackend) ...) (See the examples to understand how this could be achieved at ./source/backends/lisp/tensor.lisp. or ./source/backends/cpu.) [function] shape Returns a visible shape of tensor [function] dims Returns the number of axes of tensor [function] total Returns the number of total visible elements in tensor. [slot] orig-shape (List) the original shape of vec . (apply #'* orig-shape) must correspond with the number of total elements of vec . [slot] stride (list) An stride of tensor, can be chosen from :column :row . This slot can be accessed by (tensor-stride object) . [slot] visible-shape (list) An shape of visible-area of tensor, visible-area is that an viewed size of tensor. Can be accessed by (shape object) [slot] view (list) An list of multidimensional offsets, view. Can be accessed by (tensor-view object) [slot] projected-p (boolean) Set t if (apply #'* orig-shape) == (apply #'* visible-shape) otherwise set nil. If t, the tensor is produced by !view or view functions. [slot] scalar-p If t, the tensor is regarded as a Scalar. [slot] detach-p If t, JIT compilers stop tracing at the tensor. [slot] state Stores a corresponding StateContainer . [slot] variables (tensor-variables object) Records variables called with the tensor. [slot] tensor-id (symbol) Corresponding variable name that used in JIT compiler. [slot] grad (AbstractTensor) If the tensor is a parameter, (i.e.: requires-grad t) and backward propagation has called, the gradients has set to this slot. Reader: (grad object) . Writer: (set-grad object value) [slot] backward (AbstractNode) the node called with. [slot] requires-grad (Boolean) If t, the tensor become a parameter that gradients are saved. [slot] ancestor-param-p (Boolean) If t, the tensor has created by parameter or tensors whose ancestor-param-p=t. [slot] flexible-p (Boolean) If t, the tensor is broadcastable [slot] facet (keyword) Tensors has a two state: :input :exist :exist tensor is a just normal state, which vec is already allocated. :input tensor is a lazy-evaluated tensor, which allocation will be done until they're really needed. (often used as a cache, or training data.) ... [function] tensor-vec (tensor-vec tensor) Reading the vec of tensor. Not until tensor-vec is called, the new area isn't allocated. [function] mref (mref tensor &rest subscripts) The function mref is only used to print/initialize tensors, accessing the index of subscripts with considering views.. If you cares about performance, dont' use mref , but !view . This function is setfable. [generic] vref (vref tensor index) vref is a generic-function to access the vec slot of specific backends tensor, and returns index th element on vec slot without considering views. If you added a new backend with having different ptr-type (can't be accessed by aref), override this method and (setf vref) . Example (defmethod vref ((tensor YourBackend) index) (aref (tensor-vec tensor) index)) (defmethod (setf vref) (new-value (tensor YourBackend) index) (setf (aref (tensor-vec tensor) index) new-value)) An form of tensors in cl-waffe2 There's a two type of tensors in cl-waffe2: InputTensor and ExistTensor , each state is called facet and the keyword :input :exist is dispatched respectively. ExistTensor ExistTensor means a tensor with its vec allocated in the memory, that is, the same tensor as tensors you got when create a new tensor in Numpy , PyTorch or something. ExistTensor can be created by the function make-tensor . InputTensor On the other hand, InputTensor is a tensor with its vec unallocated in the memory, in other words, this can be a Lazy-Evaluated Tensor . InputTensor is created by the function make-input , and its shape can include a symbol. In the network, InputTensor plays a role in being caches in the operation, or being a tensor that one may want to change its content later. (e.g.: training data). [function] make-tensor (make-tensor shape-or-scalar &key (requires-grad nil) (dtype *default-dtype*) (vec nil) (view nil) (order *default-order*) (initial-element)) Refering a first-priority of using-backends (i.e.: car of *using-backends* ) to know what device to use, the function make-tensor creates and allocate a new matrix instantly. Input shape-or-scalar (Any) set list (consisted of fixnum) here to create a matrix, otherwise the ScalarTensor is forcibly created. requires-grad (Boolean) Set t to create gradient. (e.g.: the tensor is needed to be optimized.) dtype (keyword) Set dtype you wanna use. See also: (Dtype API) vec (Anything) If you wanna pass the make-instance to already-allocated matrix, use this parameter. order (member :column :row) initial-element (Optional) Example (make-tensor `(10 10) :initial-element 1.0) {CPUTENSOR[float] :shape (10 10) ((1.0 1.0 1.0 ~ 1.0 1.0 1.0) (1.0 1.0 1.0 ~ 1.0 1.0 1.0) ... (1.0 1.0 1.0 ~ 1.0 1.0 1.0) (1.0 1.0 1.0 ~ 1.0 1.0 1.0)) :facet :exist :requires-grad NIL :backward NIL} [function] make-input Referring a first-priority of *using-backend* (i.e.: car part), the function make-input creates a InputTensor. In contrast to make-tensor , allocation of vec is lazy-evaluated, and shape can include symbols. (Lazy-Evaluated Shape). For example, whichever (make-input (list 256 256 256 ... 256 256 256) nil) or (make-input (list 256) nil) is called, the memory-usage is the same until (tensor-vec tensor) is called but the moment (tensor-vec tensor) is called, the first one would cause CUDA OUT OF MEMORY or something :(. Inputs Shape [list] consisted of fixnum or symbol. (e.g.: (a 10) is OK for make-input.) Named [keyword] the name of input. If nil, the tensor is regarded as just cache. If you want to change the content of inputs later (e.g.: training data), set an appropriate name to InputTensor (e.g.: :training-data :train-x ). scalar-p [boolean] set t is the input is scalar. dtype [keyword] as it is. order [keyword] an member of :column :row Example (make-input `(a 10) :train-x) {CPUTENSOR[float] :shape (A 10) :named TRAIN-X <<Not-Embodied (A 10) Tensor>> :facet :input :requires-grad NIL :backward NIL} The InputTensor named with a keyword is called not-embodied tensor , and can be changed its vec with embody-input [class] Compiled-Composite Compiled-Composite is a callable CLOS class, and holds compiled forward/backward function of all the computation node to all the endpoints from the top of the models' neural network. Also, this class holds information of all variables used in the node. It is NOT possible to construct a computation node after Compiled-Composite, If you need this, try consider using the function cl-waffe2/base-impl:proceed . The class will appear in your project with calling the function build , set the toplevel node (e.g.: the result of criterion when the task is optimizing.) to the first argument. cl-waffe2 compiler will instantly construct an lambda function of forward/backward, which is invoked by calling (forward compiled-composite) or (backward compiled-composite) method. See also: build set-input get-input . Examples (TODO) [function] build (build toplevel &key (construct-backward? (not *no-grad*)) (compile-mode :fastest)) Receiving the toplevel node in the neural network, the function build constructs a optimal forward/backward function, returning Compiled-Composite . The constraints of toplevel tensor. The shape of topleve mustn't include a symbol . For example, this cl-waffe2 operation is invaild. because the function (!sin x) still returns (A B) tensor. (build (!sin (make-input `(A B) :Input))) In order to build this operation correctly, calling criterion (intrinsically, !sum or !mean ) is a vaild option for neural network tasks. (build (!sum (!sin (make-input `(A B) :input)))) ;; Passes Correctly! After working with adjustable shape tensor, don't forget to embody the InputTensor! (let ((compiled-model (build (!sum (!sin (make-input `(A B) :input)))))) (set-input compiled-model :input (randn `(10 10))) (forward compiled-model)) Inputs toplevel [AbstractTensor] the end of nodes. for neural network tasks, this should be scalartensor or tensors with total elements is 1, but since cl-waffe2 is intended to be applied other tasks, cl-waffe2 never produce warning while other frameworks like PyTorch will return error if <<(10 10)Tensor>>.backward() is invoked for example. construct-backward? [boolean] If t, the backward construction won't be done. compile-mode [compile-mode-t] an keyword to indicate compiling option. Example REPL: > (setq out (!add (randn `(10 10)) (make-input `(a 10) :X))) {CPUTENSOR[float] :shape (10 10) :named ChainTMP39821 :vec-state [maybe-not-computed] <<Not-Embodied (10 10) Tensor>> :facet :input :requires-grad NIL :backward <Node: ADDNODE-CPUTENSOR (A[~] B[~] -> A[~])>} REPL: > (multiple-value-list (build out)) (<Compiled-Composite forward: #<FUNCTION (LAMBDA () :IN \"/private/var/tmp/slimeDvMNag.fasl\") {53B83D2B}> backward: #<FUNCTION (LAMBDA () :IN \"/private/var/tmp/slimeDvMNag.fasl\") {53B84C8B}> += [Tensors in the computation node] =======+ Subscripts: [A -> ?, max=?] Variables: NAMES | SIZE | \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013 X | (A 10) | - The number of tmp variables : 4 - The number of parameters : 0 +========================================+ > NIL) [method] set-input (set-input (model Compiled-Composite) input-name actual-value) Embodies an InputTensor in the model. All unembodied tensors in the model can be accessed by printing the model. input-name could be a keyword indicating input-tensor, actual-value is a AbstractTensor whose facet = :exist (created by make-tensor ). [method] get-input (get-input (model Compiled-Composite) input-name) Reading all variables in the computation node, the method get-input returns an corresponding InputTensor of model. [parameter] *no-grad* [parameter] *no-grad* If t, no gradients are made for backwards. [macro] with-no-grad (with-no-grad &body body) Under the body execution, the macro sets *no-grad* = t , that is, the built nodes are regarded as: no gradients are made for backwards. [function] parameter (parameter tensor) The function parameter computes all the previous nodes of the given tensor if any, returning the new tensor with requires-grad=t . Example (parameter (randn `(3 3))) [function] call-with-view (call-with-view function tensors &key (at-least-dim 1)) call-with-view is a general-purpose interface to iterate multi-dimensional tensor with considering offsets. (TODO: Example/Documents) function [lambda] an lambda function which receives variable1.view variable2.view ... as arguments, returning an list being compiled. tensors [list of abstracttensor] tensors to be called with. at-least-dim [fixnum] ... kernel-size See also: size-of stride-of offset-of NILNILNIL [function] shape-equal a=1, b=k => T a=1, b=2 => NIL ...Returns subscript-t if view is Subscript otherwise returns a view Compiling Options TODO Dtypes TODO","title":"cl-waffe2/vm.generic-tensor"},{"location":"generic-tensor/#abstracttensor","text":"","title":"AbstractTensor"},{"location":"generic-tensor/#class-abstracttensor","text":"[class] AbstractTensor AbstractTensor is a primal class for all devices. Each devices (e.g.: ScalarTensor LispTensor CPUTensor etc...) is a subclass of this. The class provides the fundamental and necessary features for tensors. Lazy-Evaluated and Multi-Dimensional APIs, stride computations. View APIs multi-dimensional offsets To construct backward, AbstractTensor records variables called with. vec container. an space for saving gradients, copies for backward. Lazy-Evaluated Shapings Trace Informations for JIT to create well-optimized computation node.","title":"[class] AbstractTensor"},{"location":"generic-tensor/#creating-a-new-backend","text":"Users can create a new backend by extending this abstract class. (defclass MyBackend (AbstractNode) nil) To use the MyBackend as a tensor, users also has to override these methods: initialize-instance ... An allocator for tensor's vec. vref (setf vref) ... an generic function to access/write tensor's vec. ;; TODO: Establish a common API for initargs (defmethod initialize-instance :before ((tensor MyBackend) &rest initargs &key &allow-other-keys) ;; if projected-p -> alloc new vec (let* ((shape (getf initargs :shape)) (dtype (dtype->lisp-type (getf initargs :dtype))) (vec (getf initargs :vec)) (facet (getf initargs :facet)) (initial-element (coerce (or (getf initargs :initial-element) 0) dtype))) (when (eql facet :exist) (if vec (setf (tensor-vec tensor) vec) (setf (tensor-vec tensor) ;; vec can be anything. (make-array (apply #'* shape) :element-type dtype :initial-element initial-element)))))) (defmethod vref ((tensor MyBackend) index) (aref (tensor-vec tensor) index)) (defmethod (setf vref) (new-value (tensor MyBackend) index) (setf (aref (tensor-vec tensor) index) new-value)) Now, the name MyBackend is available as a brand-new cl-waffe2 backend! Users can define a new implementation following (define-impl (Name :device MyBackend) ...) (See the examples to understand how this could be achieved at ./source/backends/lisp/tensor.lisp. or ./source/backends/cpu.)","title":"Creating a new backend."},{"location":"generic-tensor/#function-shape","text":"Returns a visible shape of tensor","title":"[function] shape"},{"location":"generic-tensor/#function-dims","text":"Returns the number of axes of tensor","title":"[function] dims"},{"location":"generic-tensor/#function-total","text":"Returns the number of total visible elements in tensor.","title":"[function] total"},{"location":"generic-tensor/#slot-orig-shape-list","text":"the original shape of vec . (apply #'* orig-shape) must correspond with the number of total elements of vec .","title":"[slot] orig-shape (List)"},{"location":"generic-tensor/#slot-stride-list","text":"An stride of tensor, can be chosen from :column :row . This slot can be accessed by (tensor-stride object) .","title":"[slot] stride (list)"},{"location":"generic-tensor/#slot-visible-shape-list","text":"An shape of visible-area of tensor, visible-area is that an viewed size of tensor. Can be accessed by (shape object)","title":"[slot] visible-shape (list)"},{"location":"generic-tensor/#slot-view-list","text":"An list of multidimensional offsets, view. Can be accessed by (tensor-view object)","title":"[slot] view (list)"},{"location":"generic-tensor/#slot-projected-p-boolean","text":"Set t if (apply #'* orig-shape) == (apply #'* visible-shape) otherwise set nil. If t, the tensor is produced by !view or view functions.","title":"[slot] projected-p (boolean)"},{"location":"generic-tensor/#slot-scalar-p","text":"If t, the tensor is regarded as a Scalar.","title":"[slot] scalar-p"},{"location":"generic-tensor/#slot-detach-p","text":"If t, JIT compilers stop tracing at the tensor.","title":"[slot] detach-p"},{"location":"generic-tensor/#slot-state","text":"Stores a corresponding StateContainer .","title":"[slot] state"},{"location":"generic-tensor/#slot-variables","text":"(tensor-variables object) Records variables called with the tensor.","title":"[slot] variables"},{"location":"generic-tensor/#slot-tensor-id-symbol","text":"Corresponding variable name that used in JIT compiler.","title":"[slot] tensor-id (symbol)"},{"location":"generic-tensor/#slot-grad-abstracttensor","text":"If the tensor is a parameter, (i.e.: requires-grad t) and backward propagation has called, the gradients has set to this slot. Reader: (grad object) . Writer: (set-grad object value)","title":"[slot] grad (AbstractTensor)"},{"location":"generic-tensor/#slot-backward-abstractnode","text":"the node called with.","title":"[slot] backward (AbstractNode)"},{"location":"generic-tensor/#slot-requires-grad-boolean","text":"If t, the tensor become a parameter that gradients are saved.","title":"[slot] requires-grad (Boolean)"},{"location":"generic-tensor/#slot-ancestor-param-p-boolean","text":"If t, the tensor has created by parameter or tensors whose ancestor-param-p=t.","title":"[slot] ancestor-param-p (Boolean)"},{"location":"generic-tensor/#slot-flexible-p-boolean","text":"If t, the tensor is broadcastable","title":"[slot] flexible-p (Boolean)"},{"location":"generic-tensor/#slot-facet-keyword","text":"Tensors has a two state: :input :exist :exist tensor is a just normal state, which vec is already allocated. :input tensor is a lazy-evaluated tensor, which allocation will be done until they're really needed. (often used as a cache, or training data.) ...","title":"[slot] facet (keyword)"},{"location":"generic-tensor/#function-tensor-vec","text":"(tensor-vec tensor) Reading the vec of tensor. Not until tensor-vec is called, the new area isn't allocated.","title":"[function] tensor-vec"},{"location":"generic-tensor/#function-mref","text":"(mref tensor &rest subscripts) The function mref is only used to print/initialize tensors, accessing the index of subscripts with considering views.. If you cares about performance, dont' use mref , but !view . This function is setfable.","title":"[function] mref"},{"location":"generic-tensor/#generic-vref","text":"(vref tensor index) vref is a generic-function to access the vec slot of specific backends tensor, and returns index th element on vec slot without considering views. If you added a new backend with having different ptr-type (can't be accessed by aref), override this method and (setf vref) .","title":"[generic] vref"},{"location":"generic-tensor/#example","text":"(defmethod vref ((tensor YourBackend) index) (aref (tensor-vec tensor) index)) (defmethod (setf vref) (new-value (tensor YourBackend) index) (setf (aref (tensor-vec tensor) index) new-value))","title":"Example"},{"location":"generic-tensor/#an-form-of-tensors-in-cl-waffe2","text":"There's a two type of tensors in cl-waffe2: InputTensor and ExistTensor , each state is called facet and the keyword :input :exist is dispatched respectively.","title":"An form of tensors in cl-waffe2"},{"location":"generic-tensor/#existtensor","text":"ExistTensor means a tensor with its vec allocated in the memory, that is, the same tensor as tensors you got when create a new tensor in Numpy , PyTorch or something. ExistTensor can be created by the function make-tensor .","title":"ExistTensor"},{"location":"generic-tensor/#inputtensor","text":"On the other hand, InputTensor is a tensor with its vec unallocated in the memory, in other words, this can be a Lazy-Evaluated Tensor . InputTensor is created by the function make-input , and its shape can include a symbol. In the network, InputTensor plays a role in being caches in the operation, or being a tensor that one may want to change its content later. (e.g.: training data).","title":"InputTensor"},{"location":"generic-tensor/#function-make-tensor","text":"(make-tensor shape-or-scalar &key (requires-grad nil) (dtype *default-dtype*) (vec nil) (view nil) (order *default-order*) (initial-element)) Refering a first-priority of using-backends (i.e.: car of *using-backends* ) to know what device to use, the function make-tensor creates and allocate a new matrix instantly.","title":"[function] make-tensor"},{"location":"generic-tensor/#input","text":"shape-or-scalar (Any) set list (consisted of fixnum) here to create a matrix, otherwise the ScalarTensor is forcibly created. requires-grad (Boolean) Set t to create gradient. (e.g.: the tensor is needed to be optimized.) dtype (keyword) Set dtype you wanna use. See also: (Dtype API) vec (Anything) If you wanna pass the make-instance to already-allocated matrix, use this parameter. order (member :column :row) initial-element (Optional)","title":"Input"},{"location":"generic-tensor/#example_1","text":"(make-tensor `(10 10) :initial-element 1.0) {CPUTENSOR[float] :shape (10 10) ((1.0 1.0 1.0 ~ 1.0 1.0 1.0) (1.0 1.0 1.0 ~ 1.0 1.0 1.0) ... (1.0 1.0 1.0 ~ 1.0 1.0 1.0) (1.0 1.0 1.0 ~ 1.0 1.0 1.0)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"generic-tensor/#function-make-input","text":"Referring a first-priority of *using-backend* (i.e.: car part), the function make-input creates a InputTensor. In contrast to make-tensor , allocation of vec is lazy-evaluated, and shape can include symbols. (Lazy-Evaluated Shape). For example, whichever (make-input (list 256 256 256 ... 256 256 256) nil) or (make-input (list 256) nil) is called, the memory-usage is the same until (tensor-vec tensor) is called but the moment (tensor-vec tensor) is called, the first one would cause CUDA OUT OF MEMORY or something :(.","title":"[function] make-input"},{"location":"generic-tensor/#inputs","text":"Shape [list] consisted of fixnum or symbol. (e.g.: (a 10) is OK for make-input.) Named [keyword] the name of input. If nil, the tensor is regarded as just cache. If you want to change the content of inputs later (e.g.: training data), set an appropriate name to InputTensor (e.g.: :training-data :train-x ). scalar-p [boolean] set t is the input is scalar. dtype [keyword] as it is. order [keyword] an member of :column :row","title":"Inputs"},{"location":"generic-tensor/#example_2","text":"(make-input `(a 10) :train-x) {CPUTENSOR[float] :shape (A 10) :named TRAIN-X <<Not-Embodied (A 10) Tensor>> :facet :input :requires-grad NIL :backward NIL} The InputTensor named with a keyword is called not-embodied tensor , and can be changed its vec with embody-input","title":"Example"},{"location":"generic-tensor/#class-compiled-composite","text":"Compiled-Composite is a callable CLOS class, and holds compiled forward/backward function of all the computation node to all the endpoints from the top of the models' neural network. Also, this class holds information of all variables used in the node. It is NOT possible to construct a computation node after Compiled-Composite, If you need this, try consider using the function cl-waffe2/base-impl:proceed . The class will appear in your project with calling the function build , set the toplevel node (e.g.: the result of criterion when the task is optimizing.) to the first argument. cl-waffe2 compiler will instantly construct an lambda function of forward/backward, which is invoked by calling (forward compiled-composite) or (backward compiled-composite) method. See also: build set-input get-input .","title":"[class] Compiled-Composite"},{"location":"generic-tensor/#examples","text":"(TODO)","title":"Examples"},{"location":"generic-tensor/#function-build","text":"(build toplevel &key (construct-backward? (not *no-grad*)) (compile-mode :fastest)) Receiving the toplevel node in the neural network, the function build constructs a optimal forward/backward function, returning Compiled-Composite .","title":"[function] build"},{"location":"generic-tensor/#the-constraints-of-toplevel-tensor","text":"The shape of topleve mustn't include a symbol . For example, this cl-waffe2 operation is invaild. because the function (!sin x) still returns (A B) tensor. (build (!sin (make-input `(A B) :Input))) In order to build this operation correctly, calling criterion (intrinsically, !sum or !mean ) is a vaild option for neural network tasks. (build (!sum (!sin (make-input `(A B) :input)))) ;; Passes Correctly! After working with adjustable shape tensor, don't forget to embody the InputTensor! (let ((compiled-model (build (!sum (!sin (make-input `(A B) :input)))))) (set-input compiled-model :input (randn `(10 10))) (forward compiled-model))","title":"The constraints of toplevel tensor."},{"location":"generic-tensor/#inputs_1","text":"toplevel [AbstractTensor] the end of nodes. for neural network tasks, this should be scalartensor or tensors with total elements is 1, but since cl-waffe2 is intended to be applied other tasks, cl-waffe2 never produce warning while other frameworks like PyTorch will return error if <<(10 10)Tensor>>.backward() is invoked for example. construct-backward? [boolean] If t, the backward construction won't be done. compile-mode [compile-mode-t] an keyword to indicate compiling option.","title":"Inputs"},{"location":"generic-tensor/#example_3","text":"REPL: > (setq out (!add (randn `(10 10)) (make-input `(a 10) :X))) {CPUTENSOR[float] :shape (10 10) :named ChainTMP39821 :vec-state [maybe-not-computed] <<Not-Embodied (10 10) Tensor>> :facet :input :requires-grad NIL :backward <Node: ADDNODE-CPUTENSOR (A[~] B[~] -> A[~])>} REPL: > (multiple-value-list (build out)) (<Compiled-Composite forward: #<FUNCTION (LAMBDA () :IN \"/private/var/tmp/slimeDvMNag.fasl\") {53B83D2B}> backward: #<FUNCTION (LAMBDA () :IN \"/private/var/tmp/slimeDvMNag.fasl\") {53B84C8B}> += [Tensors in the computation node] =======+ Subscripts: [A -> ?, max=?] Variables: NAMES | SIZE | \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013 X | (A 10) | - The number of tmp variables : 4 - The number of parameters : 0 +========================================+ > NIL)","title":"Example"},{"location":"generic-tensor/#method-set-input","text":"(set-input (model Compiled-Composite) input-name actual-value) Embodies an InputTensor in the model. All unembodied tensors in the model can be accessed by printing the model. input-name could be a keyword indicating input-tensor, actual-value is a AbstractTensor whose facet = :exist (created by make-tensor ).","title":"[method] set-input"},{"location":"generic-tensor/#method-get-input","text":"(get-input (model Compiled-Composite) input-name) Reading all variables in the computation node, the method get-input returns an corresponding InputTensor of model.","title":"[method] get-input"},{"location":"generic-tensor/#parameter-no-grad","text":"[parameter] *no-grad* If t, no gradients are made for backwards.","title":"[parameter] *no-grad*"},{"location":"generic-tensor/#macro-with-no-grad","text":"(with-no-grad &body body) Under the body execution, the macro sets *no-grad* = t , that is, the built nodes are regarded as: no gradients are made for backwards.","title":"[macro] with-no-grad"},{"location":"generic-tensor/#function-parameter","text":"(parameter tensor) The function parameter computes all the previous nodes of the given tensor if any, returning the new tensor with requires-grad=t .","title":"[function] parameter"},{"location":"generic-tensor/#example_4","text":"(parameter (randn `(3 3)))","title":"Example"},{"location":"generic-tensor/#function-call-with-view","text":"(call-with-view function tensors &key (at-least-dim 1)) call-with-view is a general-purpose interface to iterate multi-dimensional tensor with considering offsets. (TODO: Example/Documents) function [lambda] an lambda function which receives variable1.view variable2.view ... as arguments, returning an list being compiled. tensors [list of abstracttensor] tensors to be called with. at-least-dim [fixnum] ... kernel-size See also: size-of stride-of offset-of NILNILNIL","title":"[function] call-with-view"},{"location":"generic-tensor/#function-shape-equal","text":"a=1, b=k => T a=1, b=2 => NIL ...Returns subscript-t if view is Subscript otherwise returns a view","title":"[function] shape-equal"},{"location":"generic-tensor/#compiling-options","text":"TODO","title":"Compiling Options"},{"location":"generic-tensor/#dtypes","text":"TODO","title":"Dtypes"},{"location":"install/","text":"Setting up Environments Are you new to Common Lisp? I know... there are few people who are attempted to do ML/DL in Common Lisp! while other languages provide a strong baseline and good platforms. However, I still believe in the clear benefits of doing such tasks on Common Lisp. I've been working with Common Lisp for the past two years, but I realise the attraction that no other language can replace it. At first glance, indeed this language has a strange syntax, and some features of the language may seem too much. but believe me! One day you will learn to use it. In fact, I guess cl-waffe2 is portable to ANSI Common Lisp, but not portable to non-lisp languages. Anyway, the first step is to set up Common Lisp Environment. I don't know which is best, and this is just my recommendations. 1. Installing Roswell Roswell is environment manager of Common Lisp (and much more!) https://github.com/roswell/roswell See the Readme.md and install Roswell 2. Installing SBCL There are many implementations of Common Lisp, and SBCL is one of the processing system. As of this writing(2023/07/02), some features of cl-waffe2 are SBCL-dependent, so this one is recommended. With roswell: $ ros install sbcl $ ros use <Installed SBCL Version> $ ros run # REPL is launched. should work. 3. Setting up IDE (Optional) I guess It's a pity to write Common Lisp without REPL. There are a lot of options, but as far as I know, Emacs with SLIME or Lem is widely supported choice. Installing cl-waffe2 With roswell, the latest repository can be fetched which is also recognised by quicklisp $ ros install hikettei/cl-waffe2 Another option is to load cl-waffe2.asd configurations manually after cloning cl-waffe2 github repos. $ git clone <Repository> $ cd ./cl-waffe2 $ ros run # start repl $ (load \"cl-waffe2.asd\") $ (ql:quickload :cl-waffe2) $ (in-pacakge :cl-waffe2-repl) With quicklisp: (It's going to take a while...) OpenBLAS Backend In your init file, (e.g.: ~/.roswell/init.lisp or ~/.sbclrc ), add the code below for example. (change the path depending on your environment). ;; In ~~/.sbclrc for example: (defparameter *cl-waffe-config* `((:libblas \\\"libblas.dylib for example\\\"))) One of cl-waffe2 backends CPUTensor loads the OpenBLAS shared library of the path written in the cl-user::*cl-waffe-config* parameter when cl-waffe2 loaded. CUDA Backend (Currently not supported yet...)","title":"Install"},{"location":"install/#setting-up-environments","text":"","title":"Setting up Environments"},{"location":"install/#are-you-new-to-common-lisp","text":"I know... there are few people who are attempted to do ML/DL in Common Lisp! while other languages provide a strong baseline and good platforms. However, I still believe in the clear benefits of doing such tasks on Common Lisp. I've been working with Common Lisp for the past two years, but I realise the attraction that no other language can replace it. At first glance, indeed this language has a strange syntax, and some features of the language may seem too much. but believe me! One day you will learn to use it. In fact, I guess cl-waffe2 is portable to ANSI Common Lisp, but not portable to non-lisp languages. Anyway, the first step is to set up Common Lisp Environment. I don't know which is best, and this is just my recommendations.","title":"Are you new to Common Lisp?"},{"location":"install/#1-installing-roswell","text":"Roswell is environment manager of Common Lisp (and much more!) https://github.com/roswell/roswell See the Readme.md and install Roswell","title":"1. Installing Roswell"},{"location":"install/#2-installing-sbcl","text":"There are many implementations of Common Lisp, and SBCL is one of the processing system. As of this writing(2023/07/02), some features of cl-waffe2 are SBCL-dependent, so this one is recommended. With roswell: $ ros install sbcl $ ros use <Installed SBCL Version> $ ros run # REPL is launched. should work.","title":"2. Installing SBCL"},{"location":"install/#3-setting-up-ide-optional","text":"I guess It's a pity to write Common Lisp without REPL. There are a lot of options, but as far as I know, Emacs with SLIME or Lem is widely supported choice.","title":"3. Setting up IDE (Optional)"},{"location":"install/#installing-cl-waffe2","text":"With roswell, the latest repository can be fetched which is also recognised by quicklisp $ ros install hikettei/cl-waffe2 Another option is to load cl-waffe2.asd configurations manually after cloning cl-waffe2 github repos. $ git clone <Repository> $ cd ./cl-waffe2 $ ros run # start repl $ (load \"cl-waffe2.asd\") $ (ql:quickload :cl-waffe2) $ (in-pacakge :cl-waffe2-repl) With quicklisp: (It's going to take a while...)","title":"Installing cl-waffe2"},{"location":"install/#openblas-backend","text":"In your init file, (e.g.: ~/.roswell/init.lisp or ~/.sbclrc ), add the code below for example. (change the path depending on your environment). ;; In ~~/.sbclrc for example: (defparameter *cl-waffe-config* `((:libblas \\\"libblas.dylib for example\\\"))) One of cl-waffe2 backends CPUTensor loads the OpenBLAS shared library of the path written in the cl-user::*cl-waffe-config* parameter when cl-waffe2 loaded.","title":"OpenBLAS Backend"},{"location":"install/#cuda-backend","text":"(Currently not supported yet...)","title":"CUDA Backend"},{"location":"nn/","text":"cl-waffe2/nn","title":"cl-waffe2/nn"},{"location":"nn/#cl-waffe2nn","text":"","title":"cl-waffe2/nn"},{"location":"nodes/","text":".katex img { object-fit: fill; padding: unset; display: block; position: absolute; width: 100%; height: inherit; } Formulate Neural Networks The package :cl-waffe2/vm.nodes provides a fundamental system for building neural networks using AbstractTensor . This package can be divided into three main parts. Shaping APIs ( :where ) defnode (The smallest unit of differentiable operations) defmodel (Operations consisted of defnode, and static functions) Shaping API When defining an operation in cl-waffe2 with a defnode macro, the shape of the matrix used in the operation must also be defined in the :where keyword. This is a Shaping API, and responsible for shape inspection of all operations. Introducing Subscript DSL I assume you have already seen defnode macro. This macro takes a strange syntax language after :where keyword. (defnode (TransposeNode (myself) :where (A[~ i j] -> A[~ j i]) ...)) (defnode (ScalarAdd (myself) :where (A[~] Scal[scal] -> A[~] where scal = 1) ...)) (defnode (ReshapeNode (myself tensor after &aux (before (shape tensor))) :where (A[before] -> A[after]) ...)) This is a DSL (Domain Specific Language) called Subscript DSL , which is used to notate the pointer and shape to be handled before and after the operation. For example, TransposeNode is said to be: Before and after the operation, we use the same pointer. A is a tensor with more than two dimensions, and after the operation, transposed the last two axes. (i.e.: A=(10 5 2), (10 2 5) is returned) ScalarAdd is said to be: The first argument A can be anything. The second argument Scal is a scalar tensor. The returned tensor shares the pointer with the given A . ReshapeNode is: Before and after the operation, pointers are common. The shape of A will be transformed from before into after Basic Grammar Let's start with learning the grammar. One line code of Subscript DSL follows this format: [Before The Operation] -> [After The Operation] where [symbol = expression (Optional)] ... Note that: the pharse where [symbol = expression (Optional)] ... is Optional One Subscript DSL place can include one line of code. [Before The Operation] and [After The Operation] has the common grammar rule. Let <Arguments> be a grammar rule of [Before The Operation] and [After The Operation], <Arguments> can be defined as: <Arguments> ::= <Arguments> <Argument> <Argument> ::= <PointerName> [ <SubScripts> ] | NIL <PointerName> ::= Symbol // the same as CL's symbol. <SubScripts> ::= <Subscripts> <Subscript> <Subscript> ::= Symbol | NIL To put it bluntly, can be a sequence of: PointerName[SubScripts] // SubScripts can be one of: [A], [A B] [~ i j] etc... Assigned task A[a b] B[a b] -> B[a b] In the DSL above, A and B indicates the name of pointer, they're not needed to be defined in advance. On the other hand a and b inside [ ... ], indicates subscripts of A and B , DSL's assigned work is to inference these undetermined symbols from: determined symbol from where pharse and symbols in arguments of constructor. Shape of the given inputs at runtime. If any, DSL compiles and display a report on Shape-Error before performing the operation. (!add (randn `(3 2)) (randn `(2 4))) ;; will produce... [cl-waffe] Shaping-Error: Couldn't step forward because of shape-error. The operation was : <Node: ADDNODE-CPUTENSOR (A[~] B[~] -> A[~])> Input(s) : ((3 2) (2 4)) Predicted Output(s) : ((3 2)) Here's a list of reports. 1. Couldn't idenfity ~: ~ is determined as 3 butgot: 2. Excepted ~ = (3 2), butgot: (2 4) Also, these reports could be helpful for you (calculated ignoring the first errors.) 2. Couldn't idenfity ~: ~ is determined as 2 butgot: 4. Excepted ~ = (3 2), butgot: (2 4) Determine Rules (defnode (ExampleNode (myself) :where (A[~ i j] B[~ j k] C[~ k i] -> C[~ k i]) ...)) Symbols used in subscripts has a two state: Determined (those that can say i=1, j=2!) Undetermined (those that cannot say i=1, j=2) Before doing (call (ExampleNode) ...) , we create a table which stores determined/undetermined symbols and corresponding values. [TABLE] ~ -> ? // Undetermined before runtime i -> ? // Undetermined before runtime j -> ? // Undetermined before runtime k -> ? // Undetermined before runtime The moment we do (call (ExampleNode) TensorA TensorB TensorC) , we will be able to inference the value of i j k from the shape of given TensorA, TensorB, and TensorC. For Example, Let TensorA be a 2x3x4 Matrix, then the table become: [TABLE] ~ -> 2 i -> 3 j -> 4 k -> ? Then continue to do the same thing for TensorB. Let TensorB be a 2x4x9 Matrix, then the table become: [TABLE] ~ -> 2 i -> 3 j -> 4 k -> 9 Last, applying this operation into TensorC, but what if I gave the wrong shape to TensorC? Let TensorC be a 999x999x999 Matrix. (Obviously this is wrong). [TABLE] ~ -> 2 // \u2260999 i -> 3 // \u2260999 j -> 4 // \u2260999 k -> 9 // \u2260999 All subscripts in the table do not match with 999, resuting in shape-error. In that case, we can try again the operation with giving the correct shape to TensorC. Let TensorC be 2x9x3 Matrix. [TABLE] ~ -> 2 // =2 i -> 3 // = 3 j -> 4 // k -> 9 // = 9 All subscripts passed! (puts error If there's still undetermined symbol.) Using the determined table, we can also inference the shape of output tensor. The returned tensor is the shape of (~ k i) , that is, (2 9 3) . This operation can be done in a chain of lazy-evaluated nodes. Now, moving on to another topic, subscripts can be one of them. [TABLE] a = 1 // Fixnum b = `(1 2) // List consisted of fixnum ~ = `(1 2 3) // ~ is a special symbol which represents batched-input. DSL flattens the list in the subscript. (e.g.: b=(1 2) in A[b] is the equivalent to A[1 2] ) Note that ~ is a reserved word by cl-waffe2 and has a special rule: ~ is used to express dimensions from 0 to N ~ can only be used once for one input of subscript. In tables, ~ is interpreted as one of: NIL or List In addition, ~ has a three behaviour: If ~ never appears in [Before The Operation] and [After The Operation] parts, the length of ~ could be Any. If ~ appears more than once, the length of ~ and content should be common. If ~ appears only in [After The Operation], returns error because we can't determine ~. In conclusion, I believe introducing Subscript DSL produces two benefits: Rigorous Shape Inspection in all operations with small code, and produce better Shape-Error (Initially I'm inspired in: nalgebra ). JIT Compiler can use a shape of given arguments in advance. (If only CL has a const-generics like Rust, Subscript DSL isn't needed anymore!). Initial value of table In order to give a initial value to tables, you can declare symbols with initial value. Using where pharse in :where form Add this form to your :where form. ;; Syntax is that: Symbol-Name = Expression (defnode (... :where (A[i] B[j] -> C[k] where i = 1 j = 2 k = 3) .... will produce: [TABLE] i = 1 j = 2 k = 3 Using arguments declared in constructor . (defnode (ExampleNode (self i) :where (A[~] -> A[i])) ...) Arguments used in constructor, will automatically interpreted as initial value . (e.g.: i is a initial value.) [TABLE] ~ = ? i = i That is, when ExampleNode is initialized with (ExampleNode 3) , the table become: [TABLE] ~ = ? i = 3 arguments of constructor API: create-subscript-p (create-subscript-p subscripts &key macroexpand fixed return-body) Inputs: macroexpand[Boolean] If t, displays the generated program. fixed[Boolean] If t, ~ is ignored. return-body[Boolean] If t, the returned is S-exp. Outputs: (values compiled-function To-Refer-Pointer-Idx Broadcastable_List) Example: (TODO) [macro] defnode (defnode ((abstract-name (self &rest constructor-arguments) &key (where t) (out-scalar-p nil) (save-for-backward nil) (slots nil) (backward nil) (documentation \"\")) &body constructor-body)) defnode is a macro to define computation nodes in cl-waffe2, which is a subclass of AbstractNode . The class defined is named after abstract-name , and they possess the following datum: Generic definition of forward, including Subscript DSL , (whch is transimission state of the operation), and slots which is shared at forward and backward time. (Optional) Generic definition of backward. Inputs abstract-name the macro defines a new class named after it. where the place to put Subscript DSL save-for-backward corresponding position of input arguments will produce a copy, which is used at backward time. backward (Optional) Any back-propagation described in define-impl is disabled; instead, the definitions given here are used. documentation docstring out-scalar-p Set t If the returned tensor is ScalarTensor. This can be dynamically modified via the accessor (out-scalar-p self) . Effects Defines a class named abstract-name Defines a function which is used to initialize the node named abstract-name Useful Tips In order to simplify parameter initialisation, if the keyword name of the :initarg is the same as the keyword name of the argument, the initialisation code is automatically generated. (defnode (ExampleNode (self arg) :slots ((arg :initarg :arg)))) (slot-value (ExampleNode 10) 'arg) ;; => 10 When to define backward? The backward follows this format: ((self dout dx dy ... dn) (values dx.grad dy.grad ... dn.grad)) dout is a previous node's gradient, and dx dy ... dn is a variables that used when forward. No guarantee that dx dy ... dn isn't being destructed due to in-place operation. If you need them in order to compute gradients, set :save-for-backward (t t ... t) at define-impl macro. Find the partial derivative of each variable according to the derivative of the composite function. The definition of backward must be placed either of defnode or define-impl. Basically, if the original defnode describes the backward, define-impl's backward is ignored. 1. ================================================================= AddNode (defnode) <- Place Backward | |-> (AddNode :CPUTensor) (define-impl) |-> (AddNode :LispTensor) (define-impl) |-> (AddNode :CUDATensor) (define-impl) ================================================================= 2. ================================================================= AddNode (defnode) <- Backward=nil | |-> (AddNode :CPUTensor) (define-impl) <- place backward |-> (AddNode :LispTensor) (define-impl) <- place backward |-> (AddNode :CUDATensor) (define-impl) <- place backward ================================================================= Depending on *using-backend* , the implementation to use is determined at node-building time. See also: with-devices. How to define backward? g ( d o u t , d x i n , d y i n , . . . , d n i n ) \u225c M o v e ( d x o u t , d o u t \u00d7 d x g r a d ) , M o v e ( d y o u t , d o u t \u00d7 d y g r a d ) , . . . , M o v e ( d n o u t , d o u t \u00d7 d n g r a d ) g(dout, dx_{in}, dy_{in}, ..., dn_{in}) \\triangleq \\\\ Move(dx_{out}, {dout} \\times {dx_{grad}}),\\\\ Move(dy_{out}, {dout} \\times {dy_{grad}}),\\\\ ...,\\\\ Move(dn_{out}, {dout} \\times {dn_{grad}}) g ( d o u t , d x in \u200b , d y in \u200b , ... , d n in \u200b ) \u225c M o v e ( d x o u t \u200b , d o u t \u00d7 d x g r a d \u200b ) , M o v e ( d y o u t \u200b , d o u t \u00d7 d y g r a d \u200b ) , ... , M o v e ( d n o u t \u200b , d o u t \u00d7 d n g r a d \u200b ) :save-for-backward (t t) :backward ((self dout dx dy) (values (!mul dout dy) (!mul dout dx))) self is a place to pass the node class. dout is a AbstractTensor of previous node's gradient. dx, dy, ..., dn are variables used in forward. In the case of the tensor is computed as In-place , there's no guarantee that variables aren't destructed. So, to ensure that variables remains as it was, set :save-for-backward at corresponding positions if the variable is needed to compute gradient. According to the derivative of the composite function , :backward definition should return next node's dout following this form: (values dx.grad dy.grad ... dn.grad) After the computing, cl-waffe2 automatically selects where to store the result, and moves it. x i n = { x s a v e f o r b a c k w a r d SaveForBackward is t x otherwise \\begin{equation} x_{in}= \\begin{cases} x_{saveforbackward} & \\text{SaveForBackward is t} \\\\ \\text{x} & \\text{otherwise} \\end{cases} \\end{equation} x in \u200b = { x s a v e f or ba c k w a r d \u200b x \u200b SaveForBackward is t otherwise \u200b \u200b \u200b x o u t = { x c o p y If the tensor is a ExistTensor or cause conflicts x If the tensor make no conflicts. \\begin{equation} x_{out}= \\begin{cases} x_{copy} & \\text{If the tensor is a ExistTensor or cause conflicts} \\\\ \\text{x} & \\text{If the tensor make no conflicts.} \\end{cases} \\end{equation} x o u t \u200b = { x co p y \u200b x \u200b If the tensor is a ExistTensor or cause conflicts If the tensor make no conflicts. \u200b \u200b \u200b ( x is a variable called with (forward node &rest inputs) function.) Example (defnode (MatMulNode (myself dtype &key transpose-a transpose-b) :where (A[~ i j] B[~ j k] C[~ i k] -> C[~ i k]) :slots ((transpose-a :initarg :transpose-a :type boolean :reader trans-a?) (transpose-b :initarg :transpose-b :type boolean :reader trans-b?)) :documentation \"gemm\" :backward ((self dout da db do) (declare (ignore do)) (values (!matmul dout (!t db)) (!matmul (!t da) dout) nil)))) (MatmulNode :float) ;; <Node: MATMULNODE-CPUTENSOR (A[~ I J] B[~ J K] C[~ I K] -> C[~ I K])> [macro] define-impl (define-impl ((abstract-name &key (device t) (cache-when-compiled t) (reject-p nil)) &key save-for-backward forward backward) Gives an implementation to AbstractNode . Inputs device Set here symbol the impl working on. The symbol must be a subclass of AbstractTensor . If t, the impl has the highest priority assigned to all implementations. reject-p[null or predicate] Set here predicator, If the predicator is t, the implementation refures to be dispatched. save-for-backward The corresponding variable which is t will be made a copy when forward. (e.g.: forward=(x y) and save-for-backward=(t nil) , x is copied, y isn't copied.) cache-when-compiled[boolean] If t, call-with-view function used in :forward will be cached when compiling. Set nil to disable this behaviour. forward Place the expanded lisp-code for forward propagation. backward Place the definition of backward as the same forward of defnode does. Tips: reject-p One of the practical usage of reject-p is to restrict dtypes that implementation can handle. reject-p takes an function: #'(lambda (&rest inputs) ...) where inputs is constructor-arguments in defnode. (e.g.: (AddNode :float) -> inputs=(list :float) ). AddNode for CPUTensor only supports dense matrix. (define-impl (AddNode :device CPUTensor :reject-p (supported-dtypes-are 0 :float :double)) :forward ((self x y) `(,@(expand-axpy-form x y) ,x))) The macro supported-dtypes-are returns an predicator which returns nil if the first argument is the equivalent to :float or :double . forward/backward forward/backward is given as: ((self &rest arguments) body) [generic] forward (forward node &rest inputs) Reading an state of *using-devies* and the given nodes, the method forward returns a new tensor with applied the forward definition of a given node with inputs lazily. The moment forward is called, the computation node is constructed for building forward/backward kernel. Since then, forward is AbstractNode dedicated operation, not applied into calling Composite . Example (forward (AddNode :float) (randn `(3 3)) (randn `(3 3))) {CPUTENSOR[float] :shape (3 3) :named ChainTMP31939 :vec-state [maybe-not-computed] ((0.109944925 0.42675912 1.9701254) (1.5735719 0.7928889 1.1698933) (0.08926714 0.0937486 -1.1063566)) :facet :input :requires-grad NIL :backward <Node: ADDNODE-CPUTENSOR (A[~] B[~] -> A[~])>} [class] defmodel (defmodel ((name (self-name &rest constructor-arguments) &key (slots nil) (initargs) (where nil) (on-call-> nil) (documentation \"\")) &body constructor-body) defmodel defines a new Composite class which describes network structures with using lazy-evaluated tensor. Viewing the set of AbstractNode as a single cohesive entity, you can formulate the forward propagation in on-call-> keyword. Composite is used as a neural network model if used as a merely data structure, but combined with define-composite-function , Composite can also define a single statically-operation function from a set of nodes. A new Composite class is initialized with (name &rest inputs) function, being called with a call method. Effects defines a class named name defines a function named name with the constructor-arguments and constructor-body. Inputs name[Symbol] the macro defines an class and constructor function named after it. (self-name &rest constructor-arguments) An initializer form of constructor function . slots ((slot-option1) (slot-option2) ...) Parameters of the inherited Composite class. It has the same syntax as defclass slots initargs (:accessor-name1 accessor-init-form1 :accessor-name2 accessor-init-form2 ... Unlike structures, CLOS classes are somewhat more cumbersome to initialise. To make this simple, this argument was introduced. Describe here initializer form in advance. documentation[String] on-call-> [One of: nil symbol-name function list] on-call-> is used to control the behaviour of call function. where[Subscript DSL] (Optional) Describe the state of the Tensor before and after on-call-> Example (defmodel (ExampleLayer (self features) ;; Options/Utils Here, :slots ((param :initarg :param)) :initargs (:param (make-tensor `(,features) :requires-grad t)) :documentation \"ExampleLayer is a ...\") ;; After make-instance is called, the form below is called. ;; make-instance -> make-instance :after -> this form. (print self) ;; <- Initialized ExampleLayer (print features) ;; <- constructor-arguments are also used here. (print \"ExampleLayer is created!\")) ;; The model you created, works like: (let ((layer (ExampleLayer 10))) (call layer ...)) (defmodel (Softmax-Model (self) :where (X[~] -> [~]) :on-call-> ((self x) (declare (ignore self)) (let* ((x1 (!sub x (!mean x :axis 1 :keepdims t))) (z (!sum (!exp x1) :axis 1 :keepdims t))) (!div (!exp x1) z))))) ;; Using Lazily... (proceed (call (Softmax-Model) (randn `(10 10))) {CPUTENSOR[float] :shape (10 10) :named ChainTMP33497 :vec-state [computed] ((0.04800622 0.118814774 0.050377533 ~ 0.053051848 0.050124187 0.25575548) (0.15909052 0.11368358 0.12642372 ~ 0.114795394 0.033397682 0.07605342) ... (0.035624444 0.24828684 0.109363265 ~ 0.020787988 0.027314318 0.04515641) (0.030307569 0.24117047 0.03900468 ~ 0.014522874 0.036584295 0.0971196)) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} ;; Defines a statically working function. (define-composite-function (Softmax-Model) !softmax-static) (!softmax-static (randn `(10 10))) {CPUTENSOR[float] :shape (10 10) :named ChainTMP33788 ((0.16722792 0.018530384 0.014159603 ~ 0.035353966 0.06128503 0.13559735) (0.14498742 0.11881006 0.0692616 ~ 0.03911829 0.10358454 0.02131605) ... (0.055657785 0.44042623 0.030706322 ~ 0.11048273 0.0097645 0.11959953) (0.059088983 0.11067564 0.120767005 ~ 0.15042976 0.06570089 0.20548664)) :facet :input :requires-grad NIL :backward NIL} How to use on-call-> form? In the keyword on-call-> , describe the behaviour when called with a call function following this forms: on-call-> = nil In that case, cl-waffe2 calls the call method when doing forward propagation of the model. on-call-> is a symbol-name cl-waffe2 calls the function named symbol-name . For example, setting :on-call-> = call-example-layer and defining a call-example-layer method. (defmethod call-example-layer ((model ExampleLayer) x y) (print \"call-example-layer is used!\")) (call (ExampleLayer 10) tensor) ;; call-example-layer is used! on-call-> is a function name or a lambda. cl-waffe2 calls the given lambda function as a forward propagation. on-call-> is a list (Example) :on-call-> ((self x) (!sin x)) This argument is expanded into #'(lambda ,@on-call->) and works as well as 3. call (call model &rest inputs) call is a generic function which is used to :forward / :on-call-> forms for an AbstractNode / Composite class respectively. with-devices The macro with-devices declares the priority of dispatching nodes. Input backend-priority An list of device's name (e.g.: CPUTensor, LispTensor...) Devices on the left have higher priority. Example Let ATensor and BTensor be a pointer compatible, and subclass of AbstractTensor , and operations defined is following: ATensor has !add. BTensor has !mul. (setq a (make-tensor `(10 10))) ;; The tensor a is ATensor. ;; (Priority1=ATensor Priority2=BTensor) (with-devices (ATensor BTensor) (!add a (!mul a a))) cl-waffe2's backend dispatching rule is following: If the priority 1 backend does not have an implementation of the specified operation, check if the priority 2 backend does, if it still does not have it, 3, 4... and so on. The order of priority would be `(,@backend-priority ScalarTensor t). (t is a special name, and it implys the implement works for all the backends.) (with-devices (LispTensor CPUTensor) (!add a b)) [macro] define-and-impl-node (define-and-impl-node (abstract-name (self &rest constructor-arguments) &key (device t) (cache-when-compiled t) (reject-p nil) (where t) (out-scalar-p nil) (slots nil) (save-for-backward nil) (forward nil) (backward nil) (documentation \"\"))) Expands defnode and define-impl at the same time. [macro] define-composite-function (define-composite-function composite-init-form function-name &key (dtype t) (order :column) (compile-mode :default)) Tracing the on-call-> form of a given composite-init-form, the macro define-composite-function defines a function of calling on-call-> statically. On the condition where composite should be defined as polymorphic, the function is also defined as generic definition/dispatching, otherwise, defines as a single defun form. Inputs composite-init-form Set here an initform of Composite , to be traced. function-name the compiled function is defined as this name. :dtype[boolean or keyword] Set t to make compiled function work on any dtypes, or set keyword to use. order[keyword] Element major. compile-mode[compile-mode-t] compiling option. [class] Composite [class] Composite Composite is a fundamental datatype for all neural network models. The name composite is so named because it is used to bundle computation nodes constructed by defnode. In cl-waffe2, All models should be a subtype of this class, and shall return a forward propagation computation node using the call function. In order to define your model with Composite, two methods are available. Extend Composite Class (Slightly Complicated) First, define your class with extending Composite Class. (defclass LinearModel (Composite) ((weight ...) ; <- set parameters here. (bias ...)) Second, define forwarrd step with overriding call method. (defmethod call ((model LinearModel) &rest inputs) ... ) It should work like: (call (make-instance 'LinearModel in-features out-features) args1 ...) Using defmodel macro The defmodel macro simplifies the above redundant notation and also solves the problem that call can only use &rest as an argument. Therefore, I'm depcrecated with the method above, instead, use defmacro. For detailed usage, see the documentation of defmacro. [class] AbstractNode [class] AbstractNode The class AbstractNode is a fundamental object of describing computation nodes in cl-waffe. AbstractNode must possess following: Transimission State Slots (for passing forward/backward) Variables (for building computation nodes) with-instant-kernel (with-instant-kernel tensor &body body) Continues the computation node following tensor with embedding an instant-kernel . Instant is Lisp code that can be embedded in compiled functions. Embedding Lisp Code for building-time. (setq a (randn `(10 10))) (with-instant-kernel a (print a)) ;; -> (print a) is evaluated Embedding Lisp Code for compile-time. (setq a (randn `(10 10))) (with-instant-kernel a `(print ,a)) ;; -> (print a) isn't evaluated (funcall (build *)) ;; -> (print a) will be evaluated. Note that (equal (with-instant-kernel a) a) is NIL , that is, the returned value of this macro must be followed by a calculation node. If the return value of body can be expanded as a macro, the values are compiled together at JIT compile time. Otherwise, the given tensor is returned as is.","title":"cl-waffe2/vm.nodes"},{"location":"nodes/#formulate-neural-networks","text":"The package :cl-waffe2/vm.nodes provides a fundamental system for building neural networks using AbstractTensor . This package can be divided into three main parts. Shaping APIs ( :where ) defnode (The smallest unit of differentiable operations) defmodel (Operations consisted of defnode, and static functions)","title":"Formulate Neural Networks"},{"location":"nodes/#shaping-api","text":"When defining an operation in cl-waffe2 with a defnode macro, the shape of the matrix used in the operation must also be defined in the :where keyword. This is a Shaping API, and responsible for shape inspection of all operations.","title":"Shaping API"},{"location":"nodes/#introducing-subscript-dsl","text":"I assume you have already seen defnode macro. This macro takes a strange syntax language after :where keyword. (defnode (TransposeNode (myself) :where (A[~ i j] -> A[~ j i]) ...)) (defnode (ScalarAdd (myself) :where (A[~] Scal[scal] -> A[~] where scal = 1) ...)) (defnode (ReshapeNode (myself tensor after &aux (before (shape tensor))) :where (A[before] -> A[after]) ...)) This is a DSL (Domain Specific Language) called Subscript DSL , which is used to notate the pointer and shape to be handled before and after the operation. For example, TransposeNode is said to be: Before and after the operation, we use the same pointer. A is a tensor with more than two dimensions, and after the operation, transposed the last two axes. (i.e.: A=(10 5 2), (10 2 5) is returned) ScalarAdd is said to be: The first argument A can be anything. The second argument Scal is a scalar tensor. The returned tensor shares the pointer with the given A . ReshapeNode is: Before and after the operation, pointers are common. The shape of A will be transformed from before into after","title":"Introducing Subscript DSL"},{"location":"nodes/#basic-grammar","text":"Let's start with learning the grammar. One line code of Subscript DSL follows this format: [Before The Operation] -> [After The Operation] where [symbol = expression (Optional)] ... Note that: the pharse where [symbol = expression (Optional)] ... is Optional One Subscript DSL place can include one line of code. [Before The Operation] and [After The Operation] has the common grammar rule. Let <Arguments> be a grammar rule of [Before The Operation] and [After The Operation], <Arguments> can be defined as: <Arguments> ::= <Arguments> <Argument> <Argument> ::= <PointerName> [ <SubScripts> ] | NIL <PointerName> ::= Symbol // the same as CL's symbol. <SubScripts> ::= <Subscripts> <Subscript> <Subscript> ::= Symbol | NIL To put it bluntly, can be a sequence of: PointerName[SubScripts] // SubScripts can be one of: [A], [A B] [~ i j] etc...","title":"Basic Grammar"},{"location":"nodes/#assigned-task","text":"A[a b] B[a b] -> B[a b] In the DSL above, A and B indicates the name of pointer, they're not needed to be defined in advance. On the other hand a and b inside [ ... ], indicates subscripts of A and B , DSL's assigned work is to inference these undetermined symbols from: determined symbol from where pharse and symbols in arguments of constructor. Shape of the given inputs at runtime. If any, DSL compiles and display a report on Shape-Error before performing the operation. (!add (randn `(3 2)) (randn `(2 4))) ;; will produce... [cl-waffe] Shaping-Error: Couldn't step forward because of shape-error. The operation was : <Node: ADDNODE-CPUTENSOR (A[~] B[~] -> A[~])> Input(s) : ((3 2) (2 4)) Predicted Output(s) : ((3 2)) Here's a list of reports. 1. Couldn't idenfity ~: ~ is determined as 3 butgot: 2. Excepted ~ = (3 2), butgot: (2 4) Also, these reports could be helpful for you (calculated ignoring the first errors.) 2. Couldn't idenfity ~: ~ is determined as 2 butgot: 4. Excepted ~ = (3 2), butgot: (2 4)","title":"Assigned task"},{"location":"nodes/#determine-rules","text":"(defnode (ExampleNode (myself) :where (A[~ i j] B[~ j k] C[~ k i] -> C[~ k i]) ...)) Symbols used in subscripts has a two state: Determined (those that can say i=1, j=2!) Undetermined (those that cannot say i=1, j=2) Before doing (call (ExampleNode) ...) , we create a table which stores determined/undetermined symbols and corresponding values. [TABLE] ~ -> ? // Undetermined before runtime i -> ? // Undetermined before runtime j -> ? // Undetermined before runtime k -> ? // Undetermined before runtime The moment we do (call (ExampleNode) TensorA TensorB TensorC) , we will be able to inference the value of i j k from the shape of given TensorA, TensorB, and TensorC. For Example, Let TensorA be a 2x3x4 Matrix, then the table become: [TABLE] ~ -> 2 i -> 3 j -> 4 k -> ? Then continue to do the same thing for TensorB. Let TensorB be a 2x4x9 Matrix, then the table become: [TABLE] ~ -> 2 i -> 3 j -> 4 k -> 9 Last, applying this operation into TensorC, but what if I gave the wrong shape to TensorC? Let TensorC be a 999x999x999 Matrix. (Obviously this is wrong). [TABLE] ~ -> 2 // \u2260999 i -> 3 // \u2260999 j -> 4 // \u2260999 k -> 9 // \u2260999 All subscripts in the table do not match with 999, resuting in shape-error. In that case, we can try again the operation with giving the correct shape to TensorC. Let TensorC be 2x9x3 Matrix. [TABLE] ~ -> 2 // =2 i -> 3 // = 3 j -> 4 // k -> 9 // = 9 All subscripts passed! (puts error If there's still undetermined symbol.) Using the determined table, we can also inference the shape of output tensor. The returned tensor is the shape of (~ k i) , that is, (2 9 3) . This operation can be done in a chain of lazy-evaluated nodes. Now, moving on to another topic, subscripts can be one of them. [TABLE] a = 1 // Fixnum b = `(1 2) // List consisted of fixnum ~ = `(1 2 3) // ~ is a special symbol which represents batched-input. DSL flattens the list in the subscript. (e.g.: b=(1 2) in A[b] is the equivalent to A[1 2] ) Note that ~ is a reserved word by cl-waffe2 and has a special rule: ~ is used to express dimensions from 0 to N ~ can only be used once for one input of subscript. In tables, ~ is interpreted as one of: NIL or List In addition, ~ has a three behaviour: If ~ never appears in [Before The Operation] and [After The Operation] parts, the length of ~ could be Any. If ~ appears more than once, the length of ~ and content should be common. If ~ appears only in [After The Operation], returns error because we can't determine ~. In conclusion, I believe introducing Subscript DSL produces two benefits: Rigorous Shape Inspection in all operations with small code, and produce better Shape-Error (Initially I'm inspired in: nalgebra ). JIT Compiler can use a shape of given arguments in advance. (If only CL has a const-generics like Rust, Subscript DSL isn't needed anymore!).","title":"Determine Rules"},{"location":"nodes/#initial-value-of-table","text":"In order to give a initial value to tables, you can declare symbols with initial value. Using where pharse in :where form Add this form to your :where form. ;; Syntax is that: Symbol-Name = Expression (defnode (... :where (A[i] B[j] -> C[k] where i = 1 j = 2 k = 3) .... will produce: [TABLE] i = 1 j = 2 k = 3 Using arguments declared in constructor . (defnode (ExampleNode (self i) :where (A[~] -> A[i])) ...) Arguments used in constructor, will automatically interpreted as initial value . (e.g.: i is a initial value.) [TABLE] ~ = ? i = i That is, when ExampleNode is initialized with (ExampleNode 3) , the table become: [TABLE] ~ = ? i = 3 arguments of constructor","title":"Initial value of table"},{"location":"nodes/#api-create-subscript-p","text":"(create-subscript-p subscripts &key macroexpand fixed return-body) Inputs: macroexpand[Boolean] If t, displays the generated program. fixed[Boolean] If t, ~ is ignored. return-body[Boolean] If t, the returned is S-exp. Outputs: (values compiled-function To-Refer-Pointer-Idx Broadcastable_List) Example: (TODO)","title":"API: create-subscript-p"},{"location":"nodes/#macro-defnode","text":"(defnode ((abstract-name (self &rest constructor-arguments) &key (where t) (out-scalar-p nil) (save-for-backward nil) (slots nil) (backward nil) (documentation \"\")) &body constructor-body)) defnode is a macro to define computation nodes in cl-waffe2, which is a subclass of AbstractNode . The class defined is named after abstract-name , and they possess the following datum: Generic definition of forward, including Subscript DSL , (whch is transimission state of the operation), and slots which is shared at forward and backward time. (Optional) Generic definition of backward.","title":"[macro] defnode"},{"location":"nodes/#inputs","text":"abstract-name the macro defines a new class named after it. where the place to put Subscript DSL save-for-backward corresponding position of input arguments will produce a copy, which is used at backward time. backward (Optional) Any back-propagation described in define-impl is disabled; instead, the definitions given here are used. documentation docstring out-scalar-p Set t If the returned tensor is ScalarTensor. This can be dynamically modified via the accessor (out-scalar-p self) .","title":"Inputs"},{"location":"nodes/#effects","text":"Defines a class named abstract-name Defines a function which is used to initialize the node named abstract-name","title":"Effects"},{"location":"nodes/#useful-tips","text":"In order to simplify parameter initialisation, if the keyword name of the :initarg is the same as the keyword name of the argument, the initialisation code is automatically generated. (defnode (ExampleNode (self arg) :slots ((arg :initarg :arg)))) (slot-value (ExampleNode 10) 'arg) ;; => 10","title":"Useful Tips"},{"location":"nodes/#when-to-define-backward","text":"The backward follows this format: ((self dout dx dy ... dn) (values dx.grad dy.grad ... dn.grad)) dout is a previous node's gradient, and dx dy ... dn is a variables that used when forward. No guarantee that dx dy ... dn isn't being destructed due to in-place operation. If you need them in order to compute gradients, set :save-for-backward (t t ... t) at define-impl macro. Find the partial derivative of each variable according to the derivative of the composite function. The definition of backward must be placed either of defnode or define-impl. Basically, if the original defnode describes the backward, define-impl's backward is ignored. 1. ================================================================= AddNode (defnode) <- Place Backward | |-> (AddNode :CPUTensor) (define-impl) |-> (AddNode :LispTensor) (define-impl) |-> (AddNode :CUDATensor) (define-impl) ================================================================= 2. ================================================================= AddNode (defnode) <- Backward=nil | |-> (AddNode :CPUTensor) (define-impl) <- place backward |-> (AddNode :LispTensor) (define-impl) <- place backward |-> (AddNode :CUDATensor) (define-impl) <- place backward ================================================================= Depending on *using-backend* , the implementation to use is determined at node-building time. See also: with-devices.","title":"When to define backward?"},{"location":"nodes/#how-to-define-backward","text":"g ( d o u t , d x i n , d y i n , . . . , d n i n ) \u225c M o v e ( d x o u t , d o u t \u00d7 d x g r a d ) , M o v e ( d y o u t , d o u t \u00d7 d y g r a d ) , . . . , M o v e ( d n o u t , d o u t \u00d7 d n g r a d ) g(dout, dx_{in}, dy_{in}, ..., dn_{in}) \\triangleq \\\\ Move(dx_{out}, {dout} \\times {dx_{grad}}),\\\\ Move(dy_{out}, {dout} \\times {dy_{grad}}),\\\\ ...,\\\\ Move(dn_{out}, {dout} \\times {dn_{grad}}) g ( d o u t , d x in \u200b , d y in \u200b , ... , d n in \u200b ) \u225c M o v e ( d x o u t \u200b , d o u t \u00d7 d x g r a d \u200b ) , M o v e ( d y o u t \u200b , d o u t \u00d7 d y g r a d \u200b ) , ... , M o v e ( d n o u t \u200b , d o u t \u00d7 d n g r a d \u200b ) :save-for-backward (t t) :backward ((self dout dx dy) (values (!mul dout dy) (!mul dout dx))) self is a place to pass the node class. dout is a AbstractTensor of previous node's gradient. dx, dy, ..., dn are variables used in forward. In the case of the tensor is computed as In-place , there's no guarantee that variables aren't destructed. So, to ensure that variables remains as it was, set :save-for-backward at corresponding positions if the variable is needed to compute gradient. According to the derivative of the composite function , :backward definition should return next node's dout following this form: (values dx.grad dy.grad ... dn.grad) After the computing, cl-waffe2 automatically selects where to store the result, and moves it. x i n = { x s a v e f o r b a c k w a r d SaveForBackward is t x otherwise \\begin{equation} x_{in}= \\begin{cases} x_{saveforbackward} & \\text{SaveForBackward is t} \\\\ \\text{x} & \\text{otherwise} \\end{cases} \\end{equation} x in \u200b = { x s a v e f or ba c k w a r d \u200b x \u200b SaveForBackward is t otherwise \u200b \u200b \u200b x o u t = { x c o p y If the tensor is a ExistTensor or cause conflicts x If the tensor make no conflicts. \\begin{equation} x_{out}= \\begin{cases} x_{copy} & \\text{If the tensor is a ExistTensor or cause conflicts} \\\\ \\text{x} & \\text{If the tensor make no conflicts.} \\end{cases} \\end{equation} x o u t \u200b = { x co p y \u200b x \u200b If the tensor is a ExistTensor or cause conflicts If the tensor make no conflicts. \u200b \u200b \u200b ( x is a variable called with (forward node &rest inputs) function.)","title":"How to define backward?"},{"location":"nodes/#example","text":"(defnode (MatMulNode (myself dtype &key transpose-a transpose-b) :where (A[~ i j] B[~ j k] C[~ i k] -> C[~ i k]) :slots ((transpose-a :initarg :transpose-a :type boolean :reader trans-a?) (transpose-b :initarg :transpose-b :type boolean :reader trans-b?)) :documentation \"gemm\" :backward ((self dout da db do) (declare (ignore do)) (values (!matmul dout (!t db)) (!matmul (!t da) dout) nil)))) (MatmulNode :float) ;; <Node: MATMULNODE-CPUTENSOR (A[~ I J] B[~ J K] C[~ I K] -> C[~ I K])>","title":"Example"},{"location":"nodes/#macro-define-impl","text":"(define-impl ((abstract-name &key (device t) (cache-when-compiled t) (reject-p nil)) &key save-for-backward forward backward) Gives an implementation to AbstractNode .","title":"[macro] define-impl"},{"location":"nodes/#inputs_1","text":"device Set here symbol the impl working on. The symbol must be a subclass of AbstractTensor . If t, the impl has the highest priority assigned to all implementations. reject-p[null or predicate] Set here predicator, If the predicator is t, the implementation refures to be dispatched. save-for-backward The corresponding variable which is t will be made a copy when forward. (e.g.: forward=(x y) and save-for-backward=(t nil) , x is copied, y isn't copied.) cache-when-compiled[boolean] If t, call-with-view function used in :forward will be cached when compiling. Set nil to disable this behaviour. forward Place the expanded lisp-code for forward propagation. backward Place the definition of backward as the same forward of defnode does.","title":"Inputs"},{"location":"nodes/#tips-reject-p","text":"One of the practical usage of reject-p is to restrict dtypes that implementation can handle. reject-p takes an function: #'(lambda (&rest inputs) ...) where inputs is constructor-arguments in defnode. (e.g.: (AddNode :float) -> inputs=(list :float) ). AddNode for CPUTensor only supports dense matrix. (define-impl (AddNode :device CPUTensor :reject-p (supported-dtypes-are 0 :float :double)) :forward ((self x y) `(,@(expand-axpy-form x y) ,x))) The macro supported-dtypes-are returns an predicator which returns nil if the first argument is the equivalent to :float or :double .","title":"Tips: reject-p"},{"location":"nodes/#forwardbackward","text":"forward/backward is given as: ((self &rest arguments) body)","title":"forward/backward"},{"location":"nodes/#generic-forward","text":"(forward node &rest inputs) Reading an state of *using-devies* and the given nodes, the method forward returns a new tensor with applied the forward definition of a given node with inputs lazily. The moment forward is called, the computation node is constructed for building forward/backward kernel. Since then, forward is AbstractNode dedicated operation, not applied into calling Composite .","title":"[generic] forward"},{"location":"nodes/#example_1","text":"(forward (AddNode :float) (randn `(3 3)) (randn `(3 3))) {CPUTENSOR[float] :shape (3 3) :named ChainTMP31939 :vec-state [maybe-not-computed] ((0.109944925 0.42675912 1.9701254) (1.5735719 0.7928889 1.1698933) (0.08926714 0.0937486 -1.1063566)) :facet :input :requires-grad NIL :backward <Node: ADDNODE-CPUTENSOR (A[~] B[~] -> A[~])>}","title":"Example"},{"location":"nodes/#class-defmodel","text":"(defmodel ((name (self-name &rest constructor-arguments) &key (slots nil) (initargs) (where nil) (on-call-> nil) (documentation \"\")) &body constructor-body) defmodel defines a new Composite class which describes network structures with using lazy-evaluated tensor. Viewing the set of AbstractNode as a single cohesive entity, you can formulate the forward propagation in on-call-> keyword. Composite is used as a neural network model if used as a merely data structure, but combined with define-composite-function , Composite can also define a single statically-operation function from a set of nodes. A new Composite class is initialized with (name &rest inputs) function, being called with a call method.","title":"[class] defmodel"},{"location":"nodes/#effects_1","text":"defines a class named name defines a function named name with the constructor-arguments and constructor-body.","title":"Effects"},{"location":"nodes/#inputs_2","text":"name[Symbol] the macro defines an class and constructor function named after it. (self-name &rest constructor-arguments) An initializer form of constructor function . slots ((slot-option1) (slot-option2) ...) Parameters of the inherited Composite class. It has the same syntax as defclass slots initargs (:accessor-name1 accessor-init-form1 :accessor-name2 accessor-init-form2 ... Unlike structures, CLOS classes are somewhat more cumbersome to initialise. To make this simple, this argument was introduced. Describe here initializer form in advance. documentation[String] on-call-> [One of: nil symbol-name function list] on-call-> is used to control the behaviour of call function. where[Subscript DSL] (Optional) Describe the state of the Tensor before and after on-call->","title":"Inputs"},{"location":"nodes/#example_2","text":"(defmodel (ExampleLayer (self features) ;; Options/Utils Here, :slots ((param :initarg :param)) :initargs (:param (make-tensor `(,features) :requires-grad t)) :documentation \"ExampleLayer is a ...\") ;; After make-instance is called, the form below is called. ;; make-instance -> make-instance :after -> this form. (print self) ;; <- Initialized ExampleLayer (print features) ;; <- constructor-arguments are also used here. (print \"ExampleLayer is created!\")) ;; The model you created, works like: (let ((layer (ExampleLayer 10))) (call layer ...)) (defmodel (Softmax-Model (self) :where (X[~] -> [~]) :on-call-> ((self x) (declare (ignore self)) (let* ((x1 (!sub x (!mean x :axis 1 :keepdims t))) (z (!sum (!exp x1) :axis 1 :keepdims t))) (!div (!exp x1) z))))) ;; Using Lazily... (proceed (call (Softmax-Model) (randn `(10 10))) {CPUTENSOR[float] :shape (10 10) :named ChainTMP33497 :vec-state [computed] ((0.04800622 0.118814774 0.050377533 ~ 0.053051848 0.050124187 0.25575548) (0.15909052 0.11368358 0.12642372 ~ 0.114795394 0.033397682 0.07605342) ... (0.035624444 0.24828684 0.109363265 ~ 0.020787988 0.027314318 0.04515641) (0.030307569 0.24117047 0.03900468 ~ 0.014522874 0.036584295 0.0971196)) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} ;; Defines a statically working function. (define-composite-function (Softmax-Model) !softmax-static) (!softmax-static (randn `(10 10))) {CPUTENSOR[float] :shape (10 10) :named ChainTMP33788 ((0.16722792 0.018530384 0.014159603 ~ 0.035353966 0.06128503 0.13559735) (0.14498742 0.11881006 0.0692616 ~ 0.03911829 0.10358454 0.02131605) ... (0.055657785 0.44042623 0.030706322 ~ 0.11048273 0.0097645 0.11959953) (0.059088983 0.11067564 0.120767005 ~ 0.15042976 0.06570089 0.20548664)) :facet :input :requires-grad NIL :backward NIL}","title":"Example"},{"location":"nodes/#how-to-use-on-call-form","text":"In the keyword on-call-> , describe the behaviour when called with a call function following this forms:","title":"How to use on-call-&gt; form?"},{"location":"nodes/#on-call-nil","text":"In that case, cl-waffe2 calls the call method when doing forward propagation of the model.","title":"on-call-&gt; = nil"},{"location":"nodes/#on-call-is-a-symbol-name","text":"cl-waffe2 calls the function named symbol-name . For example, setting :on-call-> = call-example-layer and defining a call-example-layer method. (defmethod call-example-layer ((model ExampleLayer) x y) (print \"call-example-layer is used!\")) (call (ExampleLayer 10) tensor) ;; call-example-layer is used!","title":"on-call-&gt; is a symbol-name"},{"location":"nodes/#on-call-is-a-function-name-or-a-lambda","text":"cl-waffe2 calls the given lambda function as a forward propagation.","title":"on-call-&gt; is a function name or a lambda."},{"location":"nodes/#on-call-is-a-list","text":"(Example) :on-call-> ((self x) (!sin x)) This argument is expanded into #'(lambda ,@on-call->) and works as well as 3.","title":"on-call-&gt; is a list"},{"location":"nodes/#call","text":"(call model &rest inputs) call is a generic function which is used to :forward / :on-call-> forms for an AbstractNode / Composite class respectively.","title":"call"},{"location":"nodes/#with-devices","text":"The macro with-devices declares the priority of dispatching nodes.","title":"with-devices"},{"location":"nodes/#input","text":"backend-priority An list of device's name (e.g.: CPUTensor, LispTensor...) Devices on the left have higher priority.","title":"Input"},{"location":"nodes/#example_3","text":"Let ATensor and BTensor be a pointer compatible, and subclass of AbstractTensor , and operations defined is following: ATensor has !add. BTensor has !mul. (setq a (make-tensor `(10 10))) ;; The tensor a is ATensor. ;; (Priority1=ATensor Priority2=BTensor) (with-devices (ATensor BTensor) (!add a (!mul a a))) cl-waffe2's backend dispatching rule is following: If the priority 1 backend does not have an implementation of the specified operation, check if the priority 2 backend does, if it still does not have it, 3, 4... and so on. The order of priority would be `(,@backend-priority ScalarTensor t). (t is a special name, and it implys the implement works for all the backends.) (with-devices (LispTensor CPUTensor) (!add a b))","title":"Example"},{"location":"nodes/#macro-define-and-impl-node","text":"(define-and-impl-node (abstract-name (self &rest constructor-arguments) &key (device t) (cache-when-compiled t) (reject-p nil) (where t) (out-scalar-p nil) (slots nil) (save-for-backward nil) (forward nil) (backward nil) (documentation \"\"))) Expands defnode and define-impl at the same time.","title":"[macro] define-and-impl-node"},{"location":"nodes/#macro-define-composite-function","text":"(define-composite-function composite-init-form function-name &key (dtype t) (order :column) (compile-mode :default)) Tracing the on-call-> form of a given composite-init-form, the macro define-composite-function defines a function of calling on-call-> statically. On the condition where composite should be defined as polymorphic, the function is also defined as generic definition/dispatching, otherwise, defines as a single defun form.","title":"[macro] define-composite-function"},{"location":"nodes/#inputs_3","text":"composite-init-form Set here an initform of Composite , to be traced. function-name the compiled function is defined as this name. :dtype[boolean or keyword] Set t to make compiled function work on any dtypes, or set keyword to use. order[keyword] Element major. compile-mode[compile-mode-t] compiling option.","title":"Inputs"},{"location":"nodes/#class-composite","text":"[class] Composite Composite is a fundamental datatype for all neural network models. The name composite is so named because it is used to bundle computation nodes constructed by defnode. In cl-waffe2, All models should be a subtype of this class, and shall return a forward propagation computation node using the call function. In order to define your model with Composite, two methods are available.","title":"[class] Composite"},{"location":"nodes/#extend-composite-class-slightly-complicated","text":"First, define your class with extending Composite Class. (defclass LinearModel (Composite) ((weight ...) ; <- set parameters here. (bias ...)) Second, define forwarrd step with overriding call method. (defmethod call ((model LinearModel) &rest inputs) ... ) It should work like: (call (make-instance 'LinearModel in-features out-features) args1 ...)","title":"Extend Composite Class (Slightly Complicated)"},{"location":"nodes/#using-defmodel-macro","text":"The defmodel macro simplifies the above redundant notation and also solves the problem that call can only use &rest as an argument. Therefore, I'm depcrecated with the method above, instead, use defmacro. For detailed usage, see the documentation of defmacro.","title":"Using defmodel macro"},{"location":"nodes/#class-abstractnode","text":"[class] AbstractNode The class AbstractNode is a fundamental object of describing computation nodes in cl-waffe. AbstractNode must possess following: Transimission State Slots (for passing forward/backward) Variables (for building computation nodes)","title":"[class] AbstractNode"},{"location":"nodes/#with-instant-kernel","text":"(with-instant-kernel tensor &body body) Continues the computation node following tensor with embedding an instant-kernel . Instant is Lisp code that can be embedded in compiled functions.","title":"with-instant-kernel"},{"location":"nodes/#embedding-lisp-code-for-building-time","text":"(setq a (randn `(10 10))) (with-instant-kernel a (print a)) ;; -> (print a) is evaluated","title":"Embedding Lisp Code for building-time."},{"location":"nodes/#embedding-lisp-code-for-compile-time","text":"(setq a (randn `(10 10))) (with-instant-kernel a `(print ,a)) ;; -> (print a) isn't evaluated (funcall (build *)) ;; -> (print a) will be evaluated. Note that (equal (with-instant-kernel a) a) is NIL , that is, the returned value of this macro must be followed by a calculation node. If the return value of body can be expanded as a macro, the values are compiled together at JIT compile time. Otherwise, the given tensor is returned as is.","title":"Embedding Lisp Code for compile-time."},{"location":"optimizer/","text":"cl-waffe2/optimizers","title":"cl-waffe2/optimizers"},{"location":"optimizer/#cl-waffe2optimizers","text":"","title":"cl-waffe2/optimizers"},{"location":"overview/","text":".katex img { object-fit: fill; padding: unset; display: block; position: absolute; width: 100%; height: inherit; } A road to cl-waffe2 Project Structure Thank you for having interest in my project. Before we start the tutorial, let me explain the structure of cl-waffe2 package. Mainly, cl-waffe2 consists of the following packages. Fundamental System These two package form the basis of cl-waffe2: :cl-waffe2/vm.nodes :cl-waffe2/vm.generic-tensor The package :cl-waffe2/vm.nodes provides a system for constructing neural networks, including AbstractNode , Composite , Shaping API etc... On the other hand, :cl-waffe2/vm.generic-tensor provides features on AbstractTensor , including JIT Compiler , NodeTensor , Memory-Pool etc... Standard APIs (Figure: Dependencies of cl-waffe2) [CPUBackend ] [base-impl] --- [LispBackend] | [CUDABackend] ... | [vm.generic-tensor] [vm.nodes] :cl-waffe2/base-impl Using the basic system of cl-waffe2, :cl-waffe2/vm.generic-tensor and :cl-waffe2/vm.nodes , the package :cl-waffe2/base-impl provides a standard implementation of matrix (sometimes scalar) tensor operations. The operation we say is including: defun parts, and abstract definition of operation. Before we go any futher: cl-waffe2 is working on AbstractTensor (inspired in Julia's great idea, AbstractArray ), which separates implementation of the operation from the definition. In that respect, :cl-waffe2/base-impl provides the definition of operations, while the packages we about to mention provides implementation of operations. Standard Backends/Implemenetations As of this writing (2023/07/05), we provide two standard implementation of :cl-waffe2/base-impl , both of them are working on CPU. :cl-waffe2/backends.lisp :cl-waffe2/backends.cpu If only time and money would permit, I'm willing to implement CUDA/Metal Backends. :cl-waffe2/backends.lisp is work enough first, it is Portable (based on ANSI Common Lisp) and supports AVX2 but far from full speed . On the other hand :cl-waffe2/backends.cpu is accelerated by OpenBLAS (maybe MKL is ok) and other foreign backends, this is SBCL-Dependant and sometimes could be unsafe, but provides full speed . TODO: :cl-waffe2/backends.fastmath (NOT IMPLEMENTED YET!) (TO BE) Supporting vectorized mathematical functions, AVX512 instructions. Neural Network :cl-waffe2/nn ;; Provides Basic neural-network Implementations. :cl-waffe2/optimizers ;; Provides Basic Optimizers Utils :cl-waffe2 ;; Provides multi-threading APIs and config macros! :cl-waffe2/viz ;; Provides Vizualizing APIs of computation node etc... To Get Started! If you're going to start with defining a new package, It is recommended to :use the package to be used. Read the description above and select and describe the packages you think you need. (or you can just copy and paste it.) The lisp code below demonstrates an example case of :your-project-name package. (in-package :cl-user) (defpackage :your-project-name (:use :cl :cl-waffe2 :cl-waffe2/vm.generic-tensor :cl-waffe2/vm.nodes :cl-waffe2/base-impl :cl-waffe2/distributions :cl-waffe2/backends.lisp :cl-waffe2/backends.cpu :cl-waffe2/nn :cl-waffe2/optimizers :cl-waffe2/viz)) (in-package :your-project-name) ;; Your code follows... If you're working with REPL (or new to Common Lisp?), you can try cl-waffe2 features like this: $ ros run > (load \"cl-waffe2.asd\") # cl-waffe2.asd should be placed where SBCL can read it. > (ql:quickload :cl-waffe2) > (in-package :cl-waffe2-repl) ;; this is a playground place, and all features are available The tutorials below should be also working on REPL, (indeed, cl-waffe2 is REPL-friendly!), you can learn how cl-waffe2 works by copying and pasting the example codes. Basic: Building Computation Nodes Lazily Since Do not run until the node is optimized is a one of cl-waffe2 policy, all operations in cl-waffe2 is lazy evaluation unless defined by a special macro. Therefore, calling !add function which finds a sum of given arguments, the retuend tensor isn't still computed, only setting :vec-state = [maybe-not-computed] . (!add 3.0 2.0) {SCALARTENSOR[float] :named ChainTMP23305 :vec-state [maybe-not-computed] <<Not-Embodied (1) Tensor>> :facet :input :requires-grad NIL :backward <Node: SCALARANDSCALARADD-SCALARTENSOR (A[SCAL] B[SCAL] -> A[SCAL] WHERE SCAL = 1)>} :vec-state indicates the computation state of tensor, and it says exactly what it says. You can continue the operation by connecting the returned tensor and next operation. For example, the figure below in cl-waffe is represented as: o u t = 3 + 2 \u2217 4 out = 3 + 2 * 4 o u t = 3 + 2 \u2217 4 (defparameter out (!add 3 (!mul 2 4))) ;; out <- 3 + 2 * 4 To obtain the state in which the operation is performed, calling the function (proceed toplevel) is a vaild option. (proceed out) {SCALARTENSOR[int32] :named ChainTMP28326 :vec-state [computed] 11 :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} proceed is a differentiable operation which instantly compiles and executes all the previous node of toplevel . In addition, there's another way to accept nodes: (build out) or (define-composite-function) , but they're a little complicated, so explained in the other sections. The moment compiling function is called, cl-waffe2 prunes all unused copying, computes all View Offsets , schedules memory allocation and (Currently it's not working though) multi-threading. AbstractTensor - One operation, Multiple implementations. Background (Operations in cl-waffe2) [AbstractNode] | |--------------------|------------------------| [CPU Implementation1] [CPU Implementation2] [CUDA Implementation1] ... Julia has introduced AbstractArray in their libraries, separating the common (generic) parts of the array from each backend implementation. Since AbstractTensor increased portability between devices on which they run (even on CPU!), cl-waffe2 wholly introduced this feature. In cl-waffe2, The generic definition of operations, AbstractNode is a class declared via the defnode macro, and depending on the devices we're working on, the define-impl macro defines an implementation. Conveniently, there can be more than one implementation for a single device. (e.g.: it is possible to have a normal implementation and an approximate implementation for the exp function on single CPU). One of the policy is to minimise code re-writing by defining abstract nodes and switching the backends that executes them depending on the device they run on and the speed required. Example: AddNode Here's an example of how I've implemented the operation !add . AddNode-Revisit is AbstractNode of finding the sum of two given matrices A and B and storing the result in A. Here's the segment from the source code. ;; Reimplementation of AddNode (defnode (AddNode-Revisit (myself dtype) :where (A[~] B[~] -> A[~]) :documentation \"A <- A + B\" :backward ((self dout dx dy) (declare (ignore dx dy)) (values dout dout)))) AbstractNode is a CLOS class with the following operation. shape changes before and after the operation, and which pointer to use? is described in :where . Before -> clause refers to the arguments, after -> clause refers to the shape of matrix after the operation. It says: Takes A and B as arguments and returns a matrix of pointers of A All matrices have the same shape before and after the operation. Also, :backward defines the operation of backward. This declaration can be made either in defnode or in define-impl , whichever you declare. The declared node can be initialized using the function (AddNode-Revisit dtype) , but seems returning errors. (AddNode-Revisit :float) ;; -> Couldn't find any implementation of AddNode for (CPUTENSOR LISPTENSOR). This is because there is not yet a single implementation for AddNode-Revisit . One operation can be defined for a backend that can be declared by extending the cl-waffe2/vm.generic-tensor:AbstractTensor class. Here's LispTensor , and CPUTensor , and of course, if necessary, you can create a new backend MyTensor by just copying them: ;; 1. Creating from AbstratTensor. (defclass MyTensor (AbstractTensor) nil) ;; Initializer/Allocator (defmethod initialize-instance :before ((tensor MyTensor) &rest initargs &key &allow-other-keys) ;; if projected-p -> alloc new vec (let* ((shape (getf initargs :shape)) (dtype (dtype->lisp-type (getf initargs :dtype))) (vec (getf initargs :vec)) (facet (getf initargs :facet)) (initial-element (coerce (or (getf initargs :initial-element) 0) dtype))) (when (eql facet :exist) (if vec (setf (tensor-vec tensor) vec) (setf (tensor-vec tensor) (make-array (apply #'* shape) :element-type dtype :initial-element initial-element)))))) ;; If data storage is differ from CL Array, override vref and (setf vref) method. ;; 2. Using an existing backend. ;; If you want to use a backend that is already implemented, the following line of code is sufficient. (defclass MyTensor (CPUTensor) nil) ;; Adding a new backend is all done in this code! (See also: tensor.lisp ) The devices to use can be switched with-devices macro. (with-devices (MyTensor LispTensor) ;; The further to the left, the higher the priority. (make-tensor `(10 10))) ;; -> MyTensor is created {MYTENSOR[float] :shape (10 10) ((0.0 0.0 0.0 ~ 0.0 0.0 0.0) (0.0 0.0 0.0 ~ 0.0 0.0 0.0) ... (0.0 0.0 0.0 ~ 0.0 0.0 0.0) (0.0 0.0 0.0 ~ 0.0 0.0 0.0)) :facet :exist :requires-grad NIL :backward NIL} MyTensor has no implementation of any operations, but the code below is working. (with-devices (MyTensor LispTensor) (proceed (!add (randn `(3 3)) (randn `(3 3))))) {MYTENSOR[float] :shape (3 3) :named ChainTMP28398 :vec-state [computed] ((-1.4494231 1.0320233 -1.8852448) (1.0886636 -0.37185743 0.99227524) (2.2778857 -0.82929707 2.3525782)) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} This is because MyTensor and LispTensor are pointer compatible, and AddNode for LispTensor is used instead of undefined implementation, AddNode for MyTensor . Therefore, after defining a new backend, it is NOT necessary to give a re-implementation for all standard implementations in cl-waffe2. Select the appropriate backends in order of array compatibility. The macro define-impl adds a new implementation of device . ;; The code below is NOT working on REPL, but working in :cl-waffe2/backends.lisp package (define-impl (AddNode-Revisit :device MyTensor) :forward ((self x y) (let ((adder (matrix-add (dtype x)))) `(,@(call-with-view #'(lambda (x-view y-view) `(funcall ,adder (tensor-vec ,x) (tensor-vec ,y) ,(offset-of x-view 0) ,(offset-of y-view 0) ,(size-of x-view 0) ,(stride-of x-view 0) ,(stride-of y-view 0))) `(,x ,y)) ,x)))) In :forward , write the expansion expression for the operation in the same way as when defining a macro with defmacro . The call-with-view function is a general-purpose function to iterate the given tensor with computing offsets. (P.S.: I believe that ideas on this macro needed to be given more thoughts, indeed, this is ugly... but I guess composite-function can be install without writing macros, not tested.) The forward definition of node can be called with (forward node &rest inputs) function. (forward (AddNode :float) (randn `(10 10)) (randn `(10 10))) {CPUTENSOR[float] :shape (10 10) :named ChainTMP28407 :vec-state [maybe-not-computed] ((-0.93102205 -0.25396287 0.45237574 ~ 0.54063225 0.56266963 -0.77444124) (-0.55870235 -0.9794068 -0.21233901 ~ 1.1901267 -0.83241004 -0.69876736) ... (-0.5366255 -0.9118863 1.274197 ~ 0.19851275 0.21501832 1.064277) (-0.65124494 0.15393624 -0.6625119 ~ -1.1875637 -2.007647 0.5431197)) :facet :input :requires-grad NIL :backward <Node: ADDNODE-CPUTENSOR (A[~] B[~] -> A[~])>} Closely Looking at :vec-state, it says the operation isn't done yet. The embodied elements are displayed but this is because AddNode is defined as in-place operation, returning the first argument. To accept this state instantly, we can use proceed . (proceed (forward (AddNode :float) (randn `(10 10)) (randn `(10 10))) :measure-time t) Proceed-Time: First Trying Evaluation took: 0.000 seconds of real time 0.000028 seconds of total run time (0.000019 user, 0.000009 system) 100.00% CPU 26,990 processor cycles 0 bytes consed Proceed-Time: Second Trying Evaluation took: 0.000 seconds of real time 0.000003 seconds of total run time (0.000003 user, 0.000000 system) 100.00% CPU 6,300 processor cycles 0 bytes consed {CPUTENSOR[float] :shape (10 10) :named ChainTMP28477 :vec-state [computed] ((2.843876 2.3477855 3.3252454 ~ -1.0901415 -1.211004 -2.268893) (-2.7236757 -0.60536575 -0.61465085 ~ 2.383132 -0.22351071 -0.6449351) ... (-0.7634125 0.7340392 2.7052975 ~ 1.1768849 3.609434 -1.3465445) (4.1204114 3.696868 -2.1895533 ~ -1.5550013 2.6361299 0.31319892)) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} Compiling, and In-place optimizing Compiled Model Compiling Common Lisp Code at runtime is certainly fast, but isn't enough. In order to re-use compiled nodes, there is Compiled-Composite class to manage the state. Compiled-Composite can be obtained by calling (build toplevel) (let* ((out (!sum (!add (randn `(10 10)) (randn `(10 10))))) (compiled-model (build out))) compiled-model) <Compiled-Composite forward: #<FUNCTION (LAMBDA ()) {53D7ED1B}> backward: #<FUNCTION (LAMBDA ()) {53D4D78B}> += [Tensors in the computation node] =======+ Subscripts: Variables: NAMES | SIZE | - The number of tmp variables : 15 - The number of parameters : 0 +========================================+ > (forward compiled-composite) , (backward compiled-composite) calls forward/backward functions respectively. (let* ((out (!sum (!add (randn `(10 10)) (randn `(10 10))))) (compiled-model (build out))) (print (forward compiled-model)) (print (backward compiled-model))) {CPUTENSOR[float] :shape (1 1) -> :view (<0> <0>) -> :visible-shape (1 1) :named ChainTMP29625 ((-24.876368)) :facet :input :requires-grad NIL :backward NIL} T But what if one wants to change the value of first argument? Replace (make-tensor) to be replaced later with a (make-input) function. (make-input shape input-name) shape can include symbols, to be determined later. (let* ((out (!sum (!add (make-input `(a b) :InputA) (randn `(10 10))))) (compiled-model (build out))) compiled-model) <Compiled-Composite forward: #<FUNCTION (LAMBDA ()) {53D9AF7B}> backward: #<FUNCTION (LAMBDA ()) {53D9D53B}> += [Tensors in the computation node] =======+ Subscripts: [A -> ?, max=?] [B -> ?, max=?] Variables: NAMES | SIZE | \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013 INPUTA | (A B) | - The number of tmp variables : 15 - The number of parameters : 0 +========================================+ > The function (make-input) itself, doesn't have a vector storage. (as long as (tensor-vec tensor) function isn't called). Accordingly, someone has to embody the storage of InputTensor with ExistTensor . (set-input compiled-composite input-name actual-tensor) embodies given InputTensor in the computation node with actual-tensor. (let* ((out (!sum (!add (make-input `(a b) :InputA) (randn `(10 10))))) (compiled-model (build out))) (set-input compiled-model :InputA (randn `(10 10))) (print (forward compiled-model)) (print (backward compiled-model)) (set-input compiled-model :InputA (randn `(10 10))) ;; ... working on another input ) {CPUTENSOR[float] :shape (1 1) -> :view (<0> <0>) -> :visible-shape (1 1) :named ChainTMP29804 ((17.631124)) :facet :input :requires-grad NIL :backward NIL} T In-place optimizing This is a usual function in cl-waffe2, which finds the sum of A and B. (!add a b) However, this is how !add is defined internally. This makes a copy twice times not to make side effects. ;; In source/base-impl/arithmetic.lisp (forward (AddNode dtype) (!copy a) (!copy b)) Without copying, the content of a is overwritten: (let ((a (make-tensor `(3 3) :initial-element 1.0))) (print a) ;; {CPUTENSOR[float] :shape (3 3) ;; ((1.0 1.0 1.0) ;; (1.0 1.0 1.0) ;; (1.0 1.0 1.0)) ;; :facet :exist ;; :requires-grad NIL ;; :backward NIL} ;; (eval A <- A + B) (proceed (forward (AddNode :float) a (randn `(3 3)))) (print a) ;; {CPUTENSOR[float] :shape (3 3) ;; ((2.0100088 0.2906983 1.5334041) ;; (-0.50357413 2.389317 0.7051847) ;; (1.3005692 1.5925546 0.95498145)) ;; :facet :exist ;; :requires-grad NIL ;; :backward NIL} ) To put it bluntly, it is natural to think this copy is just a waste of memory. However, In this case, disabling !copy is a rational way to optimize the performance of the program. (i.e.: replace with in-place operation). Owing to lazy evaluation of cl-waffe2, unnecessary (!copy) operation can be deleted automatically by checking the number of tensor references in a node. Let f(x) be a operation defined as: f ( x ) = s i n ( M a y b e C o p y ( x ) ) f(x) = sin(MaybeCopy(x)) f ( x ) = s in ( M a y b e C o p y ( x )) Let the computation node be below: o u t = f ( I n p u t ) + f ( f ( T e n s o r ) ) out = f(Input) + f(f(Tensor)) o u t = f ( I n p u t ) + f ( f ( T e n sor )) Formulating the same network in cl-waffe2: ;; (Let me define the utilities to be used in defnode in advance) ;; Tips: ;; Obtain function of :lazy-evaluation -> immediate execution. (defmodel (SinModel (self) :where (X[~] -> [~]) :on-call-> ((self x) (declare (ignore self)) (!sin x)))) (define-composite-function (SinModel) !sin-static :dtype :float) ;; (!sin-static (randn `(10 10))) is instantly executed. not lazy-evaluated. ;; Basic Units in the network: ;; General Definition of f(x) (defnode (F-Node (self) :documentation \"f(x) = sin(x)\" :where (A[~] -> A[~]))) ;; Implementation of f(x) ;; Setting :device = t, -> the impl is working on all devices. (define-impl (F-Node :device t) :forward ((self x) `(!sin-static ,x)) :backward ((self dout dx) (values (!mul dx (!cos dout))))) ;; The caller of f(x) (defun !f (x) (forward (F-Node) (!copy x))) Through :cl-waffe2/viz package, we can visualize how the operation is performed. ;; (make-input ... nil): creates a caching tensor, being the elements of it isn't guaranteed to be 0.0. (let ((k (!add (make-input `(3 3) nil) (!f (!f (randn `(3 3) :requires-grad t)))))) (cl-waffe2/viz:viz-computation-node k \"assets/bad_node.dot\") (build k) ;; optimized (cl-waffe2/viz:viz-computation-node k \"assets/opt_node.dot\")) The result is written in dot language . $ dot -Tpng ./assets/bad_node.dot > ./assets/bad_node.png $ dot -Tpng ./assets/opt_node.dot > ./assets/opt_node.png Before Optimized Vs After Optimized. ExistTensor (created by make-tensor , or tensors whose requires-grad=t) is never overwritten. Network Units: Node and Composite In this section, we learn the two key units, Node and Composite , to construct neural networks in cl-waffe2. Node and Composite Node(AbstractNode) is the smallest unit of operation with forward and backward propagation. Its abstract definition is defined by a defnode macro, and It is implemented by a (define-impl) macro. The defined node is invoked by (forward node &rest inputs) function, at the same time, computation nodes are constructed. (defnode (SinNode-Revisit (self) :where (X[~] -> X[~]) :save-for-backward (t) :backward ((self dout x) (values (!mul dout (!cos x)))))) (define-impl (SinNode-Revisit :device t) :forward ((self x) `(!sin-static ,x))) (forward (SinNode-Revisit) (randn `(10 10))) {CPUTENSOR[float] :shape (10 10) :named ChainTMP32968 :vec-state [maybe-not-computed] <<Not-Embodied (10 10) Tensor>> :facet :input :requires-grad NIL :backward <Node: SINNODE-REVISIT-T (X[~] -> X[~])>} (proceed *) {CPUTENSOR[float] :shape (10 10) :named ChainTMP32955 :vec-state [computed] ((0.43090457 -0.24942507 -0.99978673 ~ 0.97256666 -0.9993819 0.37133723) (0.050297778 -0.048203766 0.11011651 ~ -0.28100008 -0.89788723 0.12841338) ... (0.32419643 0.15791988 -0.95573443 ~ 0.079026684 -0.4924342 0.99993217) (-0.04615228 -0.2262427 -0.6637178 ~ 0.8855889 -0.72787035 -0.65471023)) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} On the other hand, Composite is a unit made up of several Nodes , defined by a defmodel macro. (call model &rest inputs) method invokes the on-call-> form lazily, being compiled in the same way as nodes. Moreover, the defined Composite also can define a function for immeditate function by using the macro, define-composite-function . The behaviour is similar to TorchScript , cl-waffe2 traces the computation node, calling (build toplevel) and defines a Composite-function . (defmodel (Softmax-Model (self) :where (X[~] -> [~]) ;; :where for Composite is optional! :on-call-> ((self x) (declare (ignore self)) (let* ((x1 (!sub x (!mean x :axis 1 :keepdims t))) (z (!sum (!exp x1) :axis 1 :keepdims t))) (!div (!exp x1) z))))) ;; won't be evaluated until proceed/build is called. (call (Softmax-Model) (randn `(10 10)) (proceed *) {CPUTENSOR[float] :shape (10 10) :named ChainTMP6184 :vec-state [computed] ((0.29810402 0.11953584 0.16032213 ~ 0.033787794 0.01729085 0.03808046) (0.032921903 0.085420445 0.10371924 ~ 0.06863596 0.10435363 0.07114864) ... (0.23044951 0.14320189 0.16871664 ~ 0.019123536 0.03614414 0.10644407) (0.0377036 0.034945846 0.28327137 ~ 0.07359542 0.40399343 0.020138593)) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} ;; Works at toplevel (define-composite-function (Softmax-Model) !softmax-static) ;; No overheads of compiling, but there's a little overhead to dispatch the method. (time (!softmax-static (randn `(10 10)))) Evaluation took: 0.000 seconds of real time 0.000301 seconds of total run time (0.000253 user, 0.000048 system) 100.00% CPU 691,808 processor cycles 32,496 bytes consed {CPUTENSOR[float] :shape (10 10) :named ChainTMP6195 ((0.042827643 0.13156936 0.06729175 ~ 0.059296332 0.17645036 0.04613843) (0.32095885 0.030778391 0.091331415 ~ 0.09311637 0.28322798 0.040707175) ... (0.045369238 0.045168925 0.12002338 ~ 0.2656273 0.01337298 0.41475114) (0.020064427 0.01839381 0.013036524 ~ 0.20158055 0.3377756 0.061546378)) :facet :input :requires-grad NIL :backward NIL} Proceed vs Composite-function Compared to Proceed , Composite-function and codes which consisted of it have a small overhead in calling a function, but it becomes negligible as the matrix size increases. ;; Proceed ;; 1 * 1 Matrix (let ((a (ax+b `(1 1) 0 1))) (proceed-time (!sin a))) Proceed-Time: First Trying Evaluation took: 0.000 seconds of real time 0.000077 seconds of total run time (0.000054 user, 0.000023 system) 100.00% CPU 141,818 processor cycles 0 bytes consed Proceed-Time: Second Trying Evaluation took: 0.000 seconds of real time 0.000007 seconds of total run time (0.000006 user, 0.000001 system) 100.00% CPU 11,790 processor cycles 0 bytes consed ;; Proceed ;; 1000 * 1000 Matrix (let ((a (ax+b `(1000 1000) 0 1))) (proceed-time (!sin a))) Proceed-Time: First Trying Evaluation took: 0.019 seconds of real time 0.019118 seconds of total run time (0.017191 user, 0.001927 system) 100.00% CPU 44,118,196 processor cycles 8,000,032 bytes consed Proceed-Time: Second Trying Evaluation took: 0.015 seconds of real time 0.015628 seconds of total run time (0.015613 user, 0.000015 system) 106.67% CPU 36,025,586 processor cycles 0 bytes consed ;; Composite-Function ;; 1 * 1 Matrix (let ((a (ax+b `(1 1) 0 1))) (time (!sin-inline a))) Evaluation took: 0.000 seconds of real time 0.000103 seconds of total run time (0.000098 user, 0.000005 system) 100.00% CPU 231,840 processor cycles 0 bytes consed ;; Composite-Function ;; 1000 * 1000 Matrix (let ((a (ax+b `(1000 1000) 0 1))) (time (!sin-inline a))) Evaluation took: 0.015 seconds of real time 0.015862 seconds of total run time (0.015813 user, 0.000049 system) 106.67% CPU 36,632,326 processor cycles 0 bytes consed (Tips: the call method is designed to invoke Composite , but it is also applicatable into AbstractNode , that is, call is a general-purpose method to invoke nodes.) Sequence Model Since the shape of matrices is declared everywhere operation, cl-waffe2 can trace the structure of neural networks lazily, and being checked before the execution. In the code below, defsequence is a macro to define Composite sequentially, (asnode function) is a macro which coerce function into Composite . (defsequence MLP-Sequence (in-features hidden-dim out-features &key (activation #'!tanh)) \"3 Layers MLP\" (LinearLayer in-features hidden-dim) (asnode activation) (LinearLayer hidden-dim hidden-dim) (asnode activation) (LinearLayer hidden-dim out-features) (asnode #'!softmax)) All composites/nodes that used to define MLP-Sequence has also a definition of shape. (MLP-Sequence 784 512 256) <Composite: MLP-SEQUENCE{W23852}( <<6 Layers Sequence>> [1/6] \u2193 <Composite: LINEARLAYER{W23682}( <Input : ((~ BATCH-SIZE 784)) -> Output: ((~ BATCH-SIZE 512))> WEIGHTS -> (512 784) BIAS -> (512) )> [2/6] \u2193 <Composite: ENCAPSULATED-NODE{W23680}( #<FUNCTION !TANH> )> [3/6] \u2193 <Composite: LINEARLAYER{W23510}( <Input : ((~ BATCH-SIZE 512)) -> Output: ((~ BATCH-SIZE 512))> WEIGHTS -> (512 512) BIAS -> (512) )> [4/6] \u2193 <Composite: ENCAPSULATED-NODE{W23508}( #<FUNCTION !TANH> )> [5/6] \u2193 <Composite: LINEARLAYER{W23338}( <Input : ((~ BATCH-SIZE 512)) -> Output: ((~ BATCH-SIZE 256))> WEIGHTS -> (256 512) BIAS -> (256) )> [6/6] \u2193 <Composite: ENCAPSULATED-NODE{W23336}( #<FUNCTION CL-WAFFE2/NN:!SOFTMAX> )>)> Not an operation is performed, nor a matrix is allocated at the moment MLP-Sequence is initialized, but done when compiling/invoking the computation node. Shaping API with DSL See also: Introducing Subscript DSL View APIs (TODO) Optional Broadcasting In cl-waffe2, operations with several arguments must be called with the same shape of tensors as :where says. In the code below, since !add is declared as A[~] B[~] -> A[~] , the first and second argument, must have the same shape, same ranks. However, opeartions isn't always performed within the same ranks. In practice, !add isn't always used as just an element-wise operation because the total elements of tensor can be found via !add , adding biases to the given tensor is also realised by using !add . Indeed, broadcasting is a convenient operation when expressing matrix iterations without using (loop for ...) . (!add (randn `(3 3)) (randn `(3))) ; Evaluation aborted on #<CL-WAFFE2/VM.GENERIC-TENSOR:SHAPING-ERROR {100376BD13}>. The same code would being broadcasted well and works on libraries which supports Numpy Semantics Broadcasting , but not working on cl-waffe2 as you can see. This is because cl-waffe2 do not support automatically broadcasting but support manually broadcasting . That is, each place broadcast is needed, you also have to declare the tensor is broadcasted, since the condition of broadcastable is less restrictive which sometimes produce an unintended behaviour with no any errors even though broadcasting is only used in a limited situation. Numpy Semantic Broadcasting has two rules: Rank up: If matrices with different number of axes are called in a one operation, one is added to the tensor with smallest ranks to straighten up the number of dimensions. Repeating 1: If the dimension at corresponding position do not match, and either one is 1 . 1 is repeated with the other. There are two corresponding operations in cl-waffe2: (Two main parts of broadcasting) (<1 x N> 1 1) \u2191 \u2191 !flexible !view (!flexible (randn `(1 1))) {CPUTENSOR[float] :shape (<1 x N> 1 1) :named ChainTMP2614 :vec-state [maybe-not-computed] ((0.6495824)) :facet :input :requires-grad NIL :backward <Node: FLEXIBLE-RANK-NODE-T (A[~] -> A[~])>} (!view (randn `(1)) `(:broadcast 100)) {CPUTENSOR[float] :shape (1) -> :view (<(BROADCAST 100)>) -> :visible-shape (100) :named ChainTMP4406 :vec-state [maybe-not-computed] (-0.5008113 -0.5008113 -0.5008113 ~ -0.5008113 -0.5008113 -0.5008113) :facet :input :requires-grad NIL :backward <Node: VIEWTENSORNODE-T (A[RESULT] B[BEFORE] -> A[RESULT])>} (0) The function (!flexible) adds the broadcastable dimensions of the given tensor. In <1 x N> parts, 1 is repeated, 1 is added if any. In 1 parts, never broadcasted. (!view a `(:broadcast 10)) Mem: If both of given tensors is broadcasted, we may need to make a copy to store the result since there's no array of broadcasted size. This explicts: in which tensor, is broadcasting applied?, that is, there's more likely to useless copy is also removed. in-place broadcasting. Case1 - To higher, Batched Operation (!matmul (!flexible ...) (!flexible ...)) Case2 - To lower, add biases to columns (!add a (!view x ...)) TODO: (with-broadcasting (a1 b1 (a b)) ...) macro. Multidimensional Offsets. (TODO) Optimizing Model Parameter (TODO) defoptimizer deftrainer parameter Tutorials Over! (TO ADD: ./Examples, training MNIST, Image processing, NLP etc...) I'll keep my finger crossed.","title":"Tutorials"},{"location":"overview/#a-road-to-cl-waffe2","text":"","title":"A road to cl-waffe2"},{"location":"overview/#project-structure","text":"Thank you for having interest in my project. Before we start the tutorial, let me explain the structure of cl-waffe2 package. Mainly, cl-waffe2 consists of the following packages.","title":"Project Structure"},{"location":"overview/#fundamental-system","text":"These two package form the basis of cl-waffe2: :cl-waffe2/vm.nodes :cl-waffe2/vm.generic-tensor The package :cl-waffe2/vm.nodes provides a system for constructing neural networks, including AbstractNode , Composite , Shaping API etc... On the other hand, :cl-waffe2/vm.generic-tensor provides features on AbstractTensor , including JIT Compiler , NodeTensor , Memory-Pool etc...","title":"Fundamental System"},{"location":"overview/#standard-apis","text":"(Figure: Dependencies of cl-waffe2) [CPUBackend ] [base-impl] --- [LispBackend] | [CUDABackend] ... | [vm.generic-tensor] [vm.nodes] :cl-waffe2/base-impl Using the basic system of cl-waffe2, :cl-waffe2/vm.generic-tensor and :cl-waffe2/vm.nodes , the package :cl-waffe2/base-impl provides a standard implementation of matrix (sometimes scalar) tensor operations. The operation we say is including: defun parts, and abstract definition of operation. Before we go any futher: cl-waffe2 is working on AbstractTensor (inspired in Julia's great idea, AbstractArray ), which separates implementation of the operation from the definition. In that respect, :cl-waffe2/base-impl provides the definition of operations, while the packages we about to mention provides implementation of operations.","title":"Standard APIs"},{"location":"overview/#standard-backendsimplemenetations","text":"As of this writing (2023/07/05), we provide two standard implementation of :cl-waffe2/base-impl , both of them are working on CPU. :cl-waffe2/backends.lisp :cl-waffe2/backends.cpu If only time and money would permit, I'm willing to implement CUDA/Metal Backends. :cl-waffe2/backends.lisp is work enough first, it is Portable (based on ANSI Common Lisp) and supports AVX2 but far from full speed . On the other hand :cl-waffe2/backends.cpu is accelerated by OpenBLAS (maybe MKL is ok) and other foreign backends, this is SBCL-Dependant and sometimes could be unsafe, but provides full speed . TODO: :cl-waffe2/backends.fastmath (NOT IMPLEMENTED YET!) (TO BE) Supporting vectorized mathematical functions, AVX512 instructions.","title":"Standard Backends/Implemenetations"},{"location":"overview/#neural-network","text":":cl-waffe2/nn ;; Provides Basic neural-network Implementations. :cl-waffe2/optimizers ;; Provides Basic Optimizers","title":"Neural Network"},{"location":"overview/#utils","text":":cl-waffe2 ;; Provides multi-threading APIs and config macros! :cl-waffe2/viz ;; Provides Vizualizing APIs of computation node etc...","title":"Utils"},{"location":"overview/#to-get-started","text":"If you're going to start with defining a new package, It is recommended to :use the package to be used. Read the description above and select and describe the packages you think you need. (or you can just copy and paste it.) The lisp code below demonstrates an example case of :your-project-name package. (in-package :cl-user) (defpackage :your-project-name (:use :cl :cl-waffe2 :cl-waffe2/vm.generic-tensor :cl-waffe2/vm.nodes :cl-waffe2/base-impl :cl-waffe2/distributions :cl-waffe2/backends.lisp :cl-waffe2/backends.cpu :cl-waffe2/nn :cl-waffe2/optimizers :cl-waffe2/viz)) (in-package :your-project-name) ;; Your code follows... If you're working with REPL (or new to Common Lisp?), you can try cl-waffe2 features like this: $ ros run > (load \"cl-waffe2.asd\") # cl-waffe2.asd should be placed where SBCL can read it. > (ql:quickload :cl-waffe2) > (in-package :cl-waffe2-repl) ;; this is a playground place, and all features are available The tutorials below should be also working on REPL, (indeed, cl-waffe2 is REPL-friendly!), you can learn how cl-waffe2 works by copying and pasting the example codes.","title":"To Get Started!"},{"location":"overview/#basic-building-computation-nodes-lazily","text":"Since Do not run until the node is optimized is a one of cl-waffe2 policy, all operations in cl-waffe2 is lazy evaluation unless defined by a special macro. Therefore, calling !add function which finds a sum of given arguments, the retuend tensor isn't still computed, only setting :vec-state = [maybe-not-computed] . (!add 3.0 2.0) {SCALARTENSOR[float] :named ChainTMP23305 :vec-state [maybe-not-computed] <<Not-Embodied (1) Tensor>> :facet :input :requires-grad NIL :backward <Node: SCALARANDSCALARADD-SCALARTENSOR (A[SCAL] B[SCAL] -> A[SCAL] WHERE SCAL = 1)>} :vec-state indicates the computation state of tensor, and it says exactly what it says. You can continue the operation by connecting the returned tensor and next operation. For example, the figure below in cl-waffe is represented as: o u t = 3 + 2 \u2217 4 out = 3 + 2 * 4 o u t = 3 + 2 \u2217 4 (defparameter out (!add 3 (!mul 2 4))) ;; out <- 3 + 2 * 4 To obtain the state in which the operation is performed, calling the function (proceed toplevel) is a vaild option. (proceed out) {SCALARTENSOR[int32] :named ChainTMP28326 :vec-state [computed] 11 :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} proceed is a differentiable operation which instantly compiles and executes all the previous node of toplevel . In addition, there's another way to accept nodes: (build out) or (define-composite-function) , but they're a little complicated, so explained in the other sections. The moment compiling function is called, cl-waffe2 prunes all unused copying, computes all View Offsets , schedules memory allocation and (Currently it's not working though) multi-threading.","title":"Basic: Building Computation Nodes Lazily"},{"location":"overview/#abstracttensor-one-operation-multiple-implementations","text":"","title":"AbstractTensor - One operation, Multiple implementations."},{"location":"overview/#background","text":"(Operations in cl-waffe2) [AbstractNode] | |--------------------|------------------------| [CPU Implementation1] [CPU Implementation2] [CUDA Implementation1] ... Julia has introduced AbstractArray in their libraries, separating the common (generic) parts of the array from each backend implementation. Since AbstractTensor increased portability between devices on which they run (even on CPU!), cl-waffe2 wholly introduced this feature. In cl-waffe2, The generic definition of operations, AbstractNode is a class declared via the defnode macro, and depending on the devices we're working on, the define-impl macro defines an implementation. Conveniently, there can be more than one implementation for a single device. (e.g.: it is possible to have a normal implementation and an approximate implementation for the exp function on single CPU). One of the policy is to minimise code re-writing by defining abstract nodes and switching the backends that executes them depending on the device they run on and the speed required.","title":"Background"},{"location":"overview/#example-addnode","text":"Here's an example of how I've implemented the operation !add . AddNode-Revisit is AbstractNode of finding the sum of two given matrices A and B and storing the result in A. Here's the segment from the source code. ;; Reimplementation of AddNode (defnode (AddNode-Revisit (myself dtype) :where (A[~] B[~] -> A[~]) :documentation \"A <- A + B\" :backward ((self dout dx dy) (declare (ignore dx dy)) (values dout dout)))) AbstractNode is a CLOS class with the following operation. shape changes before and after the operation, and which pointer to use? is described in :where . Before -> clause refers to the arguments, after -> clause refers to the shape of matrix after the operation. It says: Takes A and B as arguments and returns a matrix of pointers of A All matrices have the same shape before and after the operation. Also, :backward defines the operation of backward. This declaration can be made either in defnode or in define-impl , whichever you declare. The declared node can be initialized using the function (AddNode-Revisit dtype) , but seems returning errors. (AddNode-Revisit :float) ;; -> Couldn't find any implementation of AddNode for (CPUTENSOR LISPTENSOR). This is because there is not yet a single implementation for AddNode-Revisit . One operation can be defined for a backend that can be declared by extending the cl-waffe2/vm.generic-tensor:AbstractTensor class. Here's LispTensor , and CPUTensor , and of course, if necessary, you can create a new backend MyTensor by just copying them: ;; 1. Creating from AbstratTensor. (defclass MyTensor (AbstractTensor) nil) ;; Initializer/Allocator (defmethod initialize-instance :before ((tensor MyTensor) &rest initargs &key &allow-other-keys) ;; if projected-p -> alloc new vec (let* ((shape (getf initargs :shape)) (dtype (dtype->lisp-type (getf initargs :dtype))) (vec (getf initargs :vec)) (facet (getf initargs :facet)) (initial-element (coerce (or (getf initargs :initial-element) 0) dtype))) (when (eql facet :exist) (if vec (setf (tensor-vec tensor) vec) (setf (tensor-vec tensor) (make-array (apply #'* shape) :element-type dtype :initial-element initial-element)))))) ;; If data storage is differ from CL Array, override vref and (setf vref) method. ;; 2. Using an existing backend. ;; If you want to use a backend that is already implemented, the following line of code is sufficient. (defclass MyTensor (CPUTensor) nil) ;; Adding a new backend is all done in this code! (See also: tensor.lisp ) The devices to use can be switched with-devices macro. (with-devices (MyTensor LispTensor) ;; The further to the left, the higher the priority. (make-tensor `(10 10))) ;; -> MyTensor is created {MYTENSOR[float] :shape (10 10) ((0.0 0.0 0.0 ~ 0.0 0.0 0.0) (0.0 0.0 0.0 ~ 0.0 0.0 0.0) ... (0.0 0.0 0.0 ~ 0.0 0.0 0.0) (0.0 0.0 0.0 ~ 0.0 0.0 0.0)) :facet :exist :requires-grad NIL :backward NIL} MyTensor has no implementation of any operations, but the code below is working. (with-devices (MyTensor LispTensor) (proceed (!add (randn `(3 3)) (randn `(3 3))))) {MYTENSOR[float] :shape (3 3) :named ChainTMP28398 :vec-state [computed] ((-1.4494231 1.0320233 -1.8852448) (1.0886636 -0.37185743 0.99227524) (2.2778857 -0.82929707 2.3525782)) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} This is because MyTensor and LispTensor are pointer compatible, and AddNode for LispTensor is used instead of undefined implementation, AddNode for MyTensor . Therefore, after defining a new backend, it is NOT necessary to give a re-implementation for all standard implementations in cl-waffe2. Select the appropriate backends in order of array compatibility. The macro define-impl adds a new implementation of device . ;; The code below is NOT working on REPL, but working in :cl-waffe2/backends.lisp package (define-impl (AddNode-Revisit :device MyTensor) :forward ((self x y) (let ((adder (matrix-add (dtype x)))) `(,@(call-with-view #'(lambda (x-view y-view) `(funcall ,adder (tensor-vec ,x) (tensor-vec ,y) ,(offset-of x-view 0) ,(offset-of y-view 0) ,(size-of x-view 0) ,(stride-of x-view 0) ,(stride-of y-view 0))) `(,x ,y)) ,x)))) In :forward , write the expansion expression for the operation in the same way as when defining a macro with defmacro . The call-with-view function is a general-purpose function to iterate the given tensor with computing offsets. (P.S.: I believe that ideas on this macro needed to be given more thoughts, indeed, this is ugly... but I guess composite-function can be install without writing macros, not tested.) The forward definition of node can be called with (forward node &rest inputs) function. (forward (AddNode :float) (randn `(10 10)) (randn `(10 10))) {CPUTENSOR[float] :shape (10 10) :named ChainTMP28407 :vec-state [maybe-not-computed] ((-0.93102205 -0.25396287 0.45237574 ~ 0.54063225 0.56266963 -0.77444124) (-0.55870235 -0.9794068 -0.21233901 ~ 1.1901267 -0.83241004 -0.69876736) ... (-0.5366255 -0.9118863 1.274197 ~ 0.19851275 0.21501832 1.064277) (-0.65124494 0.15393624 -0.6625119 ~ -1.1875637 -2.007647 0.5431197)) :facet :input :requires-grad NIL :backward <Node: ADDNODE-CPUTENSOR (A[~] B[~] -> A[~])>} Closely Looking at :vec-state, it says the operation isn't done yet. The embodied elements are displayed but this is because AddNode is defined as in-place operation, returning the first argument. To accept this state instantly, we can use proceed . (proceed (forward (AddNode :float) (randn `(10 10)) (randn `(10 10))) :measure-time t) Proceed-Time: First Trying Evaluation took: 0.000 seconds of real time 0.000028 seconds of total run time (0.000019 user, 0.000009 system) 100.00% CPU 26,990 processor cycles 0 bytes consed Proceed-Time: Second Trying Evaluation took: 0.000 seconds of real time 0.000003 seconds of total run time (0.000003 user, 0.000000 system) 100.00% CPU 6,300 processor cycles 0 bytes consed {CPUTENSOR[float] :shape (10 10) :named ChainTMP28477 :vec-state [computed] ((2.843876 2.3477855 3.3252454 ~ -1.0901415 -1.211004 -2.268893) (-2.7236757 -0.60536575 -0.61465085 ~ 2.383132 -0.22351071 -0.6449351) ... (-0.7634125 0.7340392 2.7052975 ~ 1.1768849 3.609434 -1.3465445) (4.1204114 3.696868 -2.1895533 ~ -1.5550013 2.6361299 0.31319892)) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>}","title":"Example: AddNode"},{"location":"overview/#compiling-and-in-place-optimizing","text":"","title":"Compiling, and In-place optimizing"},{"location":"overview/#compiled-model","text":"Compiling Common Lisp Code at runtime is certainly fast, but isn't enough. In order to re-use compiled nodes, there is Compiled-Composite class to manage the state. Compiled-Composite can be obtained by calling (build toplevel) (let* ((out (!sum (!add (randn `(10 10)) (randn `(10 10))))) (compiled-model (build out))) compiled-model) <Compiled-Composite forward: #<FUNCTION (LAMBDA ()) {53D7ED1B}> backward: #<FUNCTION (LAMBDA ()) {53D4D78B}> += [Tensors in the computation node] =======+ Subscripts: Variables: NAMES | SIZE | - The number of tmp variables : 15 - The number of parameters : 0 +========================================+ > (forward compiled-composite) , (backward compiled-composite) calls forward/backward functions respectively. (let* ((out (!sum (!add (randn `(10 10)) (randn `(10 10))))) (compiled-model (build out))) (print (forward compiled-model)) (print (backward compiled-model))) {CPUTENSOR[float] :shape (1 1) -> :view (<0> <0>) -> :visible-shape (1 1) :named ChainTMP29625 ((-24.876368)) :facet :input :requires-grad NIL :backward NIL} T But what if one wants to change the value of first argument? Replace (make-tensor) to be replaced later with a (make-input) function. (make-input shape input-name) shape can include symbols, to be determined later. (let* ((out (!sum (!add (make-input `(a b) :InputA) (randn `(10 10))))) (compiled-model (build out))) compiled-model) <Compiled-Composite forward: #<FUNCTION (LAMBDA ()) {53D9AF7B}> backward: #<FUNCTION (LAMBDA ()) {53D9D53B}> += [Tensors in the computation node] =======+ Subscripts: [A -> ?, max=?] [B -> ?, max=?] Variables: NAMES | SIZE | \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013 INPUTA | (A B) | - The number of tmp variables : 15 - The number of parameters : 0 +========================================+ > The function (make-input) itself, doesn't have a vector storage. (as long as (tensor-vec tensor) function isn't called). Accordingly, someone has to embody the storage of InputTensor with ExistTensor . (set-input compiled-composite input-name actual-tensor) embodies given InputTensor in the computation node with actual-tensor. (let* ((out (!sum (!add (make-input `(a b) :InputA) (randn `(10 10))))) (compiled-model (build out))) (set-input compiled-model :InputA (randn `(10 10))) (print (forward compiled-model)) (print (backward compiled-model)) (set-input compiled-model :InputA (randn `(10 10))) ;; ... working on another input ) {CPUTENSOR[float] :shape (1 1) -> :view (<0> <0>) -> :visible-shape (1 1) :named ChainTMP29804 ((17.631124)) :facet :input :requires-grad NIL :backward NIL} T","title":"Compiled Model"},{"location":"overview/#in-place-optimizing","text":"This is a usual function in cl-waffe2, which finds the sum of A and B. (!add a b) However, this is how !add is defined internally. This makes a copy twice times not to make side effects. ;; In source/base-impl/arithmetic.lisp (forward (AddNode dtype) (!copy a) (!copy b)) Without copying, the content of a is overwritten: (let ((a (make-tensor `(3 3) :initial-element 1.0))) (print a) ;; {CPUTENSOR[float] :shape (3 3) ;; ((1.0 1.0 1.0) ;; (1.0 1.0 1.0) ;; (1.0 1.0 1.0)) ;; :facet :exist ;; :requires-grad NIL ;; :backward NIL} ;; (eval A <- A + B) (proceed (forward (AddNode :float) a (randn `(3 3)))) (print a) ;; {CPUTENSOR[float] :shape (3 3) ;; ((2.0100088 0.2906983 1.5334041) ;; (-0.50357413 2.389317 0.7051847) ;; (1.3005692 1.5925546 0.95498145)) ;; :facet :exist ;; :requires-grad NIL ;; :backward NIL} ) To put it bluntly, it is natural to think this copy is just a waste of memory. However, In this case, disabling !copy is a rational way to optimize the performance of the program. (i.e.: replace with in-place operation). Owing to lazy evaluation of cl-waffe2, unnecessary (!copy) operation can be deleted automatically by checking the number of tensor references in a node. Let f(x) be a operation defined as: f ( x ) = s i n ( M a y b e C o p y ( x ) ) f(x) = sin(MaybeCopy(x)) f ( x ) = s in ( M a y b e C o p y ( x )) Let the computation node be below: o u t = f ( I n p u t ) + f ( f ( T e n s o r ) ) out = f(Input) + f(f(Tensor)) o u t = f ( I n p u t ) + f ( f ( T e n sor )) Formulating the same network in cl-waffe2: ;; (Let me define the utilities to be used in defnode in advance) ;; Tips: ;; Obtain function of :lazy-evaluation -> immediate execution. (defmodel (SinModel (self) :where (X[~] -> [~]) :on-call-> ((self x) (declare (ignore self)) (!sin x)))) (define-composite-function (SinModel) !sin-static :dtype :float) ;; (!sin-static (randn `(10 10))) is instantly executed. not lazy-evaluated. ;; Basic Units in the network: ;; General Definition of f(x) (defnode (F-Node (self) :documentation \"f(x) = sin(x)\" :where (A[~] -> A[~]))) ;; Implementation of f(x) ;; Setting :device = t, -> the impl is working on all devices. (define-impl (F-Node :device t) :forward ((self x) `(!sin-static ,x)) :backward ((self dout dx) (values (!mul dx (!cos dout))))) ;; The caller of f(x) (defun !f (x) (forward (F-Node) (!copy x))) Through :cl-waffe2/viz package, we can visualize how the operation is performed. ;; (make-input ... nil): creates a caching tensor, being the elements of it isn't guaranteed to be 0.0. (let ((k (!add (make-input `(3 3) nil) (!f (!f (randn `(3 3) :requires-grad t)))))) (cl-waffe2/viz:viz-computation-node k \"assets/bad_node.dot\") (build k) ;; optimized (cl-waffe2/viz:viz-computation-node k \"assets/opt_node.dot\")) The result is written in dot language . $ dot -Tpng ./assets/bad_node.dot > ./assets/bad_node.png $ dot -Tpng ./assets/opt_node.dot > ./assets/opt_node.png","title":"In-place optimizing"},{"location":"overview/#before-optimized-vs-after-optimized","text":"ExistTensor (created by make-tensor , or tensors whose requires-grad=t) is never overwritten.","title":"Before Optimized Vs After Optimized."},{"location":"overview/#network-units-node-and-composite","text":"In this section, we learn the two key units, Node and Composite , to construct neural networks in cl-waffe2.","title":"Network Units: Node and Composite"},{"location":"overview/#node-and-composite","text":"Node(AbstractNode) is the smallest unit of operation with forward and backward propagation. Its abstract definition is defined by a defnode macro, and It is implemented by a (define-impl) macro. The defined node is invoked by (forward node &rest inputs) function, at the same time, computation nodes are constructed. (defnode (SinNode-Revisit (self) :where (X[~] -> X[~]) :save-for-backward (t) :backward ((self dout x) (values (!mul dout (!cos x)))))) (define-impl (SinNode-Revisit :device t) :forward ((self x) `(!sin-static ,x))) (forward (SinNode-Revisit) (randn `(10 10))) {CPUTENSOR[float] :shape (10 10) :named ChainTMP32968 :vec-state [maybe-not-computed] <<Not-Embodied (10 10) Tensor>> :facet :input :requires-grad NIL :backward <Node: SINNODE-REVISIT-T (X[~] -> X[~])>} (proceed *) {CPUTENSOR[float] :shape (10 10) :named ChainTMP32955 :vec-state [computed] ((0.43090457 -0.24942507 -0.99978673 ~ 0.97256666 -0.9993819 0.37133723) (0.050297778 -0.048203766 0.11011651 ~ -0.28100008 -0.89788723 0.12841338) ... (0.32419643 0.15791988 -0.95573443 ~ 0.079026684 -0.4924342 0.99993217) (-0.04615228 -0.2262427 -0.6637178 ~ 0.8855889 -0.72787035 -0.65471023)) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} On the other hand, Composite is a unit made up of several Nodes , defined by a defmodel macro. (call model &rest inputs) method invokes the on-call-> form lazily, being compiled in the same way as nodes. Moreover, the defined Composite also can define a function for immeditate function by using the macro, define-composite-function . The behaviour is similar to TorchScript , cl-waffe2 traces the computation node, calling (build toplevel) and defines a Composite-function . (defmodel (Softmax-Model (self) :where (X[~] -> [~]) ;; :where for Composite is optional! :on-call-> ((self x) (declare (ignore self)) (let* ((x1 (!sub x (!mean x :axis 1 :keepdims t))) (z (!sum (!exp x1) :axis 1 :keepdims t))) (!div (!exp x1) z))))) ;; won't be evaluated until proceed/build is called. (call (Softmax-Model) (randn `(10 10)) (proceed *) {CPUTENSOR[float] :shape (10 10) :named ChainTMP6184 :vec-state [computed] ((0.29810402 0.11953584 0.16032213 ~ 0.033787794 0.01729085 0.03808046) (0.032921903 0.085420445 0.10371924 ~ 0.06863596 0.10435363 0.07114864) ... (0.23044951 0.14320189 0.16871664 ~ 0.019123536 0.03614414 0.10644407) (0.0377036 0.034945846 0.28327137 ~ 0.07359542 0.40399343 0.020138593)) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} ;; Works at toplevel (define-composite-function (Softmax-Model) !softmax-static) ;; No overheads of compiling, but there's a little overhead to dispatch the method. (time (!softmax-static (randn `(10 10)))) Evaluation took: 0.000 seconds of real time 0.000301 seconds of total run time (0.000253 user, 0.000048 system) 100.00% CPU 691,808 processor cycles 32,496 bytes consed {CPUTENSOR[float] :shape (10 10) :named ChainTMP6195 ((0.042827643 0.13156936 0.06729175 ~ 0.059296332 0.17645036 0.04613843) (0.32095885 0.030778391 0.091331415 ~ 0.09311637 0.28322798 0.040707175) ... (0.045369238 0.045168925 0.12002338 ~ 0.2656273 0.01337298 0.41475114) (0.020064427 0.01839381 0.013036524 ~ 0.20158055 0.3377756 0.061546378)) :facet :input :requires-grad NIL :backward NIL}","title":"Node and Composite"},{"location":"overview/#proceed-vs-composite-function","text":"Compared to Proceed , Composite-function and codes which consisted of it have a small overhead in calling a function, but it becomes negligible as the matrix size increases. ;; Proceed ;; 1 * 1 Matrix (let ((a (ax+b `(1 1) 0 1))) (proceed-time (!sin a))) Proceed-Time: First Trying Evaluation took: 0.000 seconds of real time 0.000077 seconds of total run time (0.000054 user, 0.000023 system) 100.00% CPU 141,818 processor cycles 0 bytes consed Proceed-Time: Second Trying Evaluation took: 0.000 seconds of real time 0.000007 seconds of total run time (0.000006 user, 0.000001 system) 100.00% CPU 11,790 processor cycles 0 bytes consed ;; Proceed ;; 1000 * 1000 Matrix (let ((a (ax+b `(1000 1000) 0 1))) (proceed-time (!sin a))) Proceed-Time: First Trying Evaluation took: 0.019 seconds of real time 0.019118 seconds of total run time (0.017191 user, 0.001927 system) 100.00% CPU 44,118,196 processor cycles 8,000,032 bytes consed Proceed-Time: Second Trying Evaluation took: 0.015 seconds of real time 0.015628 seconds of total run time (0.015613 user, 0.000015 system) 106.67% CPU 36,025,586 processor cycles 0 bytes consed ;; Composite-Function ;; 1 * 1 Matrix (let ((a (ax+b `(1 1) 0 1))) (time (!sin-inline a))) Evaluation took: 0.000 seconds of real time 0.000103 seconds of total run time (0.000098 user, 0.000005 system) 100.00% CPU 231,840 processor cycles 0 bytes consed ;; Composite-Function ;; 1000 * 1000 Matrix (let ((a (ax+b `(1000 1000) 0 1))) (time (!sin-inline a))) Evaluation took: 0.015 seconds of real time 0.015862 seconds of total run time (0.015813 user, 0.000049 system) 106.67% CPU 36,632,326 processor cycles 0 bytes consed (Tips: the call method is designed to invoke Composite , but it is also applicatable into AbstractNode , that is, call is a general-purpose method to invoke nodes.)","title":"Proceed vs Composite-function"},{"location":"overview/#sequence-model","text":"Since the shape of matrices is declared everywhere operation, cl-waffe2 can trace the structure of neural networks lazily, and being checked before the execution. In the code below, defsequence is a macro to define Composite sequentially, (asnode function) is a macro which coerce function into Composite . (defsequence MLP-Sequence (in-features hidden-dim out-features &key (activation #'!tanh)) \"3 Layers MLP\" (LinearLayer in-features hidden-dim) (asnode activation) (LinearLayer hidden-dim hidden-dim) (asnode activation) (LinearLayer hidden-dim out-features) (asnode #'!softmax)) All composites/nodes that used to define MLP-Sequence has also a definition of shape. (MLP-Sequence 784 512 256) <Composite: MLP-SEQUENCE{W23852}( <<6 Layers Sequence>> [1/6] \u2193 <Composite: LINEARLAYER{W23682}( <Input : ((~ BATCH-SIZE 784)) -> Output: ((~ BATCH-SIZE 512))> WEIGHTS -> (512 784) BIAS -> (512) )> [2/6] \u2193 <Composite: ENCAPSULATED-NODE{W23680}( #<FUNCTION !TANH> )> [3/6] \u2193 <Composite: LINEARLAYER{W23510}( <Input : ((~ BATCH-SIZE 512)) -> Output: ((~ BATCH-SIZE 512))> WEIGHTS -> (512 512) BIAS -> (512) )> [4/6] \u2193 <Composite: ENCAPSULATED-NODE{W23508}( #<FUNCTION !TANH> )> [5/6] \u2193 <Composite: LINEARLAYER{W23338}( <Input : ((~ BATCH-SIZE 512)) -> Output: ((~ BATCH-SIZE 256))> WEIGHTS -> (256 512) BIAS -> (256) )> [6/6] \u2193 <Composite: ENCAPSULATED-NODE{W23336}( #<FUNCTION CL-WAFFE2/NN:!SOFTMAX> )>)> Not an operation is performed, nor a matrix is allocated at the moment MLP-Sequence is initialized, but done when compiling/invoking the computation node.","title":"Sequence Model"},{"location":"overview/#shaping-api-with-dsl","text":"See also: Introducing Subscript DSL","title":"Shaping API with DSL"},{"location":"overview/#view-apis","text":"(TODO)","title":"View APIs"},{"location":"overview/#optional-broadcasting","text":"In cl-waffe2, operations with several arguments must be called with the same shape of tensors as :where says. In the code below, since !add is declared as A[~] B[~] -> A[~] , the first and second argument, must have the same shape, same ranks. However, opeartions isn't always performed within the same ranks. In practice, !add isn't always used as just an element-wise operation because the total elements of tensor can be found via !add , adding biases to the given tensor is also realised by using !add . Indeed, broadcasting is a convenient operation when expressing matrix iterations without using (loop for ...) . (!add (randn `(3 3)) (randn `(3))) ; Evaluation aborted on #<CL-WAFFE2/VM.GENERIC-TENSOR:SHAPING-ERROR {100376BD13}>. The same code would being broadcasted well and works on libraries which supports Numpy Semantics Broadcasting , but not working on cl-waffe2 as you can see. This is because cl-waffe2 do not support automatically broadcasting but support manually broadcasting . That is, each place broadcast is needed, you also have to declare the tensor is broadcasted, since the condition of broadcastable is less restrictive which sometimes produce an unintended behaviour with no any errors even though broadcasting is only used in a limited situation. Numpy Semantic Broadcasting has two rules: Rank up: If matrices with different number of axes are called in a one operation, one is added to the tensor with smallest ranks to straighten up the number of dimensions. Repeating 1: If the dimension at corresponding position do not match, and either one is 1 . 1 is repeated with the other. There are two corresponding operations in cl-waffe2: (Two main parts of broadcasting) (<1 x N> 1 1) \u2191 \u2191 !flexible !view (!flexible (randn `(1 1))) {CPUTENSOR[float] :shape (<1 x N> 1 1) :named ChainTMP2614 :vec-state [maybe-not-computed] ((0.6495824)) :facet :input :requires-grad NIL :backward <Node: FLEXIBLE-RANK-NODE-T (A[~] -> A[~])>} (!view (randn `(1)) `(:broadcast 100)) {CPUTENSOR[float] :shape (1) -> :view (<(BROADCAST 100)>) -> :visible-shape (100) :named ChainTMP4406 :vec-state [maybe-not-computed] (-0.5008113 -0.5008113 -0.5008113 ~ -0.5008113 -0.5008113 -0.5008113) :facet :input :requires-grad NIL :backward <Node: VIEWTENSORNODE-T (A[RESULT] B[BEFORE] -> A[RESULT])>} (0) The function (!flexible) adds the broadcastable dimensions of the given tensor. In <1 x N> parts, 1 is repeated, 1 is added if any. In 1 parts, never broadcasted. (!view a `(:broadcast 10)) Mem: If both of given tensors is broadcasted, we may need to make a copy to store the result since there's no array of broadcasted size. This explicts: in which tensor, is broadcasting applied?, that is, there's more likely to useless copy is also removed. in-place broadcasting.","title":"Optional Broadcasting"},{"location":"overview/#case1-to-higher-batched-operation","text":"(!matmul (!flexible ...) (!flexible ...))","title":"Case1 - To higher, Batched Operation"},{"location":"overview/#case2-to-lower-add-biases-to-columns","text":"(!add a (!view x ...)) TODO: (with-broadcasting (a1 b1 (a b)) ...) macro.","title":"Case2 - To lower,  add biases to columns"},{"location":"overview/#multidimensional-offsets","text":"(TODO)","title":"Multidimensional Offsets."},{"location":"overview/#optimizing-model-parameter","text":"(TODO) defoptimizer deftrainer parameter Tutorials Over! (TO ADD: ./Examples, training MNIST, Image processing, NLP etc...) I'll keep my finger crossed.","title":"Optimizing Model Parameter"},{"location":"utils/","text":"[package] cl-waffe2","title":"cl-waffe2"},{"location":"utils/#package-cl-waffe2","text":"","title":"[package] cl-waffe2"}]}