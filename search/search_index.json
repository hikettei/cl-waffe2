{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Programmable Deep Learning Framework Repository \u00bb Issues \u00b7 Installing \u00b7 Tutorials Introduction \u26a0\ufe0f cl-waffe2 is still in the experimental stage. Things are subject to change, and APIs can be changed without warnings. DO NOT USE CL-WAFFE2 IN YOUR PRODUCT. cl-waffe2 provides fast, systematic, easy to optimize, customizable, and environment- and device- independent abstract matrix operations and reverse mode tape-based Automatic Differentiation on Common Lisp. Plus, we also provide features for building and training neural network models, accelerated by JIT Compiler. What's the next? Setting up cl-waffe2 Concepts and tutorials Examples","title":"Overview"},{"location":"#_1","text":"","title":""},{"location":"#introduction","text":"\u26a0\ufe0f cl-waffe2 is still in the experimental stage. Things are subject to change, and APIs can be changed without warnings. DO NOT USE CL-WAFFE2 IN YOUR PRODUCT. cl-waffe2 provides fast, systematic, easy to optimize, customizable, and environment- and device- independent abstract matrix operations and reverse mode tape-based Automatic Differentiation on Common Lisp. Plus, we also provide features for building and training neural network models, accelerated by JIT Compiler.","title":"Introduction"},{"location":"#whats-the-next","text":"Setting up cl-waffe2 Concepts and tutorials Examples","title":"What's the next?"},{"location":"Tips/","text":"","title":"Tips"},{"location":"base-impl-nodes/","text":".katex img { object-fit: fill; padding: unset; display: block; position: absolute; width: 100%; height: inherit; } Standard Nodes [node] ADDNODE (A[~] B[~] -> A[~]) Description AddNode is a node which computes following operation element-wise. Let X and Y be a given arguments and both are matrix. X \u2190 X + Y X\\gets{X + Y} X \u2190 X + Y Constructor (AddNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dx dy)) (values dout dout)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SUBNODE (A[~] B[~] -> A[~]) Description SubNode is a node which computes following operation element-wise. Let X and Y be a given arguments and both are matrix. X \u2190 X \u2212 Y X\\gets{X - Y} X \u2190 X \u2212 Y Constructor (SubNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dx dy)) (values dout (!mul -1 dout))) No need to implement backwards at define-impl . (they'd be ignored.) [node] MULNODE (A[~] B[~] -> A[~]) Description MulNode is a node which computes following operation element-wise. Let X and Y be a given arguments and both are matrix. X \u2190 X \u2217 Y X\\gets{X * Y} X \u2190 X \u2217 Y Constructor (MulNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx dy) (values (!mul dout dy) (!mul dout dx))) No need to implement backwards at define-impl . (they'd be ignored.) [node] DIVNODE (A[~] B[~] -> A[~]) Description DivNode is a node which computes following operation element-wise. Let X and Y be a given arguments and both are matrix. X \u2190 X / Y X\\gets{X / Y} X \u2190 X / Y Constructor (DivNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx dy) (values (!div dout dy) (!div (!mul dx (!mul -1 dout)) (!mul dy dy)))) No need to implement backwards at define-impl . (they'd be ignored.) [node] INVERSETENSORNODE (A[~] -> A[~]) Description InverseTensorNode is a node which computes following operation element-wise A \u2190 1 / A A\\gets{1 / A} A \u2190 1/ A Constructor (InverseTensorNode dtype) dtype indicates dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx) (values (!div (!mul -1 dout) (!mul dx dx)))) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALARADD (A[~] SCALAR[SCAL] -> A[~] WHERE SCAL = 1) Description ScalarAdd is a node which computes following operation element-wise. Let X be a given matrix and S be a given scalar. X \u2190 X + s c a l a r X\\gets{X + scalar} X \u2190 X + sc a l a r Constructor (ScalarAdd dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dx dy)) (values dout (->scal (!mean dout)))) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALARSUB (A[~] SCALAR[SCAL] -> A[~] WHERE SCAL = 1) Description ScalarSub is a node which computes following operation element-wise. Let X be a given matrix and S be a given scalar. X \u2190 X \u2212 s c a l a r X\\gets{X - scalar} X \u2190 X \u2212 sc a l a r Constructor (ScalarSub dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dx dy)) (values dout (->scal (!mul -1.0 (!mean dout))))) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALARMUL (A[~] SCALAR[SCAL] -> A[~] WHERE SCAL = 1) Description ScalarMul is a node which computes following operation element-wise. Let X be a given matrix and S be a given scalar. X \u2190 X \u2217 s c a l a r X\\gets{X * scalar} X \u2190 X \u2217 sc a l a r Constructor (ScalarMul dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx dy) (values (!mul dout dy) (->scal (!mean (!mul dx dout))))) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALARDIV (A[~] SCALAR[SCAL] -> A[~] WHERE SCAL = 1) Description ScalarDiv is a node which computes following operation element-wise. Let X be a given matrix and S be a given scalar. X \u2190 X / s c a l a r X\\gets{X / scalar} X \u2190 X / sc a l a r Constructor (ScalarDiv dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx dy) (values (!div dout dy) (->scal (!mean (!div (!mul dx (!mul -1 dout)) (!mul dy dy)))))) No need to implement backwards at define-impl . (they'd be ignored.) [node] MOVETENSORNODE (A[~] B[~] -> A[~]) Description Moves all the visible elements of B into visible areas of A . A \u2190 B A\\gets{B} A \u2190 B Constructor (MoveTensorNode dtype) dtype dtype to use. Backward \u2705 Already defined. ((self dout dx dy) (let ((dy-out (if (and (eql (tensor-attribute dy) chain) (movetensor-ignore-me self)) dout (if (tensor-permuted-p dout) (let ((out (make-input (shape dx) nil create-from dout dtype (dtype dx) order (order dx)))) (!move out dout force t)) (!copy dout force t))))) (values nil dy-out))) No need to implement backwards at define-impl . (they'd be ignored.) [node] ABSNODE (X[~] OUT[~] -> OUT[~]) Description The node ABSNODE takes X as an argument, applying a abs function into each element and writes the result into out. O U T \u2190 a b s ( X ) OUT\\gets{abs(X)} O U T \u2190 ab s ( X ) save-for-backward: (T NIL) See also: SCALAR-ABSNODE !abs Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dy)) (values (!mul dout (!sign dx)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-ABSNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-ABSNODE takes scalar X as an argument, applying a abs function into each element and writes the result into out. o u t \u2190 a b s ( x ) out\\gets{abs(x)} o u t \u2190 ab s ( x ) save-for-backward: (T NIL) See also: ABSNODE !abs Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dy)) (values (!mul dout (!sign dx)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SIGNNODE (X[~] OUT[~] -> OUT[~]) Description The node SIGNNODE takes X as an argument, applying a sign function into each element and writes the result into out. O U T \u2190 s i g n ( X ) OUT\\gets{sign(X)} O U T \u2190 s i g n ( X ) save-for-backward: (T NIL) See also: SCALAR-SIGNNODE !sign Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dout dy)) (values (!mul dx 0) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-SIGNNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-SIGNNODE takes scalar X as an argument, applying a sign function into each element and writes the result into out. o u t \u2190 s i g n ( x ) out\\gets{sign(x)} o u t \u2190 s i g n ( x ) save-for-backward: (T NIL) See also: SIGNNODE !sign Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dout dy)) (values (!mul dx 0) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SQRTNODE (X[~] OUT[~] -> OUT[~]) Description The node SQRTNODE takes X as an argument, applying a sqrt function into each element and writes the result into out. O U T \u2190 s q r t ( X ) OUT\\gets{sqrt(X)} O U T \u2190 s q r t ( X ) save-for-backward: (T NIL) See also: SCALAR-SQRTNODE !sqrt Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dy)) (values (!mul dout (!div 1 dx)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-SQRTNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-SQRTNODE takes scalar X as an argument, applying a sqrt function into each element and writes the result into out. o u t \u2190 s q r t ( x ) out\\gets{sqrt(x)} o u t \u2190 s q r t ( x ) save-for-backward: (T NIL) See also: SQRTNODE !sqrt Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dy)) (values (!mul dout (!div 1 dx)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SQUARENODE (X[~] OUT[~] -> OUT[~]) Description The node SQUARENODE takes X as an argument, applying a square function into each element and writes the result into out. O U T \u2190 s q u a r e ( X ) OUT\\gets{square(X)} O U T \u2190 s q u a re ( X ) save-for-backward: (T NIL) See also: SCALAR-SQUARENODE !square Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout x) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-SQUARENODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-SQUARENODE takes scalar X as an argument, applying a square function into each element and writes the result into out. o u t \u2190 s q u a r e ( x ) out\\gets{square(x)} o u t \u2190 s q u a re ( x ) save-for-backward: (T NIL) See also: SQUARENODE !square Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout x) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SINNODE (X[~] OUT[~] -> OUT[~]) Description The node SINNODE takes X as an argument, applying a sin function into each element and writes the result into out. O U T \u2190 s i n ( X ) OUT\\gets{sin(X)} O U T \u2190 s in ( X ) save-for-backward: (T NIL) See also: SCALAR-SINNODE !sin Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!cos x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-SINNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-SINNODE takes scalar X as an argument, applying a sin function into each element and writes the result into out. o u t \u2190 s i n ( x ) out\\gets{sin(x)} o u t \u2190 s in ( x ) save-for-backward: (T NIL) See also: SINNODE !sin Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!cos x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] COSNODE (X[~] OUT[~] -> OUT[~]) Description The node COSNODE takes X as an argument, applying a cos function into each element and writes the result into out. O U T \u2190 c o s ( X ) OUT\\gets{cos(X)} O U T \u2190 cos ( X ) save-for-backward: (T NIL) See also: SCALAR-COSNODE !cos Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!mul -1 (!sin x))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-COSNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-COSNODE takes scalar X as an argument, applying a cos function into each element and writes the result into out. o u t \u2190 c o s ( x ) out\\gets{cos(x)} o u t \u2190 cos ( x ) save-for-backward: (T NIL) See also: COSNODE !cos Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!mul -1 (!sin x))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] TANNODE (X[~] OUT[~] -> OUT[~]) Description The node TANNODE takes X as an argument, applying a tan function into each element and writes the result into out. O U T \u2190 t a n ( X ) OUT\\gets{tan(X)} O U T \u2190 t an ( X ) save-for-backward: (T NIL) See also: SCALAR-TANNODE !tan Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul (!cos x) (!cos x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-TANNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-TANNODE takes scalar X as an argument, applying a tan function into each element and writes the result into out. o u t \u2190 t a n ( x ) out\\gets{tan(x)} o u t \u2190 t an ( x ) save-for-backward: (T NIL) See also: TANNODE !tan Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul (!cos x) (!cos x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] ASINNODE (X[~] OUT[~] -> OUT[~]) Description The node ASINNODE takes X as an argument, applying a asin function into each element and writes the result into out. O U T \u2190 a s i n ( X ) OUT\\gets{asin(X)} O U T \u2190 a s in ( X ) save-for-backward: (T NIL) See also: SCALAR-ASINNODE !asin Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!sqrt (!sub 1 (!square x))))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-ASINNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-ASINNODE takes scalar X as an argument, applying a asin function into each element and writes the result into out. o u t \u2190 a s i n ( x ) out\\gets{asin(x)} o u t \u2190 a s in ( x ) save-for-backward: (T NIL) See also: ASINNODE !asin Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!sqrt (!sub 1 (!square x))))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] ACOSNODE (X[~] OUT[~] -> OUT[~]) Description The node ACOSNODE takes X as an argument, applying a acos function into each element and writes the result into out. O U T \u2190 a c o s ( X ) OUT\\gets{acos(X)} O U T \u2190 a cos ( X ) save-for-backward: (T NIL) See also: SCALAR-ACOSNODE !acos Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div -1 (!sqrt (!sub 1 (!square x))))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-ACOSNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-ACOSNODE takes scalar X as an argument, applying a acos function into each element and writes the result into out. o u t \u2190 a c o s ( x ) out\\gets{acos(x)} o u t \u2190 a cos ( x ) save-for-backward: (T NIL) See also: ACOSNODE !acos Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div -1 (!sqrt (!sub 1 (!square x))))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] ATANNODE (X[~] OUT[~] -> OUT[~]) Description The node ATANNODE takes X as an argument, applying a atan function into each element and writes the result into out. O U T \u2190 a t a n ( X ) OUT\\gets{atan(X)} O U T \u2190 a t an ( X ) save-for-backward: (T NIL) See also: SCALAR-ATANNODE !atan Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!add 1 (!square x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-ATANNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-ATANNODE takes scalar X as an argument, applying a atan function into each element and writes the result into out. o u t \u2190 a t a n ( x ) out\\gets{atan(x)} o u t \u2190 a t an ( x ) save-for-backward: (T NIL) See also: ATANNODE !atan Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!add 1 (!square x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SINHNODE (X[~] OUT[~] -> OUT[~]) Description The node SINHNODE takes X as an argument, applying a sinh function into each element and writes the result into out. O U T \u2190 s i n h ( X ) OUT\\gets{sinh(X)} O U T \u2190 s inh ( X ) save-for-backward: (T NIL) See also: SCALAR-SINHNODE !sinh Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!cosh x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-SINHNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-SINHNODE takes scalar X as an argument, applying a sinh function into each element and writes the result into out. o u t \u2190 s i n h ( x ) out\\gets{sinh(x)} o u t \u2190 s inh ( x ) save-for-backward: (T NIL) See also: SINHNODE !sinh Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!cosh x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] COSHNODE (X[~] OUT[~] -> OUT[~]) Description The node COSHNODE takes X as an argument, applying a cosh function into each element and writes the result into out. O U T \u2190 c o s h ( X ) OUT\\gets{cosh(X)} O U T \u2190 cos h ( X ) save-for-backward: (T NIL) See also: SCALAR-COSHNODE !cosh Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!mul -1 (!sinh x))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-COSHNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-COSHNODE takes scalar X as an argument, applying a cosh function into each element and writes the result into out. o u t \u2190 c o s h ( x ) out\\gets{cosh(x)} o u t \u2190 cos h ( x ) save-for-backward: (T NIL) See also: COSHNODE !cosh Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!mul -1 (!sinh x))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] TANHNODE (X[~] OUT[~] -> OUT[~]) Description The node TANHNODE takes X as an argument, applying a tanh function into each element and writes the result into out. O U T \u2190 t a n h ( X ) OUT\\gets{tanh(X)} O U T \u2190 t anh ( X ) save-for-backward: (T NIL) See also: SCALAR-TANHNODE !tanh Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul (!cosh x) (!cosh x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-TANHNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-TANHNODE takes scalar X as an argument, applying a tanh function into each element and writes the result into out. o u t \u2190 t a n h ( x ) out\\gets{tanh(x)} o u t \u2190 t anh ( x ) save-for-backward: (T NIL) See also: TANHNODE !tanh Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul (!cosh x) (!cosh x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] ASINHNODE (X[~] OUT[~] -> OUT[~]) Description The node ASINHNODE takes X as an argument, applying a asinh function into each element and writes the result into out. O U T \u2190 a s i n h ( X ) OUT\\gets{asinh(X)} O U T \u2190 a s inh ( X ) save-for-backward: NIL See also: SCALAR-ASINHNODE !asinh Backward \u274c Undefined. (To make it differentiable, must be defined with define-impl macro.) [node] SCALAR-ASINHNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-ASINHNODE takes scalar X as an argument, applying a asinh function into each element and writes the result into out. o u t \u2190 a s i n h ( x ) out\\gets{asinh(x)} o u t \u2190 a s inh ( x ) save-for-backward: NIL See also: ASINHNODE !asinh Backward \u274c Undefined. (To make it differentiable, must be defined with define-impl macro.) [node] ACOSHNODE (X[~] OUT[~] -> OUT[~]) Description The node ACOSHNODE takes X as an argument, applying a acosh function into each element and writes the result into out. O U T \u2190 a c o s h ( X ) OUT\\gets{acosh(X)} O U T \u2190 a cos h ( X ) save-for-backward: NIL See also: SCALAR-ACOSHNODE !acosh Backward \u274c Undefined. (To make it differentiable, must be defined with define-impl macro.) [node] SCALAR-ACOSHNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-ACOSHNODE takes scalar X as an argument, applying a acosh function into each element and writes the result into out. o u t \u2190 a c o s h ( x ) out\\gets{acosh(x)} o u t \u2190 a cos h ( x ) save-for-backward: NIL See also: ACOSHNODE !acosh Backward \u274c Undefined. (To make it differentiable, must be defined with define-impl macro.) [node] ATANHNODE (X[~] OUT[~] -> OUT[~]) Description The node ATANHNODE takes X as an argument, applying a atanh function into each element and writes the result into out. O U T \u2190 a t a n h ( X ) OUT\\gets{atanh(X)} O U T \u2190 a t anh ( X ) save-for-backward: NIL See also: SCALAR-ATANHNODE !atanh Backward \u274c Undefined. (To make it differentiable, must be defined with define-impl macro.) [node] SCALAR-ATANHNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-ATANHNODE takes scalar X as an argument, applying a atanh function into each element and writes the result into out. o u t \u2190 a t a n h ( x ) out\\gets{atanh(x)} o u t \u2190 a t anh ( x ) save-for-backward: NIL See also: ATANHNODE !atanh Backward \u274c Undefined. (To make it differentiable, must be defined with define-impl macro.) [node] EXPNODE (X[~] OUT[~] -> OUT[~]) Description The node EXPNODE takes X as an argument, applying a exp function into each element and writes the result into out. O U T \u2190 e x p ( X ) OUT\\gets{exp(X)} O U T \u2190 e x p ( X ) save-for-backward: (T NIL) See also: SCALAR-EXPNODE !exp Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!exp x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-EXPNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-EXPNODE takes scalar X as an argument, applying a exp function into each element and writes the result into out. o u t \u2190 e x p ( x ) out\\gets{exp(x)} o u t \u2190 e x p ( x ) save-for-backward: (T NIL) See also: EXPNODE !exp Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!exp x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] LOG2NODE (X[~] OUT[~] -> OUT[~]) Description The node LOG2NODE takes X as an argument, applying a log2 function into each element and writes the result into out. O U T \u2190 l o g 2 ( X ) OUT\\gets{log2(X)} O U T \u2190 l o g 2 ( X ) save-for-backward: (T NIL) See also: SCALAR-LOG2NODE !log2 Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul x (log 2)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-LOG2NODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-LOG2NODE takes scalar X as an argument, applying a log2 function into each element and writes the result into out. o u t \u2190 l o g 2 ( x ) out\\gets{log2(x)} o u t \u2190 l o g 2 ( x ) save-for-backward: (T NIL) See also: LOG2NODE !log2 Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul x (log 2)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] LOG10NODE (X[~] OUT[~] -> OUT[~]) Description The node LOG10NODE takes X as an argument, applying a log10 function into each element and writes the result into out. O U T \u2190 l o g 10 ( X ) OUT\\gets{log10(X)} O U T \u2190 l o g 10 ( X ) save-for-backward: (T NIL) See also: SCALAR-LOG10NODE !log10 Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul x (log 10)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-LOG10NODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-LOG10NODE takes scalar X as an argument, applying a log10 function into each element and writes the result into out. o u t \u2190 l o g 10 ( x ) out\\gets{log10(x)} o u t \u2190 l o g 10 ( x ) save-for-backward: (T NIL) See also: LOG10NODE !log10 Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul x (log 10)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] LOGENODE (X[~] OUT[~] -> OUT[~]) Description The node LOGENODE takes X as an argument, applying a loge function into each element and writes the result into out. O U T \u2190 l o g e ( X ) OUT\\gets{loge(X)} O U T \u2190 l o g e ( X ) save-for-backward: (T NIL) See also: SCALAR-LOGENODE !loge Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-LOGENODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-LOGENODE takes scalar X as an argument, applying a loge function into each element and writes the result into out. o u t \u2190 l o g e ( x ) out\\gets{loge(x)} o u t \u2190 l o g e ( x ) save-for-backward: (T NIL) See also: LOGENODE !loge Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] LAZYTRANSPOSENODE (A[~ I J] -> A[~ I J]) Description LazyTransposeNode is a matmul-dedicated node to implement zero-cost transpose. The node stores untransposed tensor at raw-tensor , when expanding matmul form, you can read it if needed. Backward \u2705 Already defined. ((self dout dx) (declare (ignore dx)) (values dout)) No need to implement backwards at define-impl . (they'd be ignored.) [node] ARGMAX-NODE (A[~] OUT[OUT-SIZE] -> OUT[OUT-SIZE]) Description ArgMax-Node finds an index of maximum value of all elements in A. OUT is overwritten with the result. A is a target to find a maximum value, and OUT is a place to set the index. Constructor (ArgMax-Node out-size) out-size the reducted shape of out . Backward \u2705 Already defined. ((self dout da do) (declare (ignore dout da do)) (values nil nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] ARGMIN-NODE (A[~] OUT[OUT-SIZE] -> OUT[OUT-SIZE]) Description ArgMin-Node finds an index of minimum value of all elements in A. OUT is overwritten with the result. A is a target to find a minimum value, and OUT is a place to set the index. Constructor (ArgMin-Node out-size) out-size the reducted shape of out . Backward \u2705 Already defined. ((self dout da do) (declare (ignore dout da do)) (values nil nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] MAXVALUE-NODE (A[~] OUT[OUT-SIZE] -> OUT[OUT-SIZE]) Description MaxValue-Node finds a maximum value of all elements in A. OUT is overwritten with the result. A is a target to find a maximum value, and OUT is a place to set the index. Constructor (MaxValue-Node out-size) out-size the reducted shape of out . Backward \u2705 Already defined. ((self dout da do) (declare (ignore do)) (let ((mask (a=b da (!view (!max da) (broadcast-to da))))) (values (!mul mask (!view dout (broadcast-to mask))) nil))) No need to implement backwards at define-impl . (they'd be ignored.) [node] MINVALUE-NODE (A[~] OUT[OUT-SIZE] -> OUT[OUT-SIZE]) Description MinValue-Node finds a minimum value of all elements in A. OUT is overwritten with the result. A is a target to find a minimum value, and OUT is a place to set the index. Constructor (MinValue-Node out-size) out-size the reducted shape of out . Backward \u2705 Already defined. ((self dout da do) (declare (ignore do)) (let ((mask (a=b da (!view (!min da) (broadcast-to da))))) (values (!mul mask (!view dout (broadcast-to mask))) nil))) No need to implement backwards at define-impl . (they'd be ignored.) [node] MATMULNODE (A[~ I J] B[~ J K] C[~ I K] -> C[~ I K]) Description MatmulNode Computes a matrix multiplication of given A and B, set the result to C. C \u2190 g e m m ( 1.0 , A , B , 0.0 , C ) C\\gets{gemm(1.0, A, B, 0.0, C)} C \u2190 g e mm ( 1.0 , A , B , 0.0 , C ) Constructor (MatMulNode dtype &key transpose-a transpose-b) dtype dtype to use. transpose-a transpose-b[boolean] becomes t if the given a or b needs to be transposed respectively. call (read-untransposed tensor) to read untransposed tensor. Backward \u2705 Already defined. ((self dout da db do) (declare (ignore do)) (values (!matmul dout (!t db)) (!matmul (!t da) dout) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] WHERE-OPERATION-NODE (A[~] OUT[~] -> OUT[~]) Description Where-Operation-Node is a node which set true-then , if the result of calling condition with each element of A, is t and if it is NIL, set false-then at corresponding position. Constructor (Where-Operation-Node condition true-then false-then) true-then and false-then is a number. condition a single argument function, each element of A is argument. (e.g.: this could be #'evenp #'oddp etc...) Backward \u2705 Already defined. ((self dout da do) (declare (ignore dout da do)) (values nil nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] COMPARE-OPERATION-NODE (A[~] B[~] OUT[~] -> OUT[~]) Description Compare-Operation-Node is a node which set true-then , if the result of calling condition with each element of A and B, if it is NIl set false-then at corresponding position. Constructor (Compare-Operation-Node condition true-then false-then) true-then and false-then is a number. condition a two arguments function, each element of A and B is argument. (e.g.: this could be #'> or #'< etc...) Backward \u2705 Already defined. ((self dout da db do) (declare (ignore dout da db do)) (values nil nil nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] IM2COLNODE (X[N C H W] COL[N C K-H K-W H-OUT W-OUT] -> COL[N C K-H K-W H-OUT W-OUT]) Description Im2ColNode is AbstractNode which implements forward propagation of nn.Unfold . The node is only executed through the cl-waffe2/nn:unfold function, so arguments for constructors are dispatched automatically. In addition, the tensor X it receive will be the one after padding has been performed. N indicates the number of batch-size, C is a channel-size. k-h , k-w represents the size of kernel, height and width respectively. h-out w-out is the size of output. stride-x stride-y is the number of stride, for the most case, specified by the stride argument in Pooling2D or Conv2D . img-out is AbstractTensor with the shape of (N C H-in W-in) , can be read by img-out-of . All symbols are exported from cl-waffe2/base-impl package. In order to implement device-specific implementation of Unfold , define-impl Im2ColNode and Col2ImNode . Backward \u2705 Already defined. ((self dout x col) (declare (ignore x col)) (with-slots ((n n) (c c) (k-h k-h) (k-w k-w) (h-out h-out) (w-out w-out) (stride-x stride-x) (stride-y stride-y)) self (values (call (col2imnode n c (h-of self) (w-of self) k-h k-w h-out w-out stride-x stride-y (img-out-of self)) dout) nil))) No need to implement backwards at define-impl . (they'd be ignored.) [node] COL2IMNODE (COL[N C K-H K-W H-OUT W-OUT] -> X[N C H W]) Description Col2ImNode is AbstractNode which implements backward propagation of nn.Unfold . See also: Im2ColNode documentation for argument descriptions. Backward \u274c Undefined. (To make it differentiable, must be defined with define-impl macro.)","title":"[Nodes] cl-waffe2/base-impl"},{"location":"base-impl-nodes/#standard-nodes","text":"","title":"Standard Nodes"},{"location":"base-impl-nodes/#node-addnode","text":"(A[~] B[~] -> A[~])","title":"[node] ADDNODE"},{"location":"base-impl-nodes/#description","text":"AddNode is a node which computes following operation element-wise. Let X and Y be a given arguments and both are matrix. X \u2190 X + Y X\\gets{X + Y} X \u2190 X + Y","title":"Description"},{"location":"base-impl-nodes/#constructor","text":"(AddNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dx dy)) (values dout dout)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-subnode","text":"(A[~] B[~] -> A[~])","title":"[node] SUBNODE"},{"location":"base-impl-nodes/#description_1","text":"SubNode is a node which computes following operation element-wise. Let X and Y be a given arguments and both are matrix. X \u2190 X \u2212 Y X\\gets{X - Y} X \u2190 X \u2212 Y","title":"Description"},{"location":"base-impl-nodes/#constructor_1","text":"(SubNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward_1","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dx dy)) (values dout (!mul -1 dout))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-mulnode","text":"(A[~] B[~] -> A[~])","title":"[node] MULNODE"},{"location":"base-impl-nodes/#description_2","text":"MulNode is a node which computes following operation element-wise. Let X and Y be a given arguments and both are matrix. X \u2190 X \u2217 Y X\\gets{X * Y} X \u2190 X \u2217 Y","title":"Description"},{"location":"base-impl-nodes/#constructor_2","text":"(MulNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward_2","text":"\u2705 Already defined. ((self dout dx dy) (values (!mul dout dy) (!mul dout dx))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-divnode","text":"(A[~] B[~] -> A[~])","title":"[node] DIVNODE"},{"location":"base-impl-nodes/#description_3","text":"DivNode is a node which computes following operation element-wise. Let X and Y be a given arguments and both are matrix. X \u2190 X / Y X\\gets{X / Y} X \u2190 X / Y","title":"Description"},{"location":"base-impl-nodes/#constructor_3","text":"(DivNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward_3","text":"\u2705 Already defined. ((self dout dx dy) (values (!div dout dy) (!div (!mul dx (!mul -1 dout)) (!mul dy dy)))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-inversetensornode","text":"(A[~] -> A[~])","title":"[node] INVERSETENSORNODE"},{"location":"base-impl-nodes/#description_4","text":"InverseTensorNode is a node which computes following operation element-wise A \u2190 1 / A A\\gets{1 / A} A \u2190 1/ A","title":"Description"},{"location":"base-impl-nodes/#constructor_4","text":"(InverseTensorNode dtype) dtype indicates dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward_4","text":"\u2705 Already defined. ((self dout dx) (values (!div (!mul -1 dout) (!mul dx dx)))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalaradd","text":"(A[~] SCALAR[SCAL] -> A[~] WHERE SCAL = 1)","title":"[node] SCALARADD"},{"location":"base-impl-nodes/#description_5","text":"ScalarAdd is a node which computes following operation element-wise. Let X be a given matrix and S be a given scalar. X \u2190 X + s c a l a r X\\gets{X + scalar} X \u2190 X + sc a l a r","title":"Description"},{"location":"base-impl-nodes/#constructor_5","text":"(ScalarAdd dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward_5","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dx dy)) (values dout (->scal (!mean dout)))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalarsub","text":"(A[~] SCALAR[SCAL] -> A[~] WHERE SCAL = 1)","title":"[node] SCALARSUB"},{"location":"base-impl-nodes/#description_6","text":"ScalarSub is a node which computes following operation element-wise. Let X be a given matrix and S be a given scalar. X \u2190 X \u2212 s c a l a r X\\gets{X - scalar} X \u2190 X \u2212 sc a l a r","title":"Description"},{"location":"base-impl-nodes/#constructor_6","text":"(ScalarSub dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward_6","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dx dy)) (values dout (->scal (!mul -1.0 (!mean dout))))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalarmul","text":"(A[~] SCALAR[SCAL] -> A[~] WHERE SCAL = 1)","title":"[node] SCALARMUL"},{"location":"base-impl-nodes/#description_7","text":"ScalarMul is a node which computes following operation element-wise. Let X be a given matrix and S be a given scalar. X \u2190 X \u2217 s c a l a r X\\gets{X * scalar} X \u2190 X \u2217 sc a l a r","title":"Description"},{"location":"base-impl-nodes/#constructor_7","text":"(ScalarMul dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward_7","text":"\u2705 Already defined. ((self dout dx dy) (values (!mul dout dy) (->scal (!mean (!mul dx dout))))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalardiv","text":"(A[~] SCALAR[SCAL] -> A[~] WHERE SCAL = 1)","title":"[node] SCALARDIV"},{"location":"base-impl-nodes/#description_8","text":"ScalarDiv is a node which computes following operation element-wise. Let X be a given matrix and S be a given scalar. X \u2190 X / s c a l a r X\\gets{X / scalar} X \u2190 X / sc a l a r","title":"Description"},{"location":"base-impl-nodes/#constructor_8","text":"(ScalarDiv dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward_8","text":"\u2705 Already defined. ((self dout dx dy) (values (!div dout dy) (->scal (!mean (!div (!mul dx (!mul -1 dout)) (!mul dy dy)))))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-movetensornode","text":"(A[~] B[~] -> A[~])","title":"[node] MOVETENSORNODE"},{"location":"base-impl-nodes/#description_9","text":"Moves all the visible elements of B into visible areas of A . A \u2190 B A\\gets{B} A \u2190 B","title":"Description"},{"location":"base-impl-nodes/#constructor_9","text":"(MoveTensorNode dtype) dtype dtype to use.","title":"Constructor"},{"location":"base-impl-nodes/#backward_9","text":"\u2705 Already defined. ((self dout dx dy) (let ((dy-out (if (and (eql (tensor-attribute dy) chain) (movetensor-ignore-me self)) dout (if (tensor-permuted-p dout) (let ((out (make-input (shape dx) nil create-from dout dtype (dtype dx) order (order dx)))) (!move out dout force t)) (!copy dout force t))))) (values nil dy-out))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-absnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] ABSNODE"},{"location":"base-impl-nodes/#description_10","text":"The node ABSNODE takes X as an argument, applying a abs function into each element and writes the result into out. O U T \u2190 a b s ( X ) OUT\\gets{abs(X)} O U T \u2190 ab s ( X ) save-for-backward: (T NIL) See also: SCALAR-ABSNODE !abs","title":"Description"},{"location":"base-impl-nodes/#backward_10","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dy)) (values (!mul dout (!sign dx)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-absnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-ABSNODE"},{"location":"base-impl-nodes/#description_11","text":"The node SCALAR-ABSNODE takes scalar X as an argument, applying a abs function into each element and writes the result into out. o u t \u2190 a b s ( x ) out\\gets{abs(x)} o u t \u2190 ab s ( x ) save-for-backward: (T NIL) See also: ABSNODE !abs","title":"Description"},{"location":"base-impl-nodes/#backward_11","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dy)) (values (!mul dout (!sign dx)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-signnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SIGNNODE"},{"location":"base-impl-nodes/#description_12","text":"The node SIGNNODE takes X as an argument, applying a sign function into each element and writes the result into out. O U T \u2190 s i g n ( X ) OUT\\gets{sign(X)} O U T \u2190 s i g n ( X ) save-for-backward: (T NIL) See also: SCALAR-SIGNNODE !sign","title":"Description"},{"location":"base-impl-nodes/#backward_12","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dout dy)) (values (!mul dx 0) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-signnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-SIGNNODE"},{"location":"base-impl-nodes/#description_13","text":"The node SCALAR-SIGNNODE takes scalar X as an argument, applying a sign function into each element and writes the result into out. o u t \u2190 s i g n ( x ) out\\gets{sign(x)} o u t \u2190 s i g n ( x ) save-for-backward: (T NIL) See also: SIGNNODE !sign","title":"Description"},{"location":"base-impl-nodes/#backward_13","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dout dy)) (values (!mul dx 0) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-sqrtnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SQRTNODE"},{"location":"base-impl-nodes/#description_14","text":"The node SQRTNODE takes X as an argument, applying a sqrt function into each element and writes the result into out. O U T \u2190 s q r t ( X ) OUT\\gets{sqrt(X)} O U T \u2190 s q r t ( X ) save-for-backward: (T NIL) See also: SCALAR-SQRTNODE !sqrt","title":"Description"},{"location":"base-impl-nodes/#backward_14","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dy)) (values (!mul dout (!div 1 dx)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-sqrtnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-SQRTNODE"},{"location":"base-impl-nodes/#description_15","text":"The node SCALAR-SQRTNODE takes scalar X as an argument, applying a sqrt function into each element and writes the result into out. o u t \u2190 s q r t ( x ) out\\gets{sqrt(x)} o u t \u2190 s q r t ( x ) save-for-backward: (T NIL) See also: SQRTNODE !sqrt","title":"Description"},{"location":"base-impl-nodes/#backward_15","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dy)) (values (!mul dout (!div 1 dx)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-squarenode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SQUARENODE"},{"location":"base-impl-nodes/#description_16","text":"The node SQUARENODE takes X as an argument, applying a square function into each element and writes the result into out. O U T \u2190 s q u a r e ( X ) OUT\\gets{square(X)} O U T \u2190 s q u a re ( X ) save-for-backward: (T NIL) See also: SCALAR-SQUARENODE !square","title":"Description"},{"location":"base-impl-nodes/#backward_16","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout x) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-squarenode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-SQUARENODE"},{"location":"base-impl-nodes/#description_17","text":"The node SCALAR-SQUARENODE takes scalar X as an argument, applying a square function into each element and writes the result into out. o u t \u2190 s q u a r e ( x ) out\\gets{square(x)} o u t \u2190 s q u a re ( x ) save-for-backward: (T NIL) See also: SQUARENODE !square","title":"Description"},{"location":"base-impl-nodes/#backward_17","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout x) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-sinnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SINNODE"},{"location":"base-impl-nodes/#description_18","text":"The node SINNODE takes X as an argument, applying a sin function into each element and writes the result into out. O U T \u2190 s i n ( X ) OUT\\gets{sin(X)} O U T \u2190 s in ( X ) save-for-backward: (T NIL) See also: SCALAR-SINNODE !sin","title":"Description"},{"location":"base-impl-nodes/#backward_18","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!cos x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-sinnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-SINNODE"},{"location":"base-impl-nodes/#description_19","text":"The node SCALAR-SINNODE takes scalar X as an argument, applying a sin function into each element and writes the result into out. o u t \u2190 s i n ( x ) out\\gets{sin(x)} o u t \u2190 s in ( x ) save-for-backward: (T NIL) See also: SINNODE !sin","title":"Description"},{"location":"base-impl-nodes/#backward_19","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!cos x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-cosnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] COSNODE"},{"location":"base-impl-nodes/#description_20","text":"The node COSNODE takes X as an argument, applying a cos function into each element and writes the result into out. O U T \u2190 c o s ( X ) OUT\\gets{cos(X)} O U T \u2190 cos ( X ) save-for-backward: (T NIL) See also: SCALAR-COSNODE !cos","title":"Description"},{"location":"base-impl-nodes/#backward_20","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!mul -1 (!sin x))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-cosnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-COSNODE"},{"location":"base-impl-nodes/#description_21","text":"The node SCALAR-COSNODE takes scalar X as an argument, applying a cos function into each element and writes the result into out. o u t \u2190 c o s ( x ) out\\gets{cos(x)} o u t \u2190 cos ( x ) save-for-backward: (T NIL) See also: COSNODE !cos","title":"Description"},{"location":"base-impl-nodes/#backward_21","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!mul -1 (!sin x))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-tannode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] TANNODE"},{"location":"base-impl-nodes/#description_22","text":"The node TANNODE takes X as an argument, applying a tan function into each element and writes the result into out. O U T \u2190 t a n ( X ) OUT\\gets{tan(X)} O U T \u2190 t an ( X ) save-for-backward: (T NIL) See also: SCALAR-TANNODE !tan","title":"Description"},{"location":"base-impl-nodes/#backward_22","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul (!cos x) (!cos x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-tannode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-TANNODE"},{"location":"base-impl-nodes/#description_23","text":"The node SCALAR-TANNODE takes scalar X as an argument, applying a tan function into each element and writes the result into out. o u t \u2190 t a n ( x ) out\\gets{tan(x)} o u t \u2190 t an ( x ) save-for-backward: (T NIL) See also: TANNODE !tan","title":"Description"},{"location":"base-impl-nodes/#backward_23","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul (!cos x) (!cos x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-asinnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] ASINNODE"},{"location":"base-impl-nodes/#description_24","text":"The node ASINNODE takes X as an argument, applying a asin function into each element and writes the result into out. O U T \u2190 a s i n ( X ) OUT\\gets{asin(X)} O U T \u2190 a s in ( X ) save-for-backward: (T NIL) See also: SCALAR-ASINNODE !asin","title":"Description"},{"location":"base-impl-nodes/#backward_24","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!sqrt (!sub 1 (!square x))))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-asinnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-ASINNODE"},{"location":"base-impl-nodes/#description_25","text":"The node SCALAR-ASINNODE takes scalar X as an argument, applying a asin function into each element and writes the result into out. o u t \u2190 a s i n ( x ) out\\gets{asin(x)} o u t \u2190 a s in ( x ) save-for-backward: (T NIL) See also: ASINNODE !asin","title":"Description"},{"location":"base-impl-nodes/#backward_25","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!sqrt (!sub 1 (!square x))))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-acosnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] ACOSNODE"},{"location":"base-impl-nodes/#description_26","text":"The node ACOSNODE takes X as an argument, applying a acos function into each element and writes the result into out. O U T \u2190 a c o s ( X ) OUT\\gets{acos(X)} O U T \u2190 a cos ( X ) save-for-backward: (T NIL) See also: SCALAR-ACOSNODE !acos","title":"Description"},{"location":"base-impl-nodes/#backward_26","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div -1 (!sqrt (!sub 1 (!square x))))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-acosnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-ACOSNODE"},{"location":"base-impl-nodes/#description_27","text":"The node SCALAR-ACOSNODE takes scalar X as an argument, applying a acos function into each element and writes the result into out. o u t \u2190 a c o s ( x ) out\\gets{acos(x)} o u t \u2190 a cos ( x ) save-for-backward: (T NIL) See also: ACOSNODE !acos","title":"Description"},{"location":"base-impl-nodes/#backward_27","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div -1 (!sqrt (!sub 1 (!square x))))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-atannode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] ATANNODE"},{"location":"base-impl-nodes/#description_28","text":"The node ATANNODE takes X as an argument, applying a atan function into each element and writes the result into out. O U T \u2190 a t a n ( X ) OUT\\gets{atan(X)} O U T \u2190 a t an ( X ) save-for-backward: (T NIL) See also: SCALAR-ATANNODE !atan","title":"Description"},{"location":"base-impl-nodes/#backward_28","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!add 1 (!square x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-atannode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-ATANNODE"},{"location":"base-impl-nodes/#description_29","text":"The node SCALAR-ATANNODE takes scalar X as an argument, applying a atan function into each element and writes the result into out. o u t \u2190 a t a n ( x ) out\\gets{atan(x)} o u t \u2190 a t an ( x ) save-for-backward: (T NIL) See also: ATANNODE !atan","title":"Description"},{"location":"base-impl-nodes/#backward_29","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!add 1 (!square x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-sinhnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SINHNODE"},{"location":"base-impl-nodes/#description_30","text":"The node SINHNODE takes X as an argument, applying a sinh function into each element and writes the result into out. O U T \u2190 s i n h ( X ) OUT\\gets{sinh(X)} O U T \u2190 s inh ( X ) save-for-backward: (T NIL) See also: SCALAR-SINHNODE !sinh","title":"Description"},{"location":"base-impl-nodes/#backward_30","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!cosh x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-sinhnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-SINHNODE"},{"location":"base-impl-nodes/#description_31","text":"The node SCALAR-SINHNODE takes scalar X as an argument, applying a sinh function into each element and writes the result into out. o u t \u2190 s i n h ( x ) out\\gets{sinh(x)} o u t \u2190 s inh ( x ) save-for-backward: (T NIL) See also: SINHNODE !sinh","title":"Description"},{"location":"base-impl-nodes/#backward_31","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!cosh x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-coshnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] COSHNODE"},{"location":"base-impl-nodes/#description_32","text":"The node COSHNODE takes X as an argument, applying a cosh function into each element and writes the result into out. O U T \u2190 c o s h ( X ) OUT\\gets{cosh(X)} O U T \u2190 cos h ( X ) save-for-backward: (T NIL) See also: SCALAR-COSHNODE !cosh","title":"Description"},{"location":"base-impl-nodes/#backward_32","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!mul -1 (!sinh x))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-coshnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-COSHNODE"},{"location":"base-impl-nodes/#description_33","text":"The node SCALAR-COSHNODE takes scalar X as an argument, applying a cosh function into each element and writes the result into out. o u t \u2190 c o s h ( x ) out\\gets{cosh(x)} o u t \u2190 cos h ( x ) save-for-backward: (T NIL) See also: COSHNODE !cosh","title":"Description"},{"location":"base-impl-nodes/#backward_33","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!mul -1 (!sinh x))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-tanhnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] TANHNODE"},{"location":"base-impl-nodes/#description_34","text":"The node TANHNODE takes X as an argument, applying a tanh function into each element and writes the result into out. O U T \u2190 t a n h ( X ) OUT\\gets{tanh(X)} O U T \u2190 t anh ( X ) save-for-backward: (T NIL) See also: SCALAR-TANHNODE !tanh","title":"Description"},{"location":"base-impl-nodes/#backward_34","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul (!cosh x) (!cosh x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-tanhnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-TANHNODE"},{"location":"base-impl-nodes/#description_35","text":"The node SCALAR-TANHNODE takes scalar X as an argument, applying a tanh function into each element and writes the result into out. o u t \u2190 t a n h ( x ) out\\gets{tanh(x)} o u t \u2190 t anh ( x ) save-for-backward: (T NIL) See also: TANHNODE !tanh","title":"Description"},{"location":"base-impl-nodes/#backward_35","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul (!cosh x) (!cosh x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-asinhnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] ASINHNODE"},{"location":"base-impl-nodes/#description_36","text":"The node ASINHNODE takes X as an argument, applying a asinh function into each element and writes the result into out. O U T \u2190 a s i n h ( X ) OUT\\gets{asinh(X)} O U T \u2190 a s inh ( X ) save-for-backward: NIL See also: SCALAR-ASINHNODE !asinh","title":"Description"},{"location":"base-impl-nodes/#backward_36","text":"\u274c Undefined. (To make it differentiable, must be defined with define-impl macro.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-asinhnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-ASINHNODE"},{"location":"base-impl-nodes/#description_37","text":"The node SCALAR-ASINHNODE takes scalar X as an argument, applying a asinh function into each element and writes the result into out. o u t \u2190 a s i n h ( x ) out\\gets{asinh(x)} o u t \u2190 a s inh ( x ) save-for-backward: NIL See also: ASINHNODE !asinh","title":"Description"},{"location":"base-impl-nodes/#backward_37","text":"\u274c Undefined. (To make it differentiable, must be defined with define-impl macro.)","title":"Backward"},{"location":"base-impl-nodes/#node-acoshnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] ACOSHNODE"},{"location":"base-impl-nodes/#description_38","text":"The node ACOSHNODE takes X as an argument, applying a acosh function into each element and writes the result into out. O U T \u2190 a c o s h ( X ) OUT\\gets{acosh(X)} O U T \u2190 a cos h ( X ) save-for-backward: NIL See also: SCALAR-ACOSHNODE !acosh","title":"Description"},{"location":"base-impl-nodes/#backward_38","text":"\u274c Undefined. (To make it differentiable, must be defined with define-impl macro.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-acoshnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-ACOSHNODE"},{"location":"base-impl-nodes/#description_39","text":"The node SCALAR-ACOSHNODE takes scalar X as an argument, applying a acosh function into each element and writes the result into out. o u t \u2190 a c o s h ( x ) out\\gets{acosh(x)} o u t \u2190 a cos h ( x ) save-for-backward: NIL See also: ACOSHNODE !acosh","title":"Description"},{"location":"base-impl-nodes/#backward_39","text":"\u274c Undefined. (To make it differentiable, must be defined with define-impl macro.)","title":"Backward"},{"location":"base-impl-nodes/#node-atanhnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] ATANHNODE"},{"location":"base-impl-nodes/#description_40","text":"The node ATANHNODE takes X as an argument, applying a atanh function into each element and writes the result into out. O U T \u2190 a t a n h ( X ) OUT\\gets{atanh(X)} O U T \u2190 a t anh ( X ) save-for-backward: NIL See also: SCALAR-ATANHNODE !atanh","title":"Description"},{"location":"base-impl-nodes/#backward_40","text":"\u274c Undefined. (To make it differentiable, must be defined with define-impl macro.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-atanhnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-ATANHNODE"},{"location":"base-impl-nodes/#description_41","text":"The node SCALAR-ATANHNODE takes scalar X as an argument, applying a atanh function into each element and writes the result into out. o u t \u2190 a t a n h ( x ) out\\gets{atanh(x)} o u t \u2190 a t anh ( x ) save-for-backward: NIL See also: ATANHNODE !atanh","title":"Description"},{"location":"base-impl-nodes/#backward_41","text":"\u274c Undefined. (To make it differentiable, must be defined with define-impl macro.)","title":"Backward"},{"location":"base-impl-nodes/#node-expnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] EXPNODE"},{"location":"base-impl-nodes/#description_42","text":"The node EXPNODE takes X as an argument, applying a exp function into each element and writes the result into out. O U T \u2190 e x p ( X ) OUT\\gets{exp(X)} O U T \u2190 e x p ( X ) save-for-backward: (T NIL) See also: SCALAR-EXPNODE !exp","title":"Description"},{"location":"base-impl-nodes/#backward_42","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!exp x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-expnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-EXPNODE"},{"location":"base-impl-nodes/#description_43","text":"The node SCALAR-EXPNODE takes scalar X as an argument, applying a exp function into each element and writes the result into out. o u t \u2190 e x p ( x ) out\\gets{exp(x)} o u t \u2190 e x p ( x ) save-for-backward: (T NIL) See also: EXPNODE !exp","title":"Description"},{"location":"base-impl-nodes/#backward_43","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!exp x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-log2node","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] LOG2NODE"},{"location":"base-impl-nodes/#description_44","text":"The node LOG2NODE takes X as an argument, applying a log2 function into each element and writes the result into out. O U T \u2190 l o g 2 ( X ) OUT\\gets{log2(X)} O U T \u2190 l o g 2 ( X ) save-for-backward: (T NIL) See also: SCALAR-LOG2NODE !log2","title":"Description"},{"location":"base-impl-nodes/#backward_44","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul x (log 2)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-log2node","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-LOG2NODE"},{"location":"base-impl-nodes/#description_45","text":"The node SCALAR-LOG2NODE takes scalar X as an argument, applying a log2 function into each element and writes the result into out. o u t \u2190 l o g 2 ( x ) out\\gets{log2(x)} o u t \u2190 l o g 2 ( x ) save-for-backward: (T NIL) See also: LOG2NODE !log2","title":"Description"},{"location":"base-impl-nodes/#backward_45","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul x (log 2)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-log10node","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] LOG10NODE"},{"location":"base-impl-nodes/#description_46","text":"The node LOG10NODE takes X as an argument, applying a log10 function into each element and writes the result into out. O U T \u2190 l o g 10 ( X ) OUT\\gets{log10(X)} O U T \u2190 l o g 10 ( X ) save-for-backward: (T NIL) See also: SCALAR-LOG10NODE !log10","title":"Description"},{"location":"base-impl-nodes/#backward_46","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul x (log 10)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-log10node","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-LOG10NODE"},{"location":"base-impl-nodes/#description_47","text":"The node SCALAR-LOG10NODE takes scalar X as an argument, applying a log10 function into each element and writes the result into out. o u t \u2190 l o g 10 ( x ) out\\gets{log10(x)} o u t \u2190 l o g 10 ( x ) save-for-backward: (T NIL) See also: LOG10NODE !log10","title":"Description"},{"location":"base-impl-nodes/#backward_47","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul x (log 10)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-logenode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] LOGENODE"},{"location":"base-impl-nodes/#description_48","text":"The node LOGENODE takes X as an argument, applying a loge function into each element and writes the result into out. O U T \u2190 l o g e ( X ) OUT\\gets{loge(X)} O U T \u2190 l o g e ( X ) save-for-backward: (T NIL) See also: SCALAR-LOGENODE !loge","title":"Description"},{"location":"base-impl-nodes/#backward_48","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-logenode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-LOGENODE"},{"location":"base-impl-nodes/#description_49","text":"The node SCALAR-LOGENODE takes scalar X as an argument, applying a loge function into each element and writes the result into out. o u t \u2190 l o g e ( x ) out\\gets{loge(x)} o u t \u2190 l o g e ( x ) save-for-backward: (T NIL) See also: LOGENODE !loge","title":"Description"},{"location":"base-impl-nodes/#backward_49","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-lazytransposenode","text":"(A[~ I J] -> A[~ I J])","title":"[node] LAZYTRANSPOSENODE"},{"location":"base-impl-nodes/#description_50","text":"LazyTransposeNode is a matmul-dedicated node to implement zero-cost transpose. The node stores untransposed tensor at raw-tensor , when expanding matmul form, you can read it if needed.","title":"Description"},{"location":"base-impl-nodes/#backward_50","text":"\u2705 Already defined. ((self dout dx) (declare (ignore dx)) (values dout)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-argmax-node","text":"(A[~] OUT[OUT-SIZE] -> OUT[OUT-SIZE])","title":"[node] ARGMAX-NODE"},{"location":"base-impl-nodes/#description_51","text":"ArgMax-Node finds an index of maximum value of all elements in A. OUT is overwritten with the result. A is a target to find a maximum value, and OUT is a place to set the index.","title":"Description"},{"location":"base-impl-nodes/#constructor_10","text":"(ArgMax-Node out-size) out-size the reducted shape of out .","title":"Constructor"},{"location":"base-impl-nodes/#backward_51","text":"\u2705 Already defined. ((self dout da do) (declare (ignore dout da do)) (values nil nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-argmin-node","text":"(A[~] OUT[OUT-SIZE] -> OUT[OUT-SIZE])","title":"[node] ARGMIN-NODE"},{"location":"base-impl-nodes/#description_52","text":"ArgMin-Node finds an index of minimum value of all elements in A. OUT is overwritten with the result. A is a target to find a minimum value, and OUT is a place to set the index.","title":"Description"},{"location":"base-impl-nodes/#constructor_11","text":"(ArgMin-Node out-size) out-size the reducted shape of out .","title":"Constructor"},{"location":"base-impl-nodes/#backward_52","text":"\u2705 Already defined. ((self dout da do) (declare (ignore dout da do)) (values nil nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-maxvalue-node","text":"(A[~] OUT[OUT-SIZE] -> OUT[OUT-SIZE])","title":"[node] MAXVALUE-NODE"},{"location":"base-impl-nodes/#description_53","text":"MaxValue-Node finds a maximum value of all elements in A. OUT is overwritten with the result. A is a target to find a maximum value, and OUT is a place to set the index.","title":"Description"},{"location":"base-impl-nodes/#constructor_12","text":"(MaxValue-Node out-size) out-size the reducted shape of out .","title":"Constructor"},{"location":"base-impl-nodes/#backward_53","text":"\u2705 Already defined. ((self dout da do) (declare (ignore do)) (let ((mask (a=b da (!view (!max da) (broadcast-to da))))) (values (!mul mask (!view dout (broadcast-to mask))) nil))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-minvalue-node","text":"(A[~] OUT[OUT-SIZE] -> OUT[OUT-SIZE])","title":"[node] MINVALUE-NODE"},{"location":"base-impl-nodes/#description_54","text":"MinValue-Node finds a minimum value of all elements in A. OUT is overwritten with the result. A is a target to find a minimum value, and OUT is a place to set the index.","title":"Description"},{"location":"base-impl-nodes/#constructor_13","text":"(MinValue-Node out-size) out-size the reducted shape of out .","title":"Constructor"},{"location":"base-impl-nodes/#backward_54","text":"\u2705 Already defined. ((self dout da do) (declare (ignore do)) (let ((mask (a=b da (!view (!min da) (broadcast-to da))))) (values (!mul mask (!view dout (broadcast-to mask))) nil))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-matmulnode","text":"(A[~ I J] B[~ J K] C[~ I K] -> C[~ I K])","title":"[node] MATMULNODE"},{"location":"base-impl-nodes/#description_55","text":"MatmulNode Computes a matrix multiplication of given A and B, set the result to C. C \u2190 g e m m ( 1.0 , A , B , 0.0 , C ) C\\gets{gemm(1.0, A, B, 0.0, C)} C \u2190 g e mm ( 1.0 , A , B , 0.0 , C )","title":"Description"},{"location":"base-impl-nodes/#constructor_14","text":"(MatMulNode dtype &key transpose-a transpose-b) dtype dtype to use. transpose-a transpose-b[boolean] becomes t if the given a or b needs to be transposed respectively. call (read-untransposed tensor) to read untransposed tensor.","title":"Constructor"},{"location":"base-impl-nodes/#backward_55","text":"\u2705 Already defined. ((self dout da db do) (declare (ignore do)) (values (!matmul dout (!t db)) (!matmul (!t da) dout) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-where-operation-node","text":"(A[~] OUT[~] -> OUT[~])","title":"[node] WHERE-OPERATION-NODE"},{"location":"base-impl-nodes/#description_56","text":"Where-Operation-Node is a node which set true-then , if the result of calling condition with each element of A, is t and if it is NIL, set false-then at corresponding position.","title":"Description"},{"location":"base-impl-nodes/#constructor_15","text":"(Where-Operation-Node condition true-then false-then) true-then and false-then is a number. condition a single argument function, each element of A is argument. (e.g.: this could be #'evenp #'oddp etc...)","title":"Constructor"},{"location":"base-impl-nodes/#backward_56","text":"\u2705 Already defined. ((self dout da do) (declare (ignore dout da do)) (values nil nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-compare-operation-node","text":"(A[~] B[~] OUT[~] -> OUT[~])","title":"[node] COMPARE-OPERATION-NODE"},{"location":"base-impl-nodes/#description_57","text":"Compare-Operation-Node is a node which set true-then , if the result of calling condition with each element of A and B, if it is NIl set false-then at corresponding position.","title":"Description"},{"location":"base-impl-nodes/#constructor_16","text":"(Compare-Operation-Node condition true-then false-then) true-then and false-then is a number. condition a two arguments function, each element of A and B is argument. (e.g.: this could be #'> or #'< etc...)","title":"Constructor"},{"location":"base-impl-nodes/#backward_57","text":"\u2705 Already defined. ((self dout da db do) (declare (ignore dout da db do)) (values nil nil nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-im2colnode","text":"(X[N C H W] COL[N C K-H K-W H-OUT W-OUT] -> COL[N C K-H K-W H-OUT W-OUT])","title":"[node] IM2COLNODE"},{"location":"base-impl-nodes/#description_58","text":"Im2ColNode is AbstractNode which implements forward propagation of nn.Unfold . The node is only executed through the cl-waffe2/nn:unfold function, so arguments for constructors are dispatched automatically. In addition, the tensor X it receive will be the one after padding has been performed. N indicates the number of batch-size, C is a channel-size. k-h , k-w represents the size of kernel, height and width respectively. h-out w-out is the size of output. stride-x stride-y is the number of stride, for the most case, specified by the stride argument in Pooling2D or Conv2D . img-out is AbstractTensor with the shape of (N C H-in W-in) , can be read by img-out-of . All symbols are exported from cl-waffe2/base-impl package. In order to implement device-specific implementation of Unfold , define-impl Im2ColNode and Col2ImNode .","title":"Description"},{"location":"base-impl-nodes/#backward_58","text":"\u2705 Already defined. ((self dout x col) (declare (ignore x col)) (with-slots ((n n) (c c) (k-h k-h) (k-w k-w) (h-out h-out) (w-out w-out) (stride-x stride-x) (stride-y stride-y)) self (values (call (col2imnode n c (h-of self) (w-of self) k-h k-w h-out w-out stride-x stride-y (img-out-of self)) dout) nil))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-col2imnode","text":"(COL[N C K-H K-W H-OUT W-OUT] -> X[N C H W])","title":"[node] COL2IMNODE"},{"location":"base-impl-nodes/#description_59","text":"Col2ImNode is AbstractNode which implements backward propagation of nn.Unfold . See also: Im2ColNode documentation for argument descriptions.","title":"Description"},{"location":"base-impl-nodes/#backward_59","text":"\u274c Undefined. (To make it differentiable, must be defined with define-impl macro.)","title":"Backward"},{"location":"base-impl/","text":".katex img { object-fit: fill; padding: unset; display: block; position: absolute; width: 100%; height: inherit; } Basic APIs [function] !matrix-add (!matrix-add x y) The function !matrix-add calls ADDNODE and adds X and Y element-wise, returning a new tensor. X c o p y \u2190 X + Y X_{copy}\\gets{X + Y} X co p y \u200b \u2190 X + Y Inputs X and Y must be a AbstractTensor (not a ScalarTensor), with the same shape. SideEffects None. [function] !matrix-sub (!matrix-sub x y) The function !matrix-sub calls SUBNODE and substracts X by Y element-wise, returning a new tensor. X c o p y \u2190 X \u2212 Y X_{copy}\\gets{X - Y} X co p y \u200b \u2190 X \u2212 Y Inputs X and Y must be a AbstractTensor (not a ScalarTensor), with the same shape. SideEffects None. [function] !matrix-mul (!matrix-mul x y) The function !matrix-mul calls MULNODE and multiplies X and Y element-wise, returning a new tensor. X c o p y \u2190 X \u2217 Y X_{copy}\\gets{X * Y} X co p y \u200b \u2190 X \u2217 Y Inputs X and Y must be a AbstractTensor (not a ScalarTensor), with the same shape. SideEffects None. [function] !matrix-div (!matrix-div x y) The function !matrix-div calls DIVNODE and divides X by Y element-wise, returning a new tensor. X c o p y \u2190 X / Y X_{copy}\\gets{X / Y} X co p y \u200b \u2190 X / Y Inputs X and Y must be a AbstractTensor (not a ScalarTensor), with the same shape. SideEffects None. [function] !inverse (!inverse tensor) The function !inverse calls InverseTensorNode , and finds the inverse of the received Tensor/Scalar, returning a new tensor. X c o p y \u2190 1 / X X_{copy}\\gets{1 / X} X co p y \u200b \u2190 1/ X Inputs tensor[ScalarTensor/AbstractTensor/Number] [function] !scalar-add (!scalar-add scalar x) The function !SCALAR-ADD computes following operation with calling SCALARADD , returning a new tensor. X c o p y \u2190 X + s c a l a r X_{copy}\\gets{X + scalar} X co p y \u200b \u2190 X + sc a l a r Inputs scalar could be one of ScalarTensor or number tensor AbstractTensor (should not be a scalar) [function] !scalar-sub (!scalar-sub scalar x) The function !SCALAR-SUB computes following operation with calling SCALARSUB , returning a new tensor. X c o p y \u2190 X \u2212 s c a l a r X_{copy}\\gets{X - scalar} X co p y \u200b \u2190 X \u2212 sc a l a r Inputs scalar could be one of ScalarTensor or number tensor AbstractTensor (should not be a scalar) [function] !scalar-mul (!scalar-mul scalar x) The function !SCALAR-MUL computes following operation with calling SCALARMUL , returning a new tensor. X c o p y \u2190 X \u2217 s c a l a r X_{copy}\\gets{X * scalar} X co p y \u200b \u2190 X \u2217 sc a l a r Inputs scalar could be one of ScalarTensor or number tensor AbstractTensor (should not be a scalar) [function] !scalar-div (!scalar-div scalar x) The function !SCALAR-DIV computes following operation with calling SCALARDIV , returning a new tensor. X c o p y \u2190 X / s c a l a r X_{copy}\\gets{X / scalar} X co p y \u200b \u2190 X / sc a l a r Inputs scalar could be one of ScalarTensor or number tensor AbstractTensor (should not be a scalar) [function] !sas-add The function !sas-add provides differentiable scalar-and-scalar operation. Calling a node SCALARANDSCALARADD , the function performs following operation: x c o p y \u2190 x + y x_{copy}\\gets{x + y} x co p y \u200b \u2190 x + y Inputs x y could be one of: ScalarTensor or number [function] !sas-sub The function !sas-sub provides differentiable scalar-and-scalar operation. Calling a node SCALARANDSCALARSUB , the function performs following operation: x c o p y \u2190 x \u2212 y x_{copy}\\gets{x - y} x co p y \u200b \u2190 x \u2212 y Inputs x y could be one of: ScalarTensor or number [function] !sas-mul The function !sas-mul provides differentiable scalar-and-scalar operation. Calling a node SCALARANDSCALARMUL , the function performs following operation: x c o p y \u2190 x \u2217 y x_{copy}\\gets{x * y} x co p y \u200b \u2190 x \u2217 y Inputs x y could be one of: ScalarTensor or number [function] !sas-div The function !sas-div provides differentiable scalar-and-scalar operation. Calling a node SCALARANDSCALARDIV , the function performs following operation: x c o p y \u2190 x / y x_{copy}\\gets{x / y} x co p y \u200b \u2190 x / y Inputs x y could be one of: ScalarTensor or number [function] !add (!add x y) The function provides general-purpose arithmetic operation. Given type of tensors, this function dispatches these functions automatically: !sas-add !scalar-add !matrix-add Inputs x y could be one of AbstractTensor number ScalarTensor SideEffects None [function] !sub (!sub x y) The function provides general-purpose arithmetic operation. Given type of tensors, this function dispatches these functions automatically: !sas-sub !scalar-sub !matrix-sub Inputs x y could be one of AbstractTensor number ScalarTensor SideEffects None [function] !mul (!mul x y) The function provides general-purpose arithmetic operation. Given type of tensors, this function dispatches these functions automatically: !sas-mul !scalar-mul !matrix-mul Inputs x y could be one of AbstractTensor number ScalarTensor SideEffects None [function] !div (!div x y) The function provides general-purpose arithmetic operation. Given type of tensors, this function dispatches these functions automatically: !sas-div !scalar-div !matrix-div Inputs x y could be one of AbstractTensor number ScalarTensor SideEffects None [function] !+ Is the equivalent to just doing (reduce #'!ADD numbers) Example (#'!ADD 1 2 3 4 5) [function] !- Is the equivalent to just doing (reduce #'!SUB numbers) Example (#'!SUB 1 2 3 4 5) [function] !* Is the equivalent to just doing (reduce #'!MUL numbers) Example (#'!MUL 1 2 3 4 5) [function] !/ Is the equivalent to just doing (reduce #'!DIV numbers) Example (#'!DIV 1 2 3 4 5) [function] !move (!move place tensor) A \u2190 B A\\gets{B} A \u2190 B The function !move returns a node which moves tensor's visible elements into place's visible elements. nodes one of: MoveTensorNode ScalarTensorNode Inputs place[AbstractTensor] tensor to be overwritten. tensor[AbstractTensor] tensor to be referred. force[boolean] If t, the pruning of operation by cl-waffe2 will never done. Output Unevaluated Copied Tensor. [function] !copy (!copy tensor) The function !copy returns a node which makes a copy the tensor's visible area. Note that: the function !copy never creates a new tensor larger than (tensor-vec tensor) has, (i.e.: copying broadcasted tensor will return broadcasted and copied tensor). !copy is used to make a cache before calling destructive operation to avoid side effects, therefore if the copy is included to be useless by compiler, this operations is being ignored without changing its behaviour. And this is why !copy returns InputTensor , not AbstractTensor . Input: Tensor[AbstractTensor] Output: Tensor[AbstractTensor] [function] !permute In cl-waffe2, each tensor has a slot (tensor-permute-order tensor) , which indicates the order of the dimensions to be invoked. The function !permute returns a view of the original tensor input with its dimensions permuted. (n) (n-1) ... (1) (0) ... The order ++++ ^ (0) ++++ | ++++ | | ----> (1) (A beautiful figure would be displayed in the future :<) In other view, !permute replaces the order of following operation: A = 2x2x2 Matrix. ------------------------ Shape : 2 2 2 Stride : 4 2 1 [Permution]: 2 1 0 A[1][1][1] ------------------------ When [Permution] is shuffled, the order of other parameters (e.g.: shape stride view ...) are shuffle in tandem. That is, if we give 2 0 1 as a permutation, the figure becomes: A = 2x2x2 Matrix. ------------------------ Shape : 2 2 2 Stride : 4 1 2 [Permution]: 2 0 1 A[1][1][1] ------------------------ The operation could be applied to transpose matrices. Example (defun transpose-revisit (tensor) ;; A[i j] -> A[j i] (!permute tensor :~ 0 1)) Note that the case when only the last two aces are subject to be swapped, we return Lazy-Transpsose-Node instead (for matmul). Inputs tensor[AbstractTensor] tensor to be permuted. order[list<Fixnum>] An list of permutation. Note that :~ could be used once in an order If needed. If the order and the number of dimensions of the entered tensor do not match, the part is automatically stored as long as :~ is provided. Tips: If the first element of order arguments is a function, the rest arguments of order is overwritten with its result. that is, order become the value of (funcall (car order) (tensor-permute-order tensor)) and can be used like: (!permute tensor (compose #'reverse #'tensor-permute-order)) to reverse all permution for example. Tips: (!permute tensor (torch-order 2 1 0)) to use the same notation to pytorch. [function] !reshape (!reshape tensor &rest shapes) Changes the shape of given tensor. Before and after the operation, the total elements of tensors must correspond. Inputs tensor AbstractTensor but must not includes symbol in the shape. shapes could be one of: fixnum t . t can be used at one, but the value of t is automatically inferenced. Note: If the first element of shapes is a function, shapes are overwritten with the function's value. (!reshape (ax+b `(5 3 2) 1 0) (compose #'reverse #'shape)) ;; => (2 3 5) Tensor [function] !view (!view tensor &rest subscripts) The function !view returns a tensor which is applied lazy-evaluated view. For Example, let A be a 4x8 Matrix, and we gonna create a view of A that portrays A[:, 2] . (!view A 2 t) A B 0 ++++++++ -------- 1 ++++++++ -------- 2 ++++++++ -> [make a view] -> ++++++++ 3 ++++++++ -------- Here, A and B shares the pointer. Calling (shape B) returns (1 8) . Subscripts Subscripts are following: t all elements in the axis. fixnum points out the specified index. (start end) slices the area. (start end step-by) slices the area by step-by . step-by can be a negative-fixnum. (Not tested) (:broadcast N-times) broadcasts the axis for N-times, the axis to be broadcasted must be 1 or broadcasted-axis. (:tflist ...) (TODO) (:indices ...) (TODO) Return (values sliced-tensor broadcast-reverser) Tips: Applying !view again to the returned sliced-tensor with broadcast-reverser will remove broadcasts from the tensor. Tips: If a function is passed as the first element of subscript , the subscript is overwritten based on the return value of the function. The function is called like: (funcall function tensor) can be used like: (!view tensor (compose #'reverse #'tensor-view)) . [function] !flatten (!flatten tensor) equivalent to the (!reshape tensor t) [function] !rankup (!rankup tensor ntimes &key (at 0)) The function !rankup appends/reduces 1 at at into the given tensor's shape for ntimes. If ntimes > 0, appends 1 If ntimes < 0, reduces 1, if the axis=1, otherwise returns error. Examples CL-WAFFE2-REPL> (!rankup (randn `(3 3)) 3 :at 1) {CPUTENSOR[float] :shape (3 1 1 1 3) :named ChainTMP1459457 :vec-state [maybe-not-computed] <<Not-Embodied (3 1 1 1 3) Tensor>> :facet :input :requires-grad NIL :backward <Node: RESHAPETENSORNODE-T (A[BEFORE] B[AFTER] -> B[AFTER])>} CL-WAFFE2-REPL> (!rankup * -3 :at 1) {CPUTENSOR[float] :shape (3 3) :named ChainTMP1459467 :vec-state [maybe-not-computed] <<Not-Embodied (3 3) Tensor>> :facet :input :requires-grad NIL :backward <Node: RESHAPETENSORNODE-T (A[BEFORE] B[AFTER] -> B[AFTER])>} CL-WAFFE2-REPL> [function] ->scal (->scal matrix-tensor) The function ->scal receives matrix-tensor with total-size = 1, returning a ScalarTensor. [function] ->mat (->mat scalar-tensor &key (dims 1)) The function ->mat receives ScalarTensor , returning a matrix with the number of axis=dims. [function] ->contiguous Returns a copy of the given tensor if is is permuted. Otherwise returns the argumement as it is. A memory-layout of returned copies are arranged into the same array as the array seen on the REPL. Example (!t (ax+b `(3 3) 1 0)) {CPUTENSOR[float] :shape (3 3) -> :view (<T> <T>) -> :visible-shape (3 3) :named ChainTMP110110 :vec-state [maybe-not-computed] ((0.0 3.0 6.0) (1.0 4.0 7.0) (2.0 5.0 8.0)) :facet :input :requires-grad NIL :backward <Node: LAZYTRANSPOSENODE-T (A[~ I J] -> A[~ I J])>} (tensor-vec *) #(0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0) ;; calling ->contiguous... (->contiguous (!t (ax+b `(3 3) 1 0))) {CPUTENSOR[float] :shape (3 3) :named ChainTMP110149 :vec-state [maybe-not-computed] <<Not-Embodied (3 3) Tensor>> :facet :input :requires-grad NIL :backward <Node: MOVETENSORNODE-CPUTENSOR (A[~] B[~] -> A[~])>} (tensor-vec (proceed *)) #(0.0 3.0 6.0 1.0 4.0 7.0 2.0 5.0 8.0) [function] proceed (proceed tensor &key (measure-time nil)) The function proceed invokes special node, ProceedNode , which takes all the previous computation node before tensor, returning the result of it. The backward is created with the previous node. This function will be useful especially when debugging on REPL. Inputs If measure-time =t, ProceedNode wraps with time macro when calling COMPILED forward and backward propagation. Compiling time isn't included to the displayed time while (time (proceed tensor)) includes. compile-mode is a keyword, type of compile-mode-t . [function] proceed-time (proceed-time tensor) An alias for (proceed tensor :measure-time t) Note that: the proceed-time function invokes forward function twice times, in order for processing system to trace compiled lisp code, and ignoring allocation time. [function] proceed-backward (proceed-backward tensor) The function proceed-backward calls forward and backwrd of the tensor. Output T (which indicates backward is succeed) [function] proceed-bench (proceed-bench tensor &key (compile-mode :default) (n-sample 1) (ignore-first-call nil) (stream t) (top-k 10) (backward nil) (fuse-p t)) Invokes cl-waffe2 VM with benchmarking the forward and (if specified) backward. Input backward[boolean] Set t in order to profile backward. Example CL-WAFFE2-REPL> (proceed-bench (!sum (randn `(3 3)))) Time(s) | Instruction ( * - Beyonds the average execution time) 2.3e-4* | <WfInst[Compiled: SCALARMUL-CPUTENSOR] : TID1389503 <= op(TID1389503(1 1) <Input>TID1389505(1))> 2.0e-6 | <WfInst[Compiled: VIEWTENSORNODE-T] : TID1389514 <= op(TID1389514(3 3) TID1389503(1 1))> 7.0e-6 | <WfInst[Compiled: ADDNODE-CPUTENSOR] : TID1389514 <= op(TID1389514(3 3) <Input>TID1389488(3 3))> 1.0e-6 | <WfInst[Compiled: VIEWTENSORNODE-T] : TID1389536 <= op(TID1389536(1 1) TID1389514(3 3))> 4 Instructions | 5 Tensors Total Time: 2.4e-4 sec Instruction | Total time (s) | Time/Total (n-sample=1) <WfInst[Compiled: SCALARMUL-CPUTENSOR] | 2.3e-4 | 95.833336% <WfInst[Compiled: ADDNODE-CPUTENSOR] | 7.0e-6 | 2.916667% <WfInst[Compiled: VIEWTENSORNODE-T] | 3.0e-6 | 1.2500001% {CPUTENSOR[float] :shape (1 1) -> :view (<(BROADCAST 1)> <(BROADCAST 1)>) -> :visible-shape (1 1) :named ChainTMP1389502 ((-0.43719095)) :facet :input :requires-grad NIL :backward NIL} [macro] %transform (%transform &body transform-syntax) %transform is a macro to describe !view , !permute and broadcasting of the given tensors together in a concise manner. In short word, %transform = !view + !permute + Broadcasting . The transformation of tensor are described on the same syntax of Subscript DSL but before and after -> , there is always one tensor for each. (Example) (%transform A[i j] -> A[j i]) The variable names (e.g.: A ) are exactly the name of the variable used by the %transform macro, which must be bound in scope. It is optional to give the name to the tensor after -> . (defun transpose-revisit (tensor) (%transform tensor[~ i j] -> [j i])) Syntax Following the rules below, %transform calls appropriate functions. If ~ were used after -> , the macro is expanded into !flexible ... , or call !permute as long as all symbols appeared before -> were also used after -> . Otherwise, call !view . Adding an broadcastable axis. The broadcastable axis is the range in which 1 of the shape of tensors can be added if needed, and at most one exists in one matrix. If the subscripts of the tensor after -> includes ~ , the corresponding position of the shape becomes broadcastable . For example: (%transform A[i j] -> A[~ i j]) (%transform A[~ i j] -> A[~ i j]) Adjustable dimensions the ~ symbol used before -> means: the number of dimensions of the corresponding part could be anything. (%transform A[~ i j] -> A[i j] Shuffling the permution of tensor If symbols used before -> are also appeared in after -> , the corresponding symbols indicate the permution of tensor. (%transform A[i j] -> [j i]) (%transform A[~ i j] -> [j i]) (%transform A[i ~ j] -> [j i]) ;; the same as (!permute a 1 :~ 0) Make a view of tensors. Set symbols (which aren't used before -> ) or fixnum to make a index. (start end) also creates a slice. Setting characters like *10 *a broadcasts the axis. [function] !flexible (!flexible tensor) The function !flexible inserts a broadcastable axes to the tensor at the given position at (specified like: 1 2 ... -1 -2 ...). That is: Tensor = (10 10) -> [!flexible] -> Tensor' = (1 ... 1 10 10) ^ <1 x N> Note that added axes could be broadcasted automatically when the operation called with multiple arguments. Example !flexible is a fundamental operation when using broadcasting in cl-waffe2. And usually called via %transform macro for readability. CL-WAFFE2-REPL> (!add (ax+b `(3 3) 0 0) (print (!flexible (ax+b `(3) 1 0) :at -1))) {CPUTENSOR[float] :shape (3 <1 x N>) :named ChainTMP1631118 :vec-state [maybe-not-computed] (0.0 1.0 2.0) :facet :input :requires-grad NIL :backward <Node: FLEXIBLE-RANK-NODE-T (A[~] -> A[~])>} {CPUTENSOR[float] :shape (3 3) :named ChainTMP1631165 :vec-state [maybe-not-computed] <<Not-Embodied (3 3) Tensor>> :facet :input :requires-grad NIL :backward <Node: ADDNODE-CPUTENSOR (A[~] B[~] -> A[~])>} CL-WAFFE2-REPL> (proceed *) {CPUTENSOR[float] :shape (3 3) :named ChainTMP1631189 :vec-state [computed] ((0.0 0.0 0.0) (1.0 1.0 1.0) (2.0 2.0 2.0)) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} CL-WAFFE2-REPL> (!add (ax+b `(3 3) 0 0) (print (!flexible (ax+b `(3) 1 0)))) {CPUTENSOR[float] :shape (<1 x N> 3) :named ChainTMP1631205 :vec-state [maybe-not-computed] (0.0 1.0 2.0) :facet :input :requires-grad NIL :backward <Node: FLEXIBLE-RANK-NODE-T (A[~] -> A[~])>} {CPUTENSOR[float] :shape (3 3) :named ChainTMP1631248 :vec-state [maybe-not-computed] <<Not-Embodied (3 3) Tensor>> :facet :input :requires-grad NIL :backward <Node: ADDNODE-CPUTENSOR (A[~] B[~] -> A[~])>} CL-WAFFE2-REPL> (proceed *) {CPUTENSOR[float] :shape (3 3) :named ChainTMP1631272 :vec-state [computed] ((0.0 1.0 2.0) (0.0 1.0 2.0) (0.0 1.0 2.0)) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} [function] !abs (!abs x &key (-> nil)) The function !abs takes x as an argument, applying a abs function into each element and writes the result into -> . O U T c o p y \u2190 a b s ( X ) OUT_{copy}\\gets{abs(X)} O U T co p y \u200b \u2190 ab s ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-ABSNODE ABSNODE SideEffects -> is destructed. [function] !sign (!sign x &key (-> nil)) The function !sign takes x as an argument, applying a sign function into each element and writes the result into -> . O U T c o p y \u2190 s i g n ( X ) OUT_{copy}\\gets{sign(X)} O U T co p y \u200b \u2190 s i g n ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-SIGNNODE SIGNNODE SideEffects -> is destructed. [function] !sqrt (!sqrt x &key (-> nil)) The function !sqrt takes x as an argument, applying a sqrt function into each element and writes the result into -> . O U T c o p y \u2190 s q r t ( X ) OUT_{copy}\\gets{sqrt(X)} O U T co p y \u200b \u2190 s q r t ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-SQRTNODE SQRTNODE SideEffects -> is destructed. [function] !square (!square x &key (-> nil)) The function !square takes x as an argument, applying a square function into each element and writes the result into -> . O U T c o p y \u2190 s q u a r e ( X ) OUT_{copy}\\gets{square(X)} O U T co p y \u200b \u2190 s q u a re ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-SQUARENODE SQUARENODE SideEffects -> is destructed. [function] !sin (!sin x &key (-> nil)) The function !sin takes x as an argument, applying a sin function into each element and writes the result into -> . O U T c o p y \u2190 s i n ( X ) OUT_{copy}\\gets{sin(X)} O U T co p y \u200b \u2190 s in ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-SINNODE SINNODE SideEffects -> is destructed. [function] !cos (!cos x &key (-> nil)) The function !cos takes x as an argument, applying a cos function into each element and writes the result into -> . O U T c o p y \u2190 c o s ( X ) OUT_{copy}\\gets{cos(X)} O U T co p y \u200b \u2190 cos ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-COSNODE COSNODE SideEffects -> is destructed. [function] !tan (!tan x &key (-> nil)) The function !tan takes x as an argument, applying a tan function into each element and writes the result into -> . O U T c o p y \u2190 t a n ( X ) OUT_{copy}\\gets{tan(X)} O U T co p y \u200b \u2190 t an ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-TANNODE TANNODE SideEffects -> is destructed. [function] !asin (!asin x &key (-> nil)) The function !asin takes x as an argument, applying a asin function into each element and writes the result into -> . O U T c o p y \u2190 a s i n ( X ) OUT_{copy}\\gets{asin(X)} O U T co p y \u200b \u2190 a s in ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-ASINNODE ASINNODE SideEffects -> is destructed. [function] !acos (!acos x &key (-> nil)) The function !acos takes x as an argument, applying a acos function into each element and writes the result into -> . O U T c o p y \u2190 a c o s ( X ) OUT_{copy}\\gets{acos(X)} O U T co p y \u200b \u2190 a cos ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-ACOSNODE ACOSNODE SideEffects -> is destructed. [function] !atan (!atan x &key (-> nil)) The function !atan takes x as an argument, applying a atan function into each element and writes the result into -> . O U T c o p y \u2190 a t a n ( X ) OUT_{copy}\\gets{atan(X)} O U T co p y \u200b \u2190 a t an ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-ATANNODE ATANNODE SideEffects -> is destructed. [function] !sinh (!sinh x &key (-> nil)) The function !sinh takes x as an argument, applying a sinh function into each element and writes the result into -> . O U T c o p y \u2190 s i n h ( X ) OUT_{copy}\\gets{sinh(X)} O U T co p y \u200b \u2190 s inh ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-SINHNODE SINHNODE SideEffects -> is destructed. [function] !cosh (!cosh x &key (-> nil)) The function !cosh takes x as an argument, applying a cosh function into each element and writes the result into -> . O U T c o p y \u2190 c o s h ( X ) OUT_{copy}\\gets{cosh(X)} O U T co p y \u200b \u2190 cos h ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-COSHNODE COSHNODE SideEffects -> is destructed. [function] !tanh (!tanh x &key (-> nil)) The function !tanh takes x as an argument, applying a tanh function into each element and writes the result into -> . O U T c o p y \u2190 t a n h ( X ) OUT_{copy}\\gets{tanh(X)} O U T co p y \u200b \u2190 t anh ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-TANHNODE TANHNODE SideEffects -> is destructed. [function] !asinh (!asinh x &key (-> nil)) The function !asinh takes x as an argument, applying a asinh function into each element and writes the result into -> . O U T c o p y \u2190 a s i n h ( X ) OUT_{copy}\\gets{asinh(X)} O U T co p y \u200b \u2190 a s inh ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-ASINHNODE ASINHNODE SideEffects -> is destructed. [function] !acosh (!acosh x &key (-> nil)) The function !acosh takes x as an argument, applying a acosh function into each element and writes the result into -> . O U T c o p y \u2190 a c o s h ( X ) OUT_{copy}\\gets{acosh(X)} O U T co p y \u200b \u2190 a cos h ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-ACOSHNODE ACOSHNODE SideEffects -> is destructed. [function] !atanh (!atanh x &key (-> nil)) The function !atanh takes x as an argument, applying a atanh function into each element and writes the result into -> . O U T c o p y \u2190 a t a n h ( X ) OUT_{copy}\\gets{atanh(X)} O U T co p y \u200b \u2190 a t anh ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-ATANHNODE ATANHNODE SideEffects -> is destructed. [function] !exp (!exp x &key (-> nil)) The function !exp takes x as an argument, applying a exp function into each element and writes the result into -> . O U T c o p y \u2190 e x p ( X ) OUT_{copy}\\gets{exp(X)} O U T co p y \u200b \u2190 e x p ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-EXPNODE EXPNODE SideEffects -> is destructed. [function] !log2 (!log2 x &key (-> nil)) The function !log2 takes x as an argument, applying a log2 function into each element and writes the result into -> . O U T c o p y \u2190 l o g 2 ( X ) OUT_{copy}\\gets{log2(X)} O U T co p y \u200b \u2190 l o g 2 ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-LOG2NODE LOG2NODE SideEffects -> is destructed. [function] !log10 (!log10 x &key (-> nil)) The function !log10 takes x as an argument, applying a log10 function into each element and writes the result into -> . O U T c o p y \u2190 l o g 10 ( X ) OUT_{copy}\\gets{log10(X)} O U T co p y \u200b \u2190 l o g 10 ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-LOG10NODE LOG10NODE SideEffects -> is destructed. [function] !loge (!loge x &key (-> nil)) The function !loge takes x as an argument, applying a loge function into each element and writes the result into -> . O U T c o p y \u2190 l o g e ( X ) OUT_{copy}\\gets{loge(X)} O U T co p y \u200b \u2190 l o g e ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-LOGENODE LOGENODE SideEffects -> is destructed. [function] !sum (!sum tensor &key (axis t) (-> nil) (keepdims nil)) The function !sum return a node which computes the sum of tensor along the given axis. Inputs tensor , a tensor to be reducted. axis [t or fixnum or list] the axis to be reducted. (-1, -2... is ok) -> [AbstractTensor or nil] the place to set the result. If nil, creates a new tensor. dims [boolean] If t, the axis reducted is broadcasted. Return: -> [AbstractTensor] the result. [function] !mean (!mean tensor &key (axis t) (-> nil) (keepdims nil)) The function !mean return a node which computes the average of tensor along the given axis. Inputs tensor , a tensor to be reducted. axis [t or fixnum or list] the axis to be reducted. (-1, -2... is ok) -> [AbstractTensor or nil] the place to set the result. If nil, creates a new tensor. keepdims [boolean] If t, the axis reducted is broadcasted. Return -> [AbstractTensor] the result. [function] !argmax (!argmax tensor &key (axis -1) (out nil)) The function !argmax computes the indices of maximum values of all elements below the axis dimension in the given tensor. Inputs tensor axis out Returns AbstractTensor[uint32] with dimensions behind axis is replaced with 1. [function] !argmin (!argmin tensor &key (axis -1) (out nil)) The function !argmin computes the indices of minimum values of all elements below the axis dimension in the given tensor. Inputs tensor axis out Returns AbstractTensor[uint32] with dimensions behind axis is replaced with 1. [function] !max (!max tensor &key (axis -1) (out nil)) The function !max finds largest values of all elements below the axis rank in the given tensor. Inputs tensor axis out Returns AbstractTensor with dimensions behind axis is replaced with 1. [function] !min (!min tensor &key (axis -1) (out nil)) The function !min finds the smallest values of all elements below the axis rank in the given tensor. Inputs tensor axis out Returns AbstractTensor with dimensions behind axis is replaced with 1. [function] !t (!t tensor) Transposes the last two axes of the given tensor. When called with !matmul, the operation is ignored. [function] !matmul (!matmul x y &key (out nil) (transpose-x nil) (transpose-y nil)) Computing a matrix multiplication of X and Y. The result is stored in out if specified, otherwise creates a new tensor. o u t \u2190 g e m m ( 1.0 , x , y , 0.0 , o u t ) out\\gets{gemm(1.0, x, y, 0.0, out)} o u t \u2190 g e mm ( 1.0 , x , y , 0.0 , o u t ) Inputs transpose-x, transpose-y[boolean] If t, the inputs are wrapped with (!t tensor) . Tips: Lazy-Transpose-Node If the last backward of given arguments are LazyTransposeNode (created with the function !t ), the function !matmul will transpose them without making a copy (i.e.: zero-cost transpose). In any other case (the last two dimensions' permution, or view are too complicated), !matmul will produce an additional copy for fast computing. [function] !dot (!dot x y) Finds a dot product of x and y. Unlike numpy.dot , !dot intentionally only supports computing the dot product of two 1D tensors with the same number of elements. (proceed (!dot (randn `(100)) (randn `(10 10)))) {CPUTENSOR[float] :shape (1) -> :view (<0>) -> :visible-shape (1) :named ChainTMP115880 :vec-state [computed] (21.594929) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} [function] !where (!where tensor condition &key (true-then 1) (false-then 0) (out nil)) The function !where returns a elements selected-from true-then or false-then , depending on condition. The operation is defined as: o u t i = { true-then c o n d i t i o n ( X i ) false-then otherwise \\begin{equation} out_i= \\begin{cases} \\text{true-then} & condition(X_i) \\\\ \\text{false-then} & \\text{otherwise} \\end{cases} \\end{equation} o u t i \u200b = { true-then false-then \u200b co n d i t i o n ( X i \u200b ) otherwise \u200b \u200b \u200b (where X = tensor) Inputs out place to set the result condition an funcallable function. (e.g.: #'evenp #'oddp etc...) [function] !where (!compare tensor1 tensor2 condition &key (true-then 1) (false-then 0) (out nil)) The function !compare returns a elements selected-from true-then or false-then , depending on condition. The operation is defined as: o u t i = { true-then c o n d i t i o n ( X i , Y i ) false-then otherwise \\begin{equation} out_i= \\begin{cases} \\text{true-then} & condition(X_i, Y_i) \\\\ \\text{false-then} & \\text{otherwise} \\end{cases} \\end{equation} o u t i \u200b = { true-then false-then \u200b co n d i t i o n ( X i \u200b , Y i \u200b ) otherwise \u200b \u200b \u200b (where X = tensor1, Y=tensor2) Inputs out place to set the result condition an funcallable function. (e.g.: #'> #'< etc...) [function] a>scal (a>scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a>scal sets true-then if the equation: element > scal is t, otherwise set false-then at the corresponding positions. Inputs A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as a scal argument) [function] a<scal (a<scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a<scal sets true-then if the equation: element < scal is t, otherwise set false-then at the corresponding positions. Inputs A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as a scal argument) [function] a>=scal (a>=scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a>=scal sets true-then if the equation: element >= scal is t, otherwise set false-then at the corresponding positions. Inputs A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as a scal argument) [function] a<=scal (a<=scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a<=scal sets true-then if the equation: element <= scal is t, otherwise set false-then at the corresponding positions. Inputs A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as a scal argument) [function] a=scal (a=scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a=scal sets true-then if the equation: element = scal is t, otherwise set false-then at the corresponding positions. Inputs A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as a scal argument) [function] a>b (a>b A B &key (out nil) (true-then 1) (false-then 0)) The function a>b sets true-then if the equation: A > B is t, otherwise set false-then at the corresponding positions. Inputs A B AbstractTensor to be compared. [function] a<b (a<b A B &key (out nil) (true-then 1) (false-then 0)) The function a<b sets true-then if the equation: A < B is t, otherwise set false-then at the corresponding positions. Inputs A B AbstractTensor to be compared. [function] a>=b (a>=b A B &key (out nil) (true-then 1) (false-then 0)) The function a>=b sets true-then if the equation: A >= B is t, otherwise set false-then at the corresponding positions. Inputs A B AbstractTensor to be compared. [function] a<=b (a<=b A B &key (out nil) (true-then 1) (false-then 0)) The function a<=b sets true-then if the equation: A <= B is t, otherwise set false-then at the corresponding positions. Inputs A B AbstractTensor to be compared. [function] a=b (a=b A B &key (out nil) (true-then 1) (false-then 0)) The function a=b sets true-then if the equation: A = B is t, otherwise set false-then at the corresponding positions. Inputs A B AbstractTensor to be compared. [function] padding (padding tensor pad-width &key (pad-maker #'ax+b) (initargs `(0 0))) Creating a new InputTensor with shape after padding, the function padding moves the given tensor into a new area. Implementation (padding (ax+b `(1 3) 0 1) `((1 1) (1 1))) [Corresponds with...] 00000 +++ -> [padding] -> 0+++0 00000 The operation is performed in the following steps: First, creates a new tensor with the shape of after padded which is initialized via the pad-maker function, where pad-maker is an initializer function, that is, functions defined by define-initializer-function or exported from the :cl-waffe2/distribution package. +++++ +++++ +++++ The function padding uses the form below to initialize tensors. (apply pad-maker initargs) In default, (apply #'ax+b (0 0))`. Second, makes a view of the new tensor and match its shape to the base tensor. the argument pad-width is used to determine offsets of each axis. pad-width is the number of values to the edges of each axis. and given as: ((before_1 after_1) (before_2 after_2) ...) . 0~before_n and before_n~last are the subject to be padded. +++++ ----- +++++ -> [view] -> -+++- +++++ ----- ^ after_2 + ... visible area - ... hide area by view Finally, moves all elements in the base tensor into viewed tensor, and later reset the view. Inputs tensor[AbstractTensor] tensor to be padded. pad-width[list] the number of the edges and given as: ((before_1 after_1) (before_2 after_2) ...) . the forward is the same as np.pad . Set t instead of (before_n after_n) and ignores the corresponding position of axis. pad-maker[function] an initializer-function initargs[list] a list of arguments for pad-maker Note that: the axes to be padded, must be fixnum. not a symbol. If the shapes does not change before/after padding, returns the given tensor as it is. Example (proceed (padding (ax+b `(3 3) 0 1) `((1 1) (1 1)))) {CPUTENSOR[float] :shape (5 5) -> :view (<T> <T>) -> :visible-shape (5 5) :named ChainTMP1104579 :vec-state [computed] ((0.0 0.0 0.0 0.0 0.0) (0.0 1.0 1.0 1.0 0.0) ... (0.0 1.0 1.0 1.0 0.0) (0.0 0.0 0.0 0.0 0.0)) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} [function] broadcast-to Returns the subscript of the !view that is broadcasting to be the same shape as the object-tensor . For example: ;; x ... ( 3 3 ) Tensor ;; (!sum x :axis 1) ... ( 3 1 ) Tensor ;; broadcast-to will return: (t `(:broadcast 3)) (!mul x (!view (!sum x :axis 1) (broadcast-to x)))","title":"[Functions] cl-waffe2/base-impl"},{"location":"base-impl/#basic-apis","text":"","title":"Basic APIs"},{"location":"base-impl/#function-matrix-add","text":"(!matrix-add x y) The function !matrix-add calls ADDNODE and adds X and Y element-wise, returning a new tensor. X c o p y \u2190 X + Y X_{copy}\\gets{X + Y} X co p y \u200b \u2190 X + Y","title":"[function] !matrix-add"},{"location":"base-impl/#inputs","text":"X and Y must be a AbstractTensor (not a ScalarTensor), with the same shape.","title":"Inputs"},{"location":"base-impl/#sideeffects","text":"None.","title":"SideEffects"},{"location":"base-impl/#function-matrix-sub","text":"(!matrix-sub x y) The function !matrix-sub calls SUBNODE and substracts X by Y element-wise, returning a new tensor. X c o p y \u2190 X \u2212 Y X_{copy}\\gets{X - Y} X co p y \u200b \u2190 X \u2212 Y","title":"[function] !matrix-sub"},{"location":"base-impl/#inputs_1","text":"X and Y must be a AbstractTensor (not a ScalarTensor), with the same shape.","title":"Inputs"},{"location":"base-impl/#sideeffects_1","text":"None.","title":"SideEffects"},{"location":"base-impl/#function-matrix-mul","text":"(!matrix-mul x y) The function !matrix-mul calls MULNODE and multiplies X and Y element-wise, returning a new tensor. X c o p y \u2190 X \u2217 Y X_{copy}\\gets{X * Y} X co p y \u200b \u2190 X \u2217 Y","title":"[function] !matrix-mul"},{"location":"base-impl/#inputs_2","text":"X and Y must be a AbstractTensor (not a ScalarTensor), with the same shape.","title":"Inputs"},{"location":"base-impl/#sideeffects_2","text":"None.","title":"SideEffects"},{"location":"base-impl/#function-matrix-div","text":"(!matrix-div x y) The function !matrix-div calls DIVNODE and divides X by Y element-wise, returning a new tensor. X c o p y \u2190 X / Y X_{copy}\\gets{X / Y} X co p y \u200b \u2190 X / Y","title":"[function] !matrix-div"},{"location":"base-impl/#inputs_3","text":"X and Y must be a AbstractTensor (not a ScalarTensor), with the same shape.","title":"Inputs"},{"location":"base-impl/#sideeffects_3","text":"None.","title":"SideEffects"},{"location":"base-impl/#function-inverse","text":"(!inverse tensor) The function !inverse calls InverseTensorNode , and finds the inverse of the received Tensor/Scalar, returning a new tensor. X c o p y \u2190 1 / X X_{copy}\\gets{1 / X} X co p y \u200b \u2190 1/ X","title":"[function] !inverse"},{"location":"base-impl/#inputs_4","text":"tensor[ScalarTensor/AbstractTensor/Number]","title":"Inputs"},{"location":"base-impl/#function-scalar-add","text":"(!scalar-add scalar x) The function !SCALAR-ADD computes following operation with calling SCALARADD , returning a new tensor. X c o p y \u2190 X + s c a l a r X_{copy}\\gets{X + scalar} X co p y \u200b \u2190 X + sc a l a r","title":"[function] !scalar-add"},{"location":"base-impl/#inputs_5","text":"scalar could be one of ScalarTensor or number tensor AbstractTensor (should not be a scalar)","title":"Inputs"},{"location":"base-impl/#function-scalar-sub","text":"(!scalar-sub scalar x) The function !SCALAR-SUB computes following operation with calling SCALARSUB , returning a new tensor. X c o p y \u2190 X \u2212 s c a l a r X_{copy}\\gets{X - scalar} X co p y \u200b \u2190 X \u2212 sc a l a r","title":"[function] !scalar-sub"},{"location":"base-impl/#inputs_6","text":"scalar could be one of ScalarTensor or number tensor AbstractTensor (should not be a scalar)","title":"Inputs"},{"location":"base-impl/#function-scalar-mul","text":"(!scalar-mul scalar x) The function !SCALAR-MUL computes following operation with calling SCALARMUL , returning a new tensor. X c o p y \u2190 X \u2217 s c a l a r X_{copy}\\gets{X * scalar} X co p y \u200b \u2190 X \u2217 sc a l a r","title":"[function] !scalar-mul"},{"location":"base-impl/#inputs_7","text":"scalar could be one of ScalarTensor or number tensor AbstractTensor (should not be a scalar)","title":"Inputs"},{"location":"base-impl/#function-scalar-div","text":"(!scalar-div scalar x) The function !SCALAR-DIV computes following operation with calling SCALARDIV , returning a new tensor. X c o p y \u2190 X / s c a l a r X_{copy}\\gets{X / scalar} X co p y \u200b \u2190 X / sc a l a r","title":"[function] !scalar-div"},{"location":"base-impl/#inputs_8","text":"scalar could be one of ScalarTensor or number tensor AbstractTensor (should not be a scalar)","title":"Inputs"},{"location":"base-impl/#function-sas-add","text":"The function !sas-add provides differentiable scalar-and-scalar operation. Calling a node SCALARANDSCALARADD , the function performs following operation: x c o p y \u2190 x + y x_{copy}\\gets{x + y} x co p y \u200b \u2190 x + y","title":"[function] !sas-add"},{"location":"base-impl/#inputs_9","text":"x y could be one of: ScalarTensor or number","title":"Inputs"},{"location":"base-impl/#function-sas-sub","text":"The function !sas-sub provides differentiable scalar-and-scalar operation. Calling a node SCALARANDSCALARSUB , the function performs following operation: x c o p y \u2190 x \u2212 y x_{copy}\\gets{x - y} x co p y \u200b \u2190 x \u2212 y","title":"[function] !sas-sub"},{"location":"base-impl/#inputs_10","text":"x y could be one of: ScalarTensor or number","title":"Inputs"},{"location":"base-impl/#function-sas-mul","text":"The function !sas-mul provides differentiable scalar-and-scalar operation. Calling a node SCALARANDSCALARMUL , the function performs following operation: x c o p y \u2190 x \u2217 y x_{copy}\\gets{x * y} x co p y \u200b \u2190 x \u2217 y","title":"[function] !sas-mul"},{"location":"base-impl/#inputs_11","text":"x y could be one of: ScalarTensor or number","title":"Inputs"},{"location":"base-impl/#function-sas-div","text":"The function !sas-div provides differentiable scalar-and-scalar operation. Calling a node SCALARANDSCALARDIV , the function performs following operation: x c o p y \u2190 x / y x_{copy}\\gets{x / y} x co p y \u200b \u2190 x / y","title":"[function] !sas-div"},{"location":"base-impl/#inputs_12","text":"x y could be one of: ScalarTensor or number","title":"Inputs"},{"location":"base-impl/#function-add","text":"(!add x y) The function provides general-purpose arithmetic operation. Given type of tensors, this function dispatches these functions automatically: !sas-add !scalar-add !matrix-add","title":"[function] !add"},{"location":"base-impl/#inputs_13","text":"x y could be one of AbstractTensor number ScalarTensor","title":"Inputs"},{"location":"base-impl/#sideeffects_4","text":"None","title":"SideEffects"},{"location":"base-impl/#function-sub","text":"(!sub x y) The function provides general-purpose arithmetic operation. Given type of tensors, this function dispatches these functions automatically: !sas-sub !scalar-sub !matrix-sub","title":"[function] !sub"},{"location":"base-impl/#inputs_14","text":"x y could be one of AbstractTensor number ScalarTensor","title":"Inputs"},{"location":"base-impl/#sideeffects_5","text":"None","title":"SideEffects"},{"location":"base-impl/#function-mul","text":"(!mul x y) The function provides general-purpose arithmetic operation. Given type of tensors, this function dispatches these functions automatically: !sas-mul !scalar-mul !matrix-mul","title":"[function] !mul"},{"location":"base-impl/#inputs_15","text":"x y could be one of AbstractTensor number ScalarTensor","title":"Inputs"},{"location":"base-impl/#sideeffects_6","text":"None","title":"SideEffects"},{"location":"base-impl/#function-div","text":"(!div x y) The function provides general-purpose arithmetic operation. Given type of tensors, this function dispatches these functions automatically: !sas-div !scalar-div !matrix-div","title":"[function] !div"},{"location":"base-impl/#inputs_16","text":"x y could be one of AbstractTensor number ScalarTensor","title":"Inputs"},{"location":"base-impl/#sideeffects_7","text":"None","title":"SideEffects"},{"location":"base-impl/#function","text":"Is the equivalent to just doing (reduce #'!ADD numbers)","title":"[function] !+"},{"location":"base-impl/#example","text":"(#'!ADD 1 2 3 4 5)","title":"Example"},{"location":"base-impl/#function-","text":"Is the equivalent to just doing (reduce #'!SUB numbers)","title":"[function] !-"},{"location":"base-impl/#example_1","text":"(#'!SUB 1 2 3 4 5)","title":"Example"},{"location":"base-impl/#function_1","text":"Is the equivalent to just doing (reduce #'!MUL numbers)","title":"[function] !*"},{"location":"base-impl/#example_2","text":"(#'!MUL 1 2 3 4 5)","title":"Example"},{"location":"base-impl/#function_2","text":"Is the equivalent to just doing (reduce #'!DIV numbers)","title":"[function] !/"},{"location":"base-impl/#example_3","text":"(#'!DIV 1 2 3 4 5)","title":"Example"},{"location":"base-impl/#function-move","text":"(!move place tensor) A \u2190 B A\\gets{B} A \u2190 B The function !move returns a node which moves tensor's visible elements into place's visible elements.","title":"[function] !move"},{"location":"base-impl/#nodes","text":"one of: MoveTensorNode ScalarTensorNode","title":"nodes"},{"location":"base-impl/#inputs_17","text":"place[AbstractTensor] tensor to be overwritten. tensor[AbstractTensor] tensor to be referred. force[boolean] If t, the pruning of operation by cl-waffe2 will never done.","title":"Inputs"},{"location":"base-impl/#output","text":"Unevaluated Copied Tensor.","title":"Output"},{"location":"base-impl/#function-copy","text":"(!copy tensor) The function !copy returns a node which makes a copy the tensor's visible area. Note that: the function !copy never creates a new tensor larger than (tensor-vec tensor) has, (i.e.: copying broadcasted tensor will return broadcasted and copied tensor). !copy is used to make a cache before calling destructive operation to avoid side effects, therefore if the copy is included to be useless by compiler, this operations is being ignored without changing its behaviour. And this is why !copy returns InputTensor , not AbstractTensor . Input: Tensor[AbstractTensor] Output: Tensor[AbstractTensor]","title":"[function] !copy"},{"location":"base-impl/#function-permute","text":"In cl-waffe2, each tensor has a slot (tensor-permute-order tensor) , which indicates the order of the dimensions to be invoked. The function !permute returns a view of the original tensor input with its dimensions permuted. (n) (n-1) ... (1) (0) ... The order ++++ ^ (0) ++++ | ++++ | | ----> (1) (A beautiful figure would be displayed in the future :<) In other view, !permute replaces the order of following operation: A = 2x2x2 Matrix. ------------------------ Shape : 2 2 2 Stride : 4 2 1 [Permution]: 2 1 0 A[1][1][1] ------------------------ When [Permution] is shuffled, the order of other parameters (e.g.: shape stride view ...) are shuffle in tandem. That is, if we give 2 0 1 as a permutation, the figure becomes: A = 2x2x2 Matrix. ------------------------ Shape : 2 2 2 Stride : 4 1 2 [Permution]: 2 0 1 A[1][1][1] ------------------------ The operation could be applied to transpose matrices.","title":"[function] !permute"},{"location":"base-impl/#example_4","text":"(defun transpose-revisit (tensor) ;; A[i j] -> A[j i] (!permute tensor :~ 0 1)) Note that the case when only the last two aces are subject to be swapped, we return Lazy-Transpsose-Node instead (for matmul).","title":"Example"},{"location":"base-impl/#inputs_18","text":"tensor[AbstractTensor] tensor to be permuted. order[list<Fixnum>] An list of permutation. Note that :~ could be used once in an order If needed. If the order and the number of dimensions of the entered tensor do not match, the part is automatically stored as long as :~ is provided. Tips: If the first element of order arguments is a function, the rest arguments of order is overwritten with its result. that is, order become the value of (funcall (car order) (tensor-permute-order tensor)) and can be used like: (!permute tensor (compose #'reverse #'tensor-permute-order)) to reverse all permution for example. Tips: (!permute tensor (torch-order 2 1 0)) to use the same notation to pytorch.","title":"Inputs"},{"location":"base-impl/#function-reshape","text":"(!reshape tensor &rest shapes) Changes the shape of given tensor. Before and after the operation, the total elements of tensors must correspond.","title":"[function] !reshape"},{"location":"base-impl/#inputs_19","text":"tensor AbstractTensor but must not includes symbol in the shape. shapes could be one of: fixnum t . t can be used at one, but the value of t is automatically inferenced. Note: If the first element of shapes is a function, shapes are overwritten with the function's value. (!reshape (ax+b `(5 3 2) 1 0) (compose #'reverse #'shape)) ;; => (2 3 5) Tensor","title":"Inputs"},{"location":"base-impl/#function-view","text":"(!view tensor &rest subscripts) The function !view returns a tensor which is applied lazy-evaluated view. For Example, let A be a 4x8 Matrix, and we gonna create a view of A that portrays A[:, 2] . (!view A 2 t) A B 0 ++++++++ -------- 1 ++++++++ -------- 2 ++++++++ -> [make a view] -> ++++++++ 3 ++++++++ -------- Here, A and B shares the pointer. Calling (shape B) returns (1 8) .","title":"[function] !view"},{"location":"base-impl/#subscripts","text":"Subscripts are following: t all elements in the axis. fixnum points out the specified index. (start end) slices the area. (start end step-by) slices the area by step-by . step-by can be a negative-fixnum. (Not tested) (:broadcast N-times) broadcasts the axis for N-times, the axis to be broadcasted must be 1 or broadcasted-axis. (:tflist ...) (TODO) (:indices ...) (TODO)","title":"Subscripts"},{"location":"base-impl/#return","text":"(values sliced-tensor broadcast-reverser) Tips: Applying !view again to the returned sliced-tensor with broadcast-reverser will remove broadcasts from the tensor. Tips: If a function is passed as the first element of subscript , the subscript is overwritten based on the return value of the function. The function is called like: (funcall function tensor) can be used like: (!view tensor (compose #'reverse #'tensor-view)) .","title":"Return"},{"location":"base-impl/#function-flatten","text":"(!flatten tensor) equivalent to the (!reshape tensor t)","title":"[function] !flatten"},{"location":"base-impl/#function-rankup","text":"(!rankup tensor ntimes &key (at 0)) The function !rankup appends/reduces 1 at at into the given tensor's shape for ntimes. If ntimes > 0, appends 1 If ntimes < 0, reduces 1, if the axis=1, otherwise returns error.","title":"[function] !rankup"},{"location":"base-impl/#examples","text":"CL-WAFFE2-REPL> (!rankup (randn `(3 3)) 3 :at 1) {CPUTENSOR[float] :shape (3 1 1 1 3) :named ChainTMP1459457 :vec-state [maybe-not-computed] <<Not-Embodied (3 1 1 1 3) Tensor>> :facet :input :requires-grad NIL :backward <Node: RESHAPETENSORNODE-T (A[BEFORE] B[AFTER] -> B[AFTER])>} CL-WAFFE2-REPL> (!rankup * -3 :at 1) {CPUTENSOR[float] :shape (3 3) :named ChainTMP1459467 :vec-state [maybe-not-computed] <<Not-Embodied (3 3) Tensor>> :facet :input :requires-grad NIL :backward <Node: RESHAPETENSORNODE-T (A[BEFORE] B[AFTER] -> B[AFTER])>} CL-WAFFE2-REPL>","title":"Examples"},{"location":"base-impl/#function-scal","text":"(->scal matrix-tensor) The function ->scal receives matrix-tensor with total-size = 1, returning a ScalarTensor.","title":"[function] -&gt;scal"},{"location":"base-impl/#function-mat","text":"(->mat scalar-tensor &key (dims 1)) The function ->mat receives ScalarTensor , returning a matrix with the number of axis=dims.","title":"[function] -&gt;mat"},{"location":"base-impl/#function-contiguous","text":"Returns a copy of the given tensor if is is permuted. Otherwise returns the argumement as it is. A memory-layout of returned copies are arranged into the same array as the array seen on the REPL.","title":"[function] -&gt;contiguous"},{"location":"base-impl/#example_5","text":"(!t (ax+b `(3 3) 1 0)) {CPUTENSOR[float] :shape (3 3) -> :view (<T> <T>) -> :visible-shape (3 3) :named ChainTMP110110 :vec-state [maybe-not-computed] ((0.0 3.0 6.0) (1.0 4.0 7.0) (2.0 5.0 8.0)) :facet :input :requires-grad NIL :backward <Node: LAZYTRANSPOSENODE-T (A[~ I J] -> A[~ I J])>} (tensor-vec *) #(0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0) ;; calling ->contiguous... (->contiguous (!t (ax+b `(3 3) 1 0))) {CPUTENSOR[float] :shape (3 3) :named ChainTMP110149 :vec-state [maybe-not-computed] <<Not-Embodied (3 3) Tensor>> :facet :input :requires-grad NIL :backward <Node: MOVETENSORNODE-CPUTENSOR (A[~] B[~] -> A[~])>} (tensor-vec (proceed *)) #(0.0 3.0 6.0 1.0 4.0 7.0 2.0 5.0 8.0)","title":"Example"},{"location":"base-impl/#function-proceed","text":"(proceed tensor &key (measure-time nil)) The function proceed invokes special node, ProceedNode , which takes all the previous computation node before tensor, returning the result of it. The backward is created with the previous node. This function will be useful especially when debugging on REPL.","title":"[function] proceed"},{"location":"base-impl/#inputs_20","text":"If measure-time =t, ProceedNode wraps with time macro when calling COMPILED forward and backward propagation. Compiling time isn't included to the displayed time while (time (proceed tensor)) includes. compile-mode is a keyword, type of compile-mode-t .","title":"Inputs"},{"location":"base-impl/#function-proceed-time","text":"(proceed-time tensor) An alias for (proceed tensor :measure-time t) Note that: the proceed-time function invokes forward function twice times, in order for processing system to trace compiled lisp code, and ignoring allocation time.","title":"[function] proceed-time"},{"location":"base-impl/#function-proceed-backward","text":"(proceed-backward tensor) The function proceed-backward calls forward and backwrd of the tensor.","title":"[function] proceed-backward"},{"location":"base-impl/#output_1","text":"T (which indicates backward is succeed)","title":"Output"},{"location":"base-impl/#function-proceed-bench","text":"(proceed-bench tensor &key (compile-mode :default) (n-sample 1) (ignore-first-call nil) (stream t) (top-k 10) (backward nil) (fuse-p t)) Invokes cl-waffe2 VM with benchmarking the forward and (if specified) backward.","title":"[function] proceed-bench"},{"location":"base-impl/#input","text":"backward[boolean] Set t in order to profile backward.","title":"Input"},{"location":"base-impl/#example_6","text":"CL-WAFFE2-REPL> (proceed-bench (!sum (randn `(3 3)))) Time(s) | Instruction ( * - Beyonds the average execution time) 2.3e-4* | <WfInst[Compiled: SCALARMUL-CPUTENSOR] : TID1389503 <= op(TID1389503(1 1) <Input>TID1389505(1))> 2.0e-6 | <WfInst[Compiled: VIEWTENSORNODE-T] : TID1389514 <= op(TID1389514(3 3) TID1389503(1 1))> 7.0e-6 | <WfInst[Compiled: ADDNODE-CPUTENSOR] : TID1389514 <= op(TID1389514(3 3) <Input>TID1389488(3 3))> 1.0e-6 | <WfInst[Compiled: VIEWTENSORNODE-T] : TID1389536 <= op(TID1389536(1 1) TID1389514(3 3))> 4 Instructions | 5 Tensors Total Time: 2.4e-4 sec Instruction | Total time (s) | Time/Total (n-sample=1) <WfInst[Compiled: SCALARMUL-CPUTENSOR] | 2.3e-4 | 95.833336% <WfInst[Compiled: ADDNODE-CPUTENSOR] | 7.0e-6 | 2.916667% <WfInst[Compiled: VIEWTENSORNODE-T] | 3.0e-6 | 1.2500001% {CPUTENSOR[float] :shape (1 1) -> :view (<(BROADCAST 1)> <(BROADCAST 1)>) -> :visible-shape (1 1) :named ChainTMP1389502 ((-0.43719095)) :facet :input :requires-grad NIL :backward NIL}","title":"Example"},{"location":"base-impl/#macro-transform","text":"(%transform &body transform-syntax) %transform is a macro to describe !view , !permute and broadcasting of the given tensors together in a concise manner. In short word, %transform = !view + !permute + Broadcasting . The transformation of tensor are described on the same syntax of Subscript DSL but before and after -> , there is always one tensor for each. (Example) (%transform A[i j] -> A[j i]) The variable names (e.g.: A ) are exactly the name of the variable used by the %transform macro, which must be bound in scope. It is optional to give the name to the tensor after -> . (defun transpose-revisit (tensor) (%transform tensor[~ i j] -> [j i]))","title":"[macro] %transform"},{"location":"base-impl/#syntax","text":"Following the rules below, %transform calls appropriate functions. If ~ were used after -> , the macro is expanded into !flexible ... , or call !permute as long as all symbols appeared before -> were also used after -> . Otherwise, call !view .","title":"Syntax"},{"location":"base-impl/#adding-an-broadcastable-axis","text":"The broadcastable axis is the range in which 1 of the shape of tensors can be added if needed, and at most one exists in one matrix. If the subscripts of the tensor after -> includes ~ , the corresponding position of the shape becomes broadcastable . For example: (%transform A[i j] -> A[~ i j]) (%transform A[~ i j] -> A[~ i j])","title":"Adding an broadcastable axis."},{"location":"base-impl/#adjustable-dimensions","text":"the ~ symbol used before -> means: the number of dimensions of the corresponding part could be anything. (%transform A[~ i j] -> A[i j]","title":"Adjustable dimensions"},{"location":"base-impl/#shuffling-the-permution-of-tensor","text":"If symbols used before -> are also appeared in after -> , the corresponding symbols indicate the permution of tensor. (%transform A[i j] -> [j i]) (%transform A[~ i j] -> [j i]) (%transform A[i ~ j] -> [j i]) ;; the same as (!permute a 1 :~ 0)","title":"Shuffling the permution of tensor"},{"location":"base-impl/#make-a-view-of-tensors","text":"Set symbols (which aren't used before -> ) or fixnum to make a index. (start end) also creates a slice. Setting characters like *10 *a broadcasts the axis.","title":"Make a view of tensors."},{"location":"base-impl/#function-flexible","text":"(!flexible tensor) The function !flexible inserts a broadcastable axes to the tensor at the given position at (specified like: 1 2 ... -1 -2 ...). That is: Tensor = (10 10) -> [!flexible] -> Tensor' = (1 ... 1 10 10) ^ <1 x N> Note that added axes could be broadcasted automatically when the operation called with multiple arguments.","title":"[function] !flexible"},{"location":"base-impl/#example_7","text":"!flexible is a fundamental operation when using broadcasting in cl-waffe2. And usually called via %transform macro for readability. CL-WAFFE2-REPL> (!add (ax+b `(3 3) 0 0) (print (!flexible (ax+b `(3) 1 0) :at -1))) {CPUTENSOR[float] :shape (3 <1 x N>) :named ChainTMP1631118 :vec-state [maybe-not-computed] (0.0 1.0 2.0) :facet :input :requires-grad NIL :backward <Node: FLEXIBLE-RANK-NODE-T (A[~] -> A[~])>} {CPUTENSOR[float] :shape (3 3) :named ChainTMP1631165 :vec-state [maybe-not-computed] <<Not-Embodied (3 3) Tensor>> :facet :input :requires-grad NIL :backward <Node: ADDNODE-CPUTENSOR (A[~] B[~] -> A[~])>} CL-WAFFE2-REPL> (proceed *) {CPUTENSOR[float] :shape (3 3) :named ChainTMP1631189 :vec-state [computed] ((0.0 0.0 0.0) (1.0 1.0 1.0) (2.0 2.0 2.0)) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} CL-WAFFE2-REPL> (!add (ax+b `(3 3) 0 0) (print (!flexible (ax+b `(3) 1 0)))) {CPUTENSOR[float] :shape (<1 x N> 3) :named ChainTMP1631205 :vec-state [maybe-not-computed] (0.0 1.0 2.0) :facet :input :requires-grad NIL :backward <Node: FLEXIBLE-RANK-NODE-T (A[~] -> A[~])>} {CPUTENSOR[float] :shape (3 3) :named ChainTMP1631248 :vec-state [maybe-not-computed] <<Not-Embodied (3 3) Tensor>> :facet :input :requires-grad NIL :backward <Node: ADDNODE-CPUTENSOR (A[~] B[~] -> A[~])>} CL-WAFFE2-REPL> (proceed *) {CPUTENSOR[float] :shape (3 3) :named ChainTMP1631272 :vec-state [computed] ((0.0 1.0 2.0) (0.0 1.0 2.0) (0.0 1.0 2.0)) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>}","title":"Example"},{"location":"base-impl/#function-abs","text":"(!abs x &key (-> nil)) The function !abs takes x as an argument, applying a abs function into each element and writes the result into -> . O U T c o p y \u2190 a b s ( X ) OUT_{copy}\\gets{abs(X)} O U T co p y \u200b \u2190 ab s ( X ) (where OUT = -> )","title":"[function] !abs"},{"location":"base-impl/#inputs_21","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns","text":"->","title":"Returns"},{"location":"base-impl/#nodes_1","text":"SCALAR-ABSNODE ABSNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_8","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-sign","text":"(!sign x &key (-> nil)) The function !sign takes x as an argument, applying a sign function into each element and writes the result into -> . O U T c o p y \u2190 s i g n ( X ) OUT_{copy}\\gets{sign(X)} O U T co p y \u200b \u2190 s i g n ( X ) (where OUT = -> )","title":"[function] !sign"},{"location":"base-impl/#inputs_22","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_1","text":"->","title":"Returns"},{"location":"base-impl/#nodes_2","text":"SCALAR-SIGNNODE SIGNNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_9","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-sqrt","text":"(!sqrt x &key (-> nil)) The function !sqrt takes x as an argument, applying a sqrt function into each element and writes the result into -> . O U T c o p y \u2190 s q r t ( X ) OUT_{copy}\\gets{sqrt(X)} O U T co p y \u200b \u2190 s q r t ( X ) (where OUT = -> )","title":"[function] !sqrt"},{"location":"base-impl/#inputs_23","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_2","text":"->","title":"Returns"},{"location":"base-impl/#nodes_3","text":"SCALAR-SQRTNODE SQRTNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_10","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-square","text":"(!square x &key (-> nil)) The function !square takes x as an argument, applying a square function into each element and writes the result into -> . O U T c o p y \u2190 s q u a r e ( X ) OUT_{copy}\\gets{square(X)} O U T co p y \u200b \u2190 s q u a re ( X ) (where OUT = -> )","title":"[function] !square"},{"location":"base-impl/#inputs_24","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_3","text":"->","title":"Returns"},{"location":"base-impl/#nodes_4","text":"SCALAR-SQUARENODE SQUARENODE","title":"Nodes"},{"location":"base-impl/#sideeffects_11","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-sin","text":"(!sin x &key (-> nil)) The function !sin takes x as an argument, applying a sin function into each element and writes the result into -> . O U T c o p y \u2190 s i n ( X ) OUT_{copy}\\gets{sin(X)} O U T co p y \u200b \u2190 s in ( X ) (where OUT = -> )","title":"[function] !sin"},{"location":"base-impl/#inputs_25","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_4","text":"->","title":"Returns"},{"location":"base-impl/#nodes_5","text":"SCALAR-SINNODE SINNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_12","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-cos","text":"(!cos x &key (-> nil)) The function !cos takes x as an argument, applying a cos function into each element and writes the result into -> . O U T c o p y \u2190 c o s ( X ) OUT_{copy}\\gets{cos(X)} O U T co p y \u200b \u2190 cos ( X ) (where OUT = -> )","title":"[function] !cos"},{"location":"base-impl/#inputs_26","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_5","text":"->","title":"Returns"},{"location":"base-impl/#nodes_6","text":"SCALAR-COSNODE COSNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_13","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-tan","text":"(!tan x &key (-> nil)) The function !tan takes x as an argument, applying a tan function into each element and writes the result into -> . O U T c o p y \u2190 t a n ( X ) OUT_{copy}\\gets{tan(X)} O U T co p y \u200b \u2190 t an ( X ) (where OUT = -> )","title":"[function] !tan"},{"location":"base-impl/#inputs_27","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_6","text":"->","title":"Returns"},{"location":"base-impl/#nodes_7","text":"SCALAR-TANNODE TANNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_14","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-asin","text":"(!asin x &key (-> nil)) The function !asin takes x as an argument, applying a asin function into each element and writes the result into -> . O U T c o p y \u2190 a s i n ( X ) OUT_{copy}\\gets{asin(X)} O U T co p y \u200b \u2190 a s in ( X ) (where OUT = -> )","title":"[function] !asin"},{"location":"base-impl/#inputs_28","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_7","text":"->","title":"Returns"},{"location":"base-impl/#nodes_8","text":"SCALAR-ASINNODE ASINNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_15","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-acos","text":"(!acos x &key (-> nil)) The function !acos takes x as an argument, applying a acos function into each element and writes the result into -> . O U T c o p y \u2190 a c o s ( X ) OUT_{copy}\\gets{acos(X)} O U T co p y \u200b \u2190 a cos ( X ) (where OUT = -> )","title":"[function] !acos"},{"location":"base-impl/#inputs_29","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_8","text":"->","title":"Returns"},{"location":"base-impl/#nodes_9","text":"SCALAR-ACOSNODE ACOSNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_16","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-atan","text":"(!atan x &key (-> nil)) The function !atan takes x as an argument, applying a atan function into each element and writes the result into -> . O U T c o p y \u2190 a t a n ( X ) OUT_{copy}\\gets{atan(X)} O U T co p y \u200b \u2190 a t an ( X ) (where OUT = -> )","title":"[function] !atan"},{"location":"base-impl/#inputs_30","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_9","text":"->","title":"Returns"},{"location":"base-impl/#nodes_10","text":"SCALAR-ATANNODE ATANNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_17","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-sinh","text":"(!sinh x &key (-> nil)) The function !sinh takes x as an argument, applying a sinh function into each element and writes the result into -> . O U T c o p y \u2190 s i n h ( X ) OUT_{copy}\\gets{sinh(X)} O U T co p y \u200b \u2190 s inh ( X ) (where OUT = -> )","title":"[function] !sinh"},{"location":"base-impl/#inputs_31","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_10","text":"->","title":"Returns"},{"location":"base-impl/#nodes_11","text":"SCALAR-SINHNODE SINHNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_18","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-cosh","text":"(!cosh x &key (-> nil)) The function !cosh takes x as an argument, applying a cosh function into each element and writes the result into -> . O U T c o p y \u2190 c o s h ( X ) OUT_{copy}\\gets{cosh(X)} O U T co p y \u200b \u2190 cos h ( X ) (where OUT = -> )","title":"[function] !cosh"},{"location":"base-impl/#inputs_32","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_11","text":"->","title":"Returns"},{"location":"base-impl/#nodes_12","text":"SCALAR-COSHNODE COSHNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_19","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-tanh","text":"(!tanh x &key (-> nil)) The function !tanh takes x as an argument, applying a tanh function into each element and writes the result into -> . O U T c o p y \u2190 t a n h ( X ) OUT_{copy}\\gets{tanh(X)} O U T co p y \u200b \u2190 t anh ( X ) (where OUT = -> )","title":"[function] !tanh"},{"location":"base-impl/#inputs_33","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_12","text":"->","title":"Returns"},{"location":"base-impl/#nodes_13","text":"SCALAR-TANHNODE TANHNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_20","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-asinh","text":"(!asinh x &key (-> nil)) The function !asinh takes x as an argument, applying a asinh function into each element and writes the result into -> . O U T c o p y \u2190 a s i n h ( X ) OUT_{copy}\\gets{asinh(X)} O U T co p y \u200b \u2190 a s inh ( X ) (where OUT = -> )","title":"[function] !asinh"},{"location":"base-impl/#inputs_34","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_13","text":"->","title":"Returns"},{"location":"base-impl/#nodes_14","text":"SCALAR-ASINHNODE ASINHNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_21","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-acosh","text":"(!acosh x &key (-> nil)) The function !acosh takes x as an argument, applying a acosh function into each element and writes the result into -> . O U T c o p y \u2190 a c o s h ( X ) OUT_{copy}\\gets{acosh(X)} O U T co p y \u200b \u2190 a cos h ( X ) (where OUT = -> )","title":"[function] !acosh"},{"location":"base-impl/#inputs_35","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_14","text":"->","title":"Returns"},{"location":"base-impl/#nodes_15","text":"SCALAR-ACOSHNODE ACOSHNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_22","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-atanh","text":"(!atanh x &key (-> nil)) The function !atanh takes x as an argument, applying a atanh function into each element and writes the result into -> . O U T c o p y \u2190 a t a n h ( X ) OUT_{copy}\\gets{atanh(X)} O U T co p y \u200b \u2190 a t anh ( X ) (where OUT = -> )","title":"[function] !atanh"},{"location":"base-impl/#inputs_36","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_15","text":"->","title":"Returns"},{"location":"base-impl/#nodes_16","text":"SCALAR-ATANHNODE ATANHNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_23","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-exp","text":"(!exp x &key (-> nil)) The function !exp takes x as an argument, applying a exp function into each element and writes the result into -> . O U T c o p y \u2190 e x p ( X ) OUT_{copy}\\gets{exp(X)} O U T co p y \u200b \u2190 e x p ( X ) (where OUT = -> )","title":"[function] !exp"},{"location":"base-impl/#inputs_37","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_16","text":"->","title":"Returns"},{"location":"base-impl/#nodes_17","text":"SCALAR-EXPNODE EXPNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_24","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-log2","text":"(!log2 x &key (-> nil)) The function !log2 takes x as an argument, applying a log2 function into each element and writes the result into -> . O U T c o p y \u2190 l o g 2 ( X ) OUT_{copy}\\gets{log2(X)} O U T co p y \u200b \u2190 l o g 2 ( X ) (where OUT = -> )","title":"[function] !log2"},{"location":"base-impl/#inputs_38","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_17","text":"->","title":"Returns"},{"location":"base-impl/#nodes_18","text":"SCALAR-LOG2NODE LOG2NODE","title":"Nodes"},{"location":"base-impl/#sideeffects_25","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-log10","text":"(!log10 x &key (-> nil)) The function !log10 takes x as an argument, applying a log10 function into each element and writes the result into -> . O U T c o p y \u2190 l o g 10 ( X ) OUT_{copy}\\gets{log10(X)} O U T co p y \u200b \u2190 l o g 10 ( X ) (where OUT = -> )","title":"[function] !log10"},{"location":"base-impl/#inputs_39","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_18","text":"->","title":"Returns"},{"location":"base-impl/#nodes_19","text":"SCALAR-LOG10NODE LOG10NODE","title":"Nodes"},{"location":"base-impl/#sideeffects_26","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-loge","text":"(!loge x &key (-> nil)) The function !loge takes x as an argument, applying a loge function into each element and writes the result into -> . O U T c o p y \u2190 l o g e ( X ) OUT_{copy}\\gets{loge(X)} O U T co p y \u200b \u2190 l o g e ( X ) (where OUT = -> )","title":"[function] !loge"},{"location":"base-impl/#inputs_40","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_19","text":"->","title":"Returns"},{"location":"base-impl/#nodes_20","text":"SCALAR-LOGENODE LOGENODE","title":"Nodes"},{"location":"base-impl/#sideeffects_27","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-sum","text":"(!sum tensor &key (axis t) (-> nil) (keepdims nil)) The function !sum return a node which computes the sum of tensor along the given axis.","title":"[function] !sum"},{"location":"base-impl/#inputs_41","text":"tensor , a tensor to be reducted. axis [t or fixnum or list] the axis to be reducted. (-1, -2... is ok) -> [AbstractTensor or nil] the place to set the result. If nil, creates a new tensor. dims [boolean] If t, the axis reducted is broadcasted. Return: -> [AbstractTensor] the result.","title":"Inputs"},{"location":"base-impl/#function-mean","text":"(!mean tensor &key (axis t) (-> nil) (keepdims nil)) The function !mean return a node which computes the average of tensor along the given axis.","title":"[function] !mean"},{"location":"base-impl/#inputs_42","text":"tensor , a tensor to be reducted. axis [t or fixnum or list] the axis to be reducted. (-1, -2... is ok) -> [AbstractTensor or nil] the place to set the result. If nil, creates a new tensor. keepdims [boolean] If t, the axis reducted is broadcasted.","title":"Inputs"},{"location":"base-impl/#return_1","text":"-> [AbstractTensor] the result.","title":"Return"},{"location":"base-impl/#function-argmax","text":"(!argmax tensor &key (axis -1) (out nil)) The function !argmax computes the indices of maximum values of all elements below the axis dimension in the given tensor.","title":"[function] !argmax"},{"location":"base-impl/#inputs_43","text":"tensor axis out","title":"Inputs"},{"location":"base-impl/#returns_20","text":"AbstractTensor[uint32] with dimensions behind axis is replaced with 1.","title":"Returns"},{"location":"base-impl/#function-argmin","text":"(!argmin tensor &key (axis -1) (out nil)) The function !argmin computes the indices of minimum values of all elements below the axis dimension in the given tensor.","title":"[function] !argmin"},{"location":"base-impl/#inputs_44","text":"tensor axis out","title":"Inputs"},{"location":"base-impl/#returns_21","text":"AbstractTensor[uint32] with dimensions behind axis is replaced with 1.","title":"Returns"},{"location":"base-impl/#function-max","text":"(!max tensor &key (axis -1) (out nil)) The function !max finds largest values of all elements below the axis rank in the given tensor.","title":"[function] !max"},{"location":"base-impl/#inputs_45","text":"tensor axis out","title":"Inputs"},{"location":"base-impl/#returns_22","text":"AbstractTensor with dimensions behind axis is replaced with 1.","title":"Returns"},{"location":"base-impl/#function-min","text":"(!min tensor &key (axis -1) (out nil)) The function !min finds the smallest values of all elements below the axis rank in the given tensor.","title":"[function] !min"},{"location":"base-impl/#inputs_46","text":"tensor axis out","title":"Inputs"},{"location":"base-impl/#returns_23","text":"AbstractTensor with dimensions behind axis is replaced with 1.","title":"Returns"},{"location":"base-impl/#function-t","text":"(!t tensor) Transposes the last two axes of the given tensor. When called with !matmul, the operation is ignored.","title":"[function] !t"},{"location":"base-impl/#function-matmul","text":"(!matmul x y &key (out nil) (transpose-x nil) (transpose-y nil)) Computing a matrix multiplication of X and Y. The result is stored in out if specified, otherwise creates a new tensor. o u t \u2190 g e m m ( 1.0 , x , y , 0.0 , o u t ) out\\gets{gemm(1.0, x, y, 0.0, out)} o u t \u2190 g e mm ( 1.0 , x , y , 0.0 , o u t )","title":"[function] !matmul"},{"location":"base-impl/#inputs_47","text":"transpose-x, transpose-y[boolean] If t, the inputs are wrapped with (!t tensor) .","title":"Inputs"},{"location":"base-impl/#tips-lazy-transpose-node","text":"If the last backward of given arguments are LazyTransposeNode (created with the function !t ), the function !matmul will transpose them without making a copy (i.e.: zero-cost transpose). In any other case (the last two dimensions' permution, or view are too complicated), !matmul will produce an additional copy for fast computing.","title":"Tips: Lazy-Transpose-Node"},{"location":"base-impl/#function-dot","text":"(!dot x y) Finds a dot product of x and y. Unlike numpy.dot , !dot intentionally only supports computing the dot product of two 1D tensors with the same number of elements. (proceed (!dot (randn `(100)) (randn `(10 10)))) {CPUTENSOR[float] :shape (1) -> :view (<0>) -> :visible-shape (1) :named ChainTMP115880 :vec-state [computed] (21.594929) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>}","title":"[function] !dot"},{"location":"base-impl/#function-where","text":"(!where tensor condition &key (true-then 1) (false-then 0) (out nil)) The function !where returns a elements selected-from true-then or false-then , depending on condition. The operation is defined as: o u t i = { true-then c o n d i t i o n ( X i ) false-then otherwise \\begin{equation} out_i= \\begin{cases} \\text{true-then} & condition(X_i) \\\\ \\text{false-then} & \\text{otherwise} \\end{cases} \\end{equation} o u t i \u200b = { true-then false-then \u200b co n d i t i o n ( X i \u200b ) otherwise \u200b \u200b \u200b (where X = tensor)","title":"[function] !where"},{"location":"base-impl/#inputs_48","text":"out place to set the result condition an funcallable function. (e.g.: #'evenp #'oddp etc...)","title":"Inputs"},{"location":"base-impl/#function-where_1","text":"(!compare tensor1 tensor2 condition &key (true-then 1) (false-then 0) (out nil)) The function !compare returns a elements selected-from true-then or false-then , depending on condition. The operation is defined as: o u t i = { true-then c o n d i t i o n ( X i , Y i ) false-then otherwise \\begin{equation} out_i= \\begin{cases} \\text{true-then} & condition(X_i, Y_i) \\\\ \\text{false-then} & \\text{otherwise} \\end{cases} \\end{equation} o u t i \u200b = { true-then false-then \u200b co n d i t i o n ( X i \u200b , Y i \u200b ) otherwise \u200b \u200b \u200b (where X = tensor1, Y=tensor2)","title":"[function] !where"},{"location":"base-impl/#inputs_49","text":"out place to set the result condition an funcallable function. (e.g.: #'> #'< etc...)","title":"Inputs"},{"location":"base-impl/#function-ascal","text":"(a>scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a>scal sets true-then if the equation: element > scal is t, otherwise set false-then at the corresponding positions.","title":"[function] a&gt;scal"},{"location":"base-impl/#inputs_50","text":"A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as a scal argument)","title":"Inputs"},{"location":"base-impl/#function-ascal_1","text":"(a<scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a<scal sets true-then if the equation: element < scal is t, otherwise set false-then at the corresponding positions.","title":"[function] a&lt;scal"},{"location":"base-impl/#inputs_51","text":"A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as a scal argument)","title":"Inputs"},{"location":"base-impl/#function-ascal_2","text":"(a>=scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a>=scal sets true-then if the equation: element >= scal is t, otherwise set false-then at the corresponding positions.","title":"[function] a&gt;=scal"},{"location":"base-impl/#inputs_52","text":"A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as a scal argument)","title":"Inputs"},{"location":"base-impl/#function-ascal_3","text":"(a<=scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a<=scal sets true-then if the equation: element <= scal is t, otherwise set false-then at the corresponding positions.","title":"[function] a&lt;=scal"},{"location":"base-impl/#inputs_53","text":"A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as a scal argument)","title":"Inputs"},{"location":"base-impl/#function-ascal_4","text":"(a=scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a=scal sets true-then if the equation: element = scal is t, otherwise set false-then at the corresponding positions.","title":"[function] a=scal"},{"location":"base-impl/#inputs_54","text":"A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as a scal argument)","title":"Inputs"},{"location":"base-impl/#function-ab","text":"(a>b A B &key (out nil) (true-then 1) (false-then 0)) The function a>b sets true-then if the equation: A > B is t, otherwise set false-then at the corresponding positions.","title":"[function] a&gt;b"},{"location":"base-impl/#inputs_55","text":"A B AbstractTensor to be compared.","title":"Inputs"},{"location":"base-impl/#function-ab_1","text":"(a<b A B &key (out nil) (true-then 1) (false-then 0)) The function a<b sets true-then if the equation: A < B is t, otherwise set false-then at the corresponding positions.","title":"[function] a&lt;b"},{"location":"base-impl/#inputs_56","text":"A B AbstractTensor to be compared.","title":"Inputs"},{"location":"base-impl/#function-ab_2","text":"(a>=b A B &key (out nil) (true-then 1) (false-then 0)) The function a>=b sets true-then if the equation: A >= B is t, otherwise set false-then at the corresponding positions.","title":"[function] a&gt;=b"},{"location":"base-impl/#inputs_57","text":"A B AbstractTensor to be compared.","title":"Inputs"},{"location":"base-impl/#function-ab_3","text":"(a<=b A B &key (out nil) (true-then 1) (false-then 0)) The function a<=b sets true-then if the equation: A <= B is t, otherwise set false-then at the corresponding positions.","title":"[function] a&lt;=b"},{"location":"base-impl/#inputs_58","text":"A B AbstractTensor to be compared.","title":"Inputs"},{"location":"base-impl/#function-ab_4","text":"(a=b A B &key (out nil) (true-then 1) (false-then 0)) The function a=b sets true-then if the equation: A = B is t, otherwise set false-then at the corresponding positions.","title":"[function] a=b"},{"location":"base-impl/#inputs_59","text":"A B AbstractTensor to be compared.","title":"Inputs"},{"location":"base-impl/#function-padding","text":"(padding tensor pad-width &key (pad-maker #'ax+b) (initargs `(0 0))) Creating a new InputTensor with shape after padding, the function padding moves the given tensor into a new area.","title":"[function] padding"},{"location":"base-impl/#implementation","text":"(padding (ax+b `(1 3) 0 1) `((1 1) (1 1))) [Corresponds with...] 00000 +++ -> [padding] -> 0+++0 00000 The operation is performed in the following steps: First, creates a new tensor with the shape of after padded which is initialized via the pad-maker function, where pad-maker is an initializer function, that is, functions defined by define-initializer-function or exported from the :cl-waffe2/distribution package. +++++ +++++ +++++ The function padding uses the form below to initialize tensors. (apply pad-maker initargs) In default, (apply #'ax+b (0 0))`. Second, makes a view of the new tensor and match its shape to the base tensor. the argument pad-width is used to determine offsets of each axis. pad-width is the number of values to the edges of each axis. and given as: ((before_1 after_1) (before_2 after_2) ...) . 0~before_n and before_n~last are the subject to be padded. +++++ ----- +++++ -> [view] -> -+++- +++++ ----- ^ after_2 + ... visible area - ... hide area by view Finally, moves all elements in the base tensor into viewed tensor, and later reset the view.","title":"Implementation"},{"location":"base-impl/#inputs_60","text":"tensor[AbstractTensor] tensor to be padded. pad-width[list] the number of the edges and given as: ((before_1 after_1) (before_2 after_2) ...) . the forward is the same as np.pad . Set t instead of (before_n after_n) and ignores the corresponding position of axis. pad-maker[function] an initializer-function initargs[list] a list of arguments for pad-maker Note that: the axes to be padded, must be fixnum. not a symbol. If the shapes does not change before/after padding, returns the given tensor as it is.","title":"Inputs"},{"location":"base-impl/#example_8","text":"(proceed (padding (ax+b `(3 3) 0 1) `((1 1) (1 1)))) {CPUTENSOR[float] :shape (5 5) -> :view (<T> <T>) -> :visible-shape (5 5) :named ChainTMP1104579 :vec-state [computed] ((0.0 0.0 0.0 0.0 0.0) (0.0 1.0 1.0 1.0 0.0) ... (0.0 1.0 1.0 1.0 0.0) (0.0 0.0 0.0 0.0 0.0)) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>}","title":"Example"},{"location":"base-impl/#function-broadcast-to","text":"Returns the subscript of the !view that is broadcasting to be the same shape as the object-tensor . For example: ;; x ... ( 3 3 ) Tensor ;; (!sum x :axis 1) ... ( 3 1 ) Tensor ;; broadcast-to will return: (t `(:broadcast 3)) (!mul x (!view (!sum x :axis 1) (broadcast-to x)))","title":"[function] broadcast-to"},{"location":"cpu-jit-tensor-backend/","text":"[package] :cl-waffe2/backends.jit.cpu [Unstable] The package :cl-waffe2/backends.jit.cpu provides an AbstractTensor JITCPUTensor which accelerated by JIT Compiling to C code dynamically, (so this backend will require gcc as an additional requirement.) [parameter] *default-c-compiler* Specify the command to compile the generated c codes. In default, \"gcc\". [parameter] *compiler-flags* In default, *compielr-flags* = '(\"-fPIC\" \"-O3\" \"-march=native\") [parameter] *viz-compiled-code* Set t to display the compiled c code to terminal. In default, nil [AbstractTensor] JITCPUTensor [AbstractTensor] JITCPUScalarTensor [function] enable-cpu-jit-toplevel (enable-cpu-jit-toplevel (&key (more-devices) (compiler \"gcc\") (viz-compiled-code nil) (openmp nil) (flags '(\"-fPIC\" \"-O3\" \"-march=native\")))) Sets JITCPUTensor and JITCPUScalarTensor to the top priority of backends. Place this function at the top of your code where JIT Compiling is needed. Of course, JITCPUTensor is developed as a one of external backends in cl-waffe2, therefore Local JIT compilation with the with-devices macro is another valid option. Inputs more-devices[List] specify the list of device names. they have lower priority than JITCPUTensor viz-compiled-code[boolean] Set t to display the compiled c codes. openMP[boolean] set T to use OpenMP. [macro] with-cpu-jit Under this macro, two backends ( JITCPUTensor and JITCPUScalarTensor ) are installed at the top of the priority list.","title":"cl-waffe2/backends.jit.cpu"},{"location":"cpu-jit-tensor-backend/#package-cl-waffe2backendsjitcpu","text":"[Unstable] The package :cl-waffe2/backends.jit.cpu provides an AbstractTensor JITCPUTensor which accelerated by JIT Compiling to C code dynamically, (so this backend will require gcc as an additional requirement.)","title":"[package] :cl-waffe2/backends.jit.cpu"},{"location":"cpu-jit-tensor-backend/#parameter-default-c-compiler","text":"Specify the command to compile the generated c codes. In default, \"gcc\".","title":"[parameter] *default-c-compiler*"},{"location":"cpu-jit-tensor-backend/#parameter-compiler-flags","text":"In default, *compielr-flags* = '(\"-fPIC\" \"-O3\" \"-march=native\")","title":"[parameter] *compiler-flags*"},{"location":"cpu-jit-tensor-backend/#parameter-viz-compiled-code","text":"Set t to display the compiled c code to terminal. In default, nil","title":"[parameter] *viz-compiled-code*"},{"location":"cpu-jit-tensor-backend/#abstracttensor-jitcputensor","text":"","title":"[AbstractTensor] JITCPUTensor"},{"location":"cpu-jit-tensor-backend/#abstracttensor-jitcpuscalartensor","text":"","title":"[AbstractTensor] JITCPUScalarTensor"},{"location":"cpu-jit-tensor-backend/#function-enable-cpu-jit-toplevel","text":"(enable-cpu-jit-toplevel (&key (more-devices) (compiler \"gcc\") (viz-compiled-code nil) (openmp nil) (flags '(\"-fPIC\" \"-O3\" \"-march=native\")))) Sets JITCPUTensor and JITCPUScalarTensor to the top priority of backends. Place this function at the top of your code where JIT Compiling is needed. Of course, JITCPUTensor is developed as a one of external backends in cl-waffe2, therefore Local JIT compilation with the with-devices macro is another valid option.","title":"[function] enable-cpu-jit-toplevel"},{"location":"cpu-jit-tensor-backend/#inputs","text":"more-devices[List] specify the list of device names. they have lower priority than JITCPUTensor viz-compiled-code[boolean] Set t to display the compiled c codes. openMP[boolean] set T to use OpenMP.","title":"Inputs"},{"location":"cpu-jit-tensor-backend/#macro-with-cpu-jit","text":"Under this macro, two backends ( JITCPUTensor and JITCPUScalarTensor ) are installed at the top of the priority list.","title":"[macro] with-cpu-jit"},{"location":"cpu-tensor-backend/","text":"[package] :cl-waffe2/backends.cpu The package :cl-waffe2/backends.cpu provides an AbstractTensor CPUTensor where most of its implementation relies on foreign libraries (e.g.: OpenBLAS, oneDNN in the coming future). Enabling the SIMD Extension For some instructions (e.g.: !max !min , sparse matrix supports, SLEEF , etc...), packages that provide SIMD-enabled CPUTensor implementations are not enabled by default as a design. To enable it, run make build_simd_extension in the same directory as cl-waffe2.asd. You can check that it is loaded properly with the (show-backends) function. [AbstractTensor] CPUTensor","title":"cl-waffe2/backends.cpu"},{"location":"cpu-tensor-backend/#package-cl-waffe2backendscpu","text":"The package :cl-waffe2/backends.cpu provides an AbstractTensor CPUTensor where most of its implementation relies on foreign libraries (e.g.: OpenBLAS, oneDNN in the coming future).","title":"[package] :cl-waffe2/backends.cpu"},{"location":"cpu-tensor-backend/#enabling-the-simd-extension","text":"For some instructions (e.g.: !max !min , sparse matrix supports, SLEEF , etc...), packages that provide SIMD-enabled CPUTensor implementations are not enabled by default as a design. To enable it, run make build_simd_extension in the same directory as cl-waffe2.asd. You can check that it is loaded properly with the (show-backends) function.","title":"Enabling the SIMD Extension"},{"location":"cpu-tensor-backend/#abstracttensor-cputensor","text":"","title":"[AbstractTensor] CPUTensor"},{"location":"distributions/","text":".katex img { object-fit: fill; padding: unset; display: block; position: absolute; width: 100%; height: inherit; } Distributions Sampling matrices from distribution cl-waffe2 provides a package :cl-waffe2/distributions which is used to sample matrices from the distributions. Common Format to the APIs All sampling functions are defined in the following format via define-tensor-initializer macro. (function-name shape [Optional Arguments] &rest args &keys &allow-other-keys) That is, arguments passed to the make-tensor function can also be passed directly to the initializer functions. Example (normal `(10 10) 0.0 1.0 :requires-grad t) {CPUTENSOR[float] :shape (10 10) ((-0.7096969 -0.534541 0.09241722 ~ 0.8181761 -0.32395983 -1.7448716) (-0.11915433 1.2616262 0.034676224 ~ -0.1889971 0.45699593 -0.14157301) ... (0.14158817 0.4875129 -0.018849093 ~ 1.9703826 -0.096768215 0.7583118) (1.3229972 -1.2871348 -0.69942784 ~ 1.4221152 -0.04533768 -0.3676781)) :facet :exist :requires-grad T :backward NIL} Example (ax+b `(10 10) 1 0 :dtype :uint8) {CPUTENSOR[uint8] :shape (10 10) ((0 1 2 ~ 7 8 9) (10 11 12 ~ 17 18 19) ... (80 81 82 ~ 87 88 89) (90 91 92 ~ 97 98 99)) :facet :exist :requires-grad NIL :backward NIL} define-tensor-initializer (define-tensor-initializer (function-name (&rest args) initializer-lambda document &key (keep-order? nil))) define-tensor-initializer is a macro which is used to define a initializer function. Initializer function is a function whose arguments follow this format: (function-name shape <Initializer's Arguments> &rest initargs &key &allow-other-keys) Input: function-name - the function is defined after this argument args - Initializer's Arguments initializer-lambda - A form to be expanded as the sampling function, which must return a function of #'(lambda (i) ...) where i is the index of element. keep-order? - set t if the index is needed to sampling matrices. Example: (define-initializer-function uniform-random (upfrom below) (let ((upfrom (coerce upfrom (dtype->lisp-type (dtype tensor)))) (below (coerce below (dtype->lisp-type (dtype tensor))))) #'(lambda (i) (declare (ignore i)) (sample-uniform-random upfrom below))) \"\") (uniform-random `(10 10) 0.1 0.3 :requires-grad t) {CPUTENSOR[float] :shape (10 10) ((0.13149574 0.15135926 0.1569588 ~ 0.103781514 0.20610212 0.19365484) (0.2638953 0.12672275 0.21630599 ~ 0.16542184 0.10228193 0.12928057) ... (0.20429519 0.12252951 0.17538154 ~ 0.22072719 0.18642941 0.11027551) (0.14372297 0.11097031 0.25514898 ~ 0.28739202 0.18398522 0.15176433)) :facet :exist :requires-grad T :backward NIL} (Note that new tensor is binded to tensor, being used to determined dtype etc...) ax+b (ax+b shape a b &rest initargs &key &allow-other-keys) The function ax+b is a family of initializer functions, and samples matrices from arithmetic progression. o u t n = a n + b out_n = an + b o u t n \u200b = an + b Inputs: a, b - Coefficients of the above formula. Example (ax+b `(3 3) 1.0 0.0) {CPUTENSOR[float] :shape (3 3) ((0.0 1.0 2.0) (3.0 4.0 5.0) (6.0 7.0 8.0)) :facet :exist :requires-grad NIL :backward NIL} beta (beta shape alpha beta &rest initargs &key &allow-other-keys) The function beta is a family of initializer functions, and sample matrices from beta distribution. Reference Generating Beta Variates with Nonintegral Shape Parameters (R. C. H. Cheng University of Wales Institute of Science and Technology) https://dl.acm.org/doi/pdf/10.1145/359460.359482 Note: My implementation is unstable, being occurs floating-overflow constantly..., especially when min(alpha, beta) < 1.0 (i.e.: beta-bc) Example (beta `(3 3) 5.0 1.0) {CPUTENSOR[float] :shape (3 3) ((0.9686189 0.76956904 0.8549012) (0.4545031 0.914095 0.54855245) (0.93831366 0.7510813 0.45294034)) :facet :exist :requires-grad NIL :backward NIL} bernoulli (bernoulli shape p &rest initargs &key &allow-other-keys) The bernoulli is a family of initializer functions, and samples matrices from bernoulli distribution. Inputs p - Takes 1 with probability p and 0 with probalibity (1-p). Example (bernoulli `(3 3) 0.3) {CPUTENSOR[float] :shape (3 3) ((0.0 0.0 0.0) (1.0 0.0 0.0) (0.0 0.0 0.0)) :facet :exist :requires-grad NIL :backward NIL} chisquare (chisquare shape df &rest initargs &key &allow-other-keys) The function chisquare is a family of initializer functions, and samples matrices from chisquare distributions. Inputs df - degree of freedom. References https://github.com/lvaruzza/cl-randist Example (chisquare `(3 3) 1.0) {CPUTENSOR[float] :shape (3 3) ((0.011596402 0.4021547 0.027511619) (0.08436891 0.5057191 0.011141564) (0.017833697 0.036469206 0.38433048)) :facet :exist :requires-grad NIL :backward NIL} expotential (expotential shape &rest initargs &key &allow-other-keys) The function expotential is a family of initializer functions, and samples the expotential distribution using ziggurat algorithm with table-size=256. References https://andantesoft.hatenablog.com/entry/2023/04/30/183032 Marsaglia, G., & Tsang, W. W. (2000). The ziggurat method for generating random variables. Journal of statistical software. https://marui.hatenablog.com/entry/2023/01/23/194507 Example (expotential `(3 3)) {CPUTENSOR[float] :shape (3 3) ((3.1640477 1.1214623 2.3461883) (0.6938687 0.08668403 0.46339378) (0.18236026 0.074848704 2.148749)) :facet :exist :requires-grad NIL :backward NIL} gamma (gamma shape k &rest initargs &key &allow-other-keys) The function gamma is a family of initializer functions, and samples matrices from the gamma distribution. References https://github.com/lvaruzza/cl-randist Example (gamma `(3 3) 1.0) {CPUTENSOR[float] :shape (3 3) ((2.4886518 1.0235898 1.0265534) (2.005943 1.0844046 0.115611516) (2.299866 0.2878098 3.3350327)) :facet :exist :requires-grad NIL :backward NIL} normal (normal shape mean stddev &rest initargs &key &allow-other-keys) The function normal is a family of initializer functions, and samples matrices from normal distribution. Reference https://github.com/lvaruzza/cl-randist (seems to create ziggurat table with size=128) Inputs mean stddev - Standard Deviation, \u03c3. Example (normal `(3 3) 1.0 0.0) {CPUTENSOR[float] :shape (3 3) ((1.0 1.0 1.0) (1.0 1.0 1.0) (1.0 1.0 1.0)) :facet :exist :requires-grad NIL :backward NIL} uniform-random (uniform-random shape upfrom below &rest initargs &key &allow-other-keys) The function uniform-random is a family of initializer funtions, and samples matrices from uniform random distribution using Common Lisp's standard function, (random arg) . Input: upfrom, below. Each elements of returned tensor is in the range of: `[upfrom, below)` Example (uniform-random `(3 3) 2 4) {CPUTENSOR[float] :shape (3 3) ((2.549292 3.2897213 3.801046) (2.0587037 3.9055057 3.4732718) (2.4112198 3.0563474 2.3659306)) :facet :exist :requires-grad NIL :backward NIL} randn (randn shape &rest initargs &key &allow-other-keys) The function randn is a family of initializer functions, and samples the gaussian distributions using ziggurat algorithm with table-size=256. References https://andantesoft.hatenablog.com/entry/2023/04/30/183032 Marsaglia, G., & Tsang, W. W. (2000). The ziggurat method for generating random variables. Journal of statistical software. https://marui.hatenablog.com/entry/2023/01/23/194507 Example (randn `(3 3)) {CPUTENSOR[float] :shape (3 3) ((-0.44232148 -0.81750864 -0.496161) (0.1307186 -0.22390747 -0.7750364) (-0.077972114 0.043979526 -0.6757204)) :facet :exist :requires-grad NIL :backward NIL}","title":"cl-waffe2/distributions"},{"location":"distributions/#distributions","text":"","title":"Distributions"},{"location":"distributions/#sampling-matrices-from-distribution","text":"cl-waffe2 provides a package :cl-waffe2/distributions which is used to sample matrices from the distributions.","title":"Sampling matrices from distribution"},{"location":"distributions/#common-format-to-the-apis","text":"All sampling functions are defined in the following format via define-tensor-initializer macro. (function-name shape [Optional Arguments] &rest args &keys &allow-other-keys) That is, arguments passed to the make-tensor function can also be passed directly to the initializer functions.","title":"Common Format to the APIs"},{"location":"distributions/#example","text":"(normal `(10 10) 0.0 1.0 :requires-grad t) {CPUTENSOR[float] :shape (10 10) ((-0.7096969 -0.534541 0.09241722 ~ 0.8181761 -0.32395983 -1.7448716) (-0.11915433 1.2616262 0.034676224 ~ -0.1889971 0.45699593 -0.14157301) ... (0.14158817 0.4875129 -0.018849093 ~ 1.9703826 -0.096768215 0.7583118) (1.3229972 -1.2871348 -0.69942784 ~ 1.4221152 -0.04533768 -0.3676781)) :facet :exist :requires-grad T :backward NIL}","title":"Example"},{"location":"distributions/#example_1","text":"(ax+b `(10 10) 1 0 :dtype :uint8) {CPUTENSOR[uint8] :shape (10 10) ((0 1 2 ~ 7 8 9) (10 11 12 ~ 17 18 19) ... (80 81 82 ~ 87 88 89) (90 91 92 ~ 97 98 99)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#define-tensor-initializer","text":"(define-tensor-initializer (function-name (&rest args) initializer-lambda document &key (keep-order? nil))) define-tensor-initializer is a macro which is used to define a initializer function. Initializer function is a function whose arguments follow this format: (function-name shape <Initializer's Arguments> &rest initargs &key &allow-other-keys) Input: function-name - the function is defined after this argument args - Initializer's Arguments initializer-lambda - A form to be expanded as the sampling function, which must return a function of #'(lambda (i) ...) where i is the index of element. keep-order? - set t if the index is needed to sampling matrices. Example: (define-initializer-function uniform-random (upfrom below) (let ((upfrom (coerce upfrom (dtype->lisp-type (dtype tensor)))) (below (coerce below (dtype->lisp-type (dtype tensor))))) #'(lambda (i) (declare (ignore i)) (sample-uniform-random upfrom below))) \"\") (uniform-random `(10 10) 0.1 0.3 :requires-grad t) {CPUTENSOR[float] :shape (10 10) ((0.13149574 0.15135926 0.1569588 ~ 0.103781514 0.20610212 0.19365484) (0.2638953 0.12672275 0.21630599 ~ 0.16542184 0.10228193 0.12928057) ... (0.20429519 0.12252951 0.17538154 ~ 0.22072719 0.18642941 0.11027551) (0.14372297 0.11097031 0.25514898 ~ 0.28739202 0.18398522 0.15176433)) :facet :exist :requires-grad T :backward NIL} (Note that new tensor is binded to tensor, being used to determined dtype etc...)","title":"define-tensor-initializer"},{"location":"distributions/#axb","text":"(ax+b shape a b &rest initargs &key &allow-other-keys) The function ax+b is a family of initializer functions, and samples matrices from arithmetic progression. o u t n = a n + b out_n = an + b o u t n \u200b = an + b Inputs: a, b - Coefficients of the above formula.","title":"ax+b"},{"location":"distributions/#example_2","text":"(ax+b `(3 3) 1.0 0.0) {CPUTENSOR[float] :shape (3 3) ((0.0 1.0 2.0) (3.0 4.0 5.0) (6.0 7.0 8.0)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#beta","text":"(beta shape alpha beta &rest initargs &key &allow-other-keys) The function beta is a family of initializer functions, and sample matrices from beta distribution.","title":"beta"},{"location":"distributions/#reference","text":"Generating Beta Variates with Nonintegral Shape Parameters (R. C. H. Cheng University of Wales Institute of Science and Technology) https://dl.acm.org/doi/pdf/10.1145/359460.359482 Note: My implementation is unstable, being occurs floating-overflow constantly..., especially when min(alpha, beta) < 1.0 (i.e.: beta-bc)","title":"Reference"},{"location":"distributions/#example_3","text":"(beta `(3 3) 5.0 1.0) {CPUTENSOR[float] :shape (3 3) ((0.9686189 0.76956904 0.8549012) (0.4545031 0.914095 0.54855245) (0.93831366 0.7510813 0.45294034)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#bernoulli","text":"(bernoulli shape p &rest initargs &key &allow-other-keys) The bernoulli is a family of initializer functions, and samples matrices from bernoulli distribution.","title":"bernoulli"},{"location":"distributions/#inputs","text":"p - Takes 1 with probability p and 0 with probalibity (1-p).","title":"Inputs"},{"location":"distributions/#example_4","text":"(bernoulli `(3 3) 0.3) {CPUTENSOR[float] :shape (3 3) ((0.0 0.0 0.0) (1.0 0.0 0.0) (0.0 0.0 0.0)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#chisquare","text":"(chisquare shape df &rest initargs &key &allow-other-keys) The function chisquare is a family of initializer functions, and samples matrices from chisquare distributions.","title":"chisquare"},{"location":"distributions/#inputs_1","text":"df - degree of freedom.","title":"Inputs"},{"location":"distributions/#references","text":"https://github.com/lvaruzza/cl-randist","title":"References"},{"location":"distributions/#example_5","text":"(chisquare `(3 3) 1.0) {CPUTENSOR[float] :shape (3 3) ((0.011596402 0.4021547 0.027511619) (0.08436891 0.5057191 0.011141564) (0.017833697 0.036469206 0.38433048)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#expotential","text":"(expotential shape &rest initargs &key &allow-other-keys) The function expotential is a family of initializer functions, and samples the expotential distribution using ziggurat algorithm with table-size=256.","title":"expotential"},{"location":"distributions/#references_1","text":"https://andantesoft.hatenablog.com/entry/2023/04/30/183032 Marsaglia, G., & Tsang, W. W. (2000). The ziggurat method for generating random variables. Journal of statistical software. https://marui.hatenablog.com/entry/2023/01/23/194507","title":"References"},{"location":"distributions/#example_6","text":"(expotential `(3 3)) {CPUTENSOR[float] :shape (3 3) ((3.1640477 1.1214623 2.3461883) (0.6938687 0.08668403 0.46339378) (0.18236026 0.074848704 2.148749)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#gamma","text":"(gamma shape k &rest initargs &key &allow-other-keys) The function gamma is a family of initializer functions, and samples matrices from the gamma distribution.","title":"gamma"},{"location":"distributions/#references_2","text":"https://github.com/lvaruzza/cl-randist","title":"References"},{"location":"distributions/#example_7","text":"(gamma `(3 3) 1.0) {CPUTENSOR[float] :shape (3 3) ((2.4886518 1.0235898 1.0265534) (2.005943 1.0844046 0.115611516) (2.299866 0.2878098 3.3350327)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#normal","text":"(normal shape mean stddev &rest initargs &key &allow-other-keys) The function normal is a family of initializer functions, and samples matrices from normal distribution.","title":"normal"},{"location":"distributions/#reference_1","text":"https://github.com/lvaruzza/cl-randist (seems to create ziggurat table with size=128)","title":"Reference"},{"location":"distributions/#inputs_2","text":"mean stddev - Standard Deviation, \u03c3.","title":"Inputs"},{"location":"distributions/#example_8","text":"(normal `(3 3) 1.0 0.0) {CPUTENSOR[float] :shape (3 3) ((1.0 1.0 1.0) (1.0 1.0 1.0) (1.0 1.0 1.0)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#uniform-random","text":"(uniform-random shape upfrom below &rest initargs &key &allow-other-keys) The function uniform-random is a family of initializer funtions, and samples matrices from uniform random distribution using Common Lisp's standard function, (random arg) . Input: upfrom, below. Each elements of returned tensor is in the range of: `[upfrom, below)`","title":"uniform-random"},{"location":"distributions/#example_9","text":"(uniform-random `(3 3) 2 4) {CPUTENSOR[float] :shape (3 3) ((2.549292 3.2897213 3.801046) (2.0587037 3.9055057 3.4732718) (2.4112198 3.0563474 2.3659306)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#randn","text":"(randn shape &rest initargs &key &allow-other-keys) The function randn is a family of initializer functions, and samples the gaussian distributions using ziggurat algorithm with table-size=256.","title":"randn"},{"location":"distributions/#references_3","text":"https://andantesoft.hatenablog.com/entry/2023/04/30/183032 Marsaglia, G., & Tsang, W. W. (2000). The ziggurat method for generating random variables. Journal of statistical software. https://marui.hatenablog.com/entry/2023/01/23/194507","title":"References"},{"location":"distributions/#example_10","text":"(randn `(3 3)) {CPUTENSOR[float] :shape (3 3) ((-0.44232148 -0.81750864 -0.496161) (0.1307186 -0.22390747 -0.7750364) (-0.077972114 0.043979526 -0.6757204)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"generic-tensor/","text":"AbstractTensor Working with AbstractTensor [class] AbstractTensor AbstractTensor is a CLOS class that Wraps existing data structures such as matrices in an abstract class in automatic differential programming using cl-waffe2, and further adds information about computation nodes, gradients, etc. Tensors can be created by the make-tensor function. (make-tensor `(3 3)) Plus, InputTensors (lazy-evaluated tensors), which is used to delay allocation timing, to use dynamic shaping, and to store the result, can be created by the make-input function. (make-input `(3 3) :A) ;; Set :A=nil to register as a temporary space. As an applied use, users can create new AbstractTensor that inherit from AbstractTensor. In addition, inheriting existing AbstractTensors (e.g.: LispTensor for CL Standard Array) allows reusing descriptions such as allocations. (defclass MyOriginalTensor (AbstractTensor) nil) (defclass MyCPUTensor (LispTensor) nil) Declare the priority of the device to be used with the with-devices macro. ;; Higher <-> Lower (with-devices (MyCPUTensor MyOriginalTensor CPUTensor) (make-tensor `(10 10))) All available devices can be accessed with the (show-backends) function, and they can only be used as devices together if they are shown to have an inheritance relationship. If a completely new Tensor is defined from AbstractTensor, cl-waffe2 can handle it completely in a fast form by writing the following additional information. Allocator: initialize-instance :before method Storage Accessor: vref and (setf vref) method Finalizer: tensor-finalizer method (Optional) Backend State: current-backend-state method (Optional) a cl-waffe2/vm:defpath macro to enable device-specific optimization. This is the simplest case of MyTensor which works on CL Standard Array. (defclass MyTensor (AbstractTensor) nil) ;; Allocators satisfy the following properties ;; 1. When facet is not `:exist`, do nothing. ;; 2. If `vec` is specified as an argument, use this, and do not allocate any tensors. ;; 3. Otherwise, allocate the tensor with: ;; 1. Dtype -> :dtype ;; 2. Size -> :shape (must be 1D on the memory) ;; 3. initial-element -> :initial-element (defmethod initialize-instance :before ((tensor MyTensor) &rest initargs &key &allow-other-keys) (let* ((shape (getf initargs :shape)) (dtype (dtype->lisp-type (getf initargs :dtype))) (vec (getf initargs :vec)) (facet (getf initargs :facet)) (initial-element (coerce (or (getf initargs :initial-element) 0) dtype))) (when (eql facet :exist) (if vec (setf (tensor-vec tensor) vec) (setf (tensor-vec tensor) (make-array (apply #'* shape) :element-type dtype :initial-element initial-element)))))) ;; vref reads the index th element of storage vec, this is must be a setfable. ;; Leave the annoying and complicated stride/offset computations to cl-waffe2! (defmethod vref ((tensor MyTensor) index) (declare (type fixnum index)) (aref (tensor-vec tensor) index)) (defmethod (setf vref) (new-value (tensor MyTensor) index) (declare (type fixnum index)) (setf (aref (tensor-vec tensor) index) new-value)) ;; The method should return a lambda function, if its storage vector isn't gc-reachable. ;; Finalizers are called when quitting (with-memory-pool ...) macro. (defmethod tensor-finalizer ((tensor MyTensor)) ;; Returning a dummy finalizer #'(lambda ())) ;; The function (show-backends) will display all devices and their information ;; If you want to put something, override this method and return a string. (defmethod current-backend-state ((backend-name (eql 'MyTensor))) \"Hello This is an demo\") ;; For FusionOp and defpath macro usage, see the :cl-waffe2/vm docs. MyTensor is now recognised as a usable device, so operations can be defined using the define-impl and define-impl-op macros. [function] shape (shape tensor) returns a visible shape of the given tensor. [function] dims (dims tensor) returns a rank of the given tensor. [function] total (total tensor) returns the number of total visible elements of the giventensor. [slot] orig-shape (List) stores the shape of storage vec. [accessor] initial-offset (fixnum) stores the offset of the tensor. In default, set to 0. Shape testing, for example, does not work, so use with caution. (tensor-initial-offset tensor) [slot] stride (list) (tensor-stride tensor) stores the stride of tensor. [slot] visible-shape (list) (shape tensor) [slot] view (list) Returns a list of ViewInstruction, created by the function (view tensor ...) or (!view tensor ...) to create a backward. (tensor-view tensor) [slot] projected-p (boolean) Set t if (apply #'* orig-shape) == (apply #'* visible-shape) otherwise set nil. If t, the tensor is created by !view or view functions. [slot] scalar-p Set t if the tensor should be represented as a scalar. In cl-waffe2, it's not a pretty thing but scalars are represented as a (apply #'* shape)=1 tensors. ranks are anything but for the most case, returns 1. [slot] detach-p Set T to detach the tensor at a certain position. [slot] state (tensor-state tensor) stores StateContainer . [slot] variables (tensor-variables tensor) stores the previous variables if the tensor is created by any operation. [slot] tensor-id (symbol) Indicates where the Tensor is stored, (e.g. in a virtual machine). In-place operations inherit tensor-id from variables called with, and should not be used for topological sorting. [slot] tensor-iid (symbol) It holds an ID that is guaranteed to be absolutely unique to the processing system generated by gensym. Used for topological sorting. [slot] grad (AbstractTensor) If the tensor is created by (parameter ...) or with :requires-grad=t , (grad tensor) will return a gradient. [slot] backward (AbstractNode) (tensor-backward tensor) returns a abstractnode if the tensor is created by any operation. [slot] requires-grad (Boolean) Set T to hold the gradients. [slot] ancestor-param-p (Boolean) Set T if compilers can reach any tensors with :requires-grad=t , by tracing the tensor. [slot] flexible-p (Fixnum or Null) Indicates the position of broadcastable axis. [slot] facet (keyword) AbstractTensors in cl-waffe2 has a two state: ExistTensor and InputTensor . ExistTensor is a just tensor with allocated storage vec, made by make-tensor function. On the other hand InputTensor is a lazy-evaluated tensor, allocation won't be done until it is needed. :exist to ExitTensor, :input to InputTensor. [method] mref (mref tensor &rest subscripts) will reads a cetrain position of storage vec. This is setfable. In terms of performance, it is much faster way to edit a storage vec that using (change-facet) function and convert into other forms. Hooking Optimizers and Optimizing Parameters (TODO) [function] hook-optimizer! (hook-optimizer! tensor optimizer) Hooks the optimizer to the tensor. Inputs tensor[AbstractTensor] optimizer[AbstractOptimizer] [function] call-optimizer! (call-optimizer! tensor) Reading the (grad tensor) , the function invokes the optimizer hooked to the tensor. [function] reset-grad! Resets the gradient of the tensor with zero with retain-grad=t . [function] tensor-vec (tensor-vec tensor) If the given tensor is a ExistTensor, returns its storage vec. If the given tensor is a InputTensor, allocates the area for tensor and return its storage vec. This function is setfable and inlined. [function] make-tensor (make-tensor shape-or-scalar &key (requires-grad nil) (dtype *default-dtype*) (view nil) (order *default-order*) (initial-element nil) (device nil)) Created a new ExistTensor of a device of (car *using-backend*) . Inputs shape-or-scalar [Anything] If set to list, creates a new matrix. Otherwise (e.g.: set to fixnum), creates a ScalarTensor. In that case, cl-waffe2 uses the highest priority device from *using-backends* parameter that inherits from the ScalarTensor class. requires-grad [Boolean] Set t to holds a gradients. (parameter tensor) will also do the same work. Under (with-no-grad ...) macro. This is set to nil forcibly. dtype [keyword] Set keyword indicating a type of elements. order [keyword] set keyword indicating the order of elments from :column or :row . in default set to :column . initial-element [Anything] Set anything which you want to set as a initial element. device[symbol or null] If set to symbol, the function returns with making a tensor of device. Example (make-tensor `(10 10) :initial-element 1.0) {CPUTENSOR[float] :shape (10 10) ((1.0 1.0 1.0 ~ 1.0 1.0 1.0) (1.0 1.0 1.0 ~ 1.0 1.0 1.0) ... (1.0 1.0 1.0 ~ 1.0 1.0 1.0) (1.0 1.0 1.0 ~ 1.0 1.0 1.0)) :facet :exist :requires-grad NIL :backward NIL} [function] make-input (make-input shape named &key (created-from nil) (scalar-p nil) (dtype *default-dtype*) (order *default-order*)) Creates a new InputTensor. The allocation won't be done until the function (tensor-vec tensor) is called. In cl-waffe2, InputTensors can be applied for various things, for example, tracing the structure of computation node, used as a temporary tensor which can be pruned later by a compiler, as an argument of the computation node compiled by the build function. Inputs Shape [list] Set the shape of tensor. You can also use symbols if shapes can be changed later. The function set-input will update all symbols declared in the computation node, and accordingly, strides/shapes etc... will be also updated to minimise compiling-time overhead (use build and forward to do this). ScalarTensors aren't created by setting it= <<Something but not a list>> . Instead, set scalar-p=t . Named [keyword or null] Indicates the name of tensor. If set to keyword, This means the name of the argument when compiled into a function, which can be changed later. If set to nil, the name is filled with gensym indicating the index in the memory-pool. scalar-p [boolean] Set t to create a scalar. dtype [keyword] Set dtype. order [keyword] Set order. create-from[nil or AbstractTensor] The returned InputTensor will extend Permutions/Strides and so on from create-from if any. Example (make-input `(a 10) :train-x) {CPUTENSOR[float] :shape (A 10) :named :TRAIN-X <<Not allocated: size=(A 10)>> :facet :input :requires-grad NIL :backward NIL} Manipulating Gradients [parameter] *no-grad* Ensures that back-propagation is not invoked inside the scope for which this parameter is set to T, with the following effects: Save For Backward is forcibly ignored. Computational nodes for back propagation are not compiled. In default, set to nil. See also the with-no-grad macro to explict this state. [macro] with-no-grad (with-no-grad &body body) Set T to *no-grad* during the execution of body. [function] parameter (parameter tensor) Creates a new tensor with :requires-grad=t from the given tensor. If the tensor is remained to be computed, parameter will use the result from proceed . Example (parameter (randn `(3 3))) Building functions from AbstractTensor [class] Compiled-Composite Stores information on computation nodes compiled by the build function. The user has to guarantee that this point is the end of the computation node. Therefore, it is not possible in principle to continue the computation node after this point. Forward and backward propagation can be invoked using the forward and backward methods respectively. ;; Example (let ((model (build (!add 1 1)))) (forward model) (backward model)) This class furthermore records information on lazy-evaluated tensors. The tensor is an argument to the function, which can change the input via the set-input method. (let ((lazy-tensor (make-input `(10 10) :A))) (let ((model (build (!sum lazy-tensor)))) (set-input model :A (randn `(10 10))) ;; :A = (randn `(10 10)) (get-input model :A) (forward model))) By passing argument information to the compiler at build time, arguments can be given together when the forward method is called. (let ((a (make-input `(A B) :A)) (b (make-input `(A B) :B))) (let ((model (build (!mul a b) :inputs `(:A :B)))) (forward model (randn `(3 3)) (randn `(3 3))))) All tensors with :requires-grad=t , can be accessed by the (model-parameters model) method. [function] build (build toplevel &key (inputs nil) (construct-backward? (not *no-grad*)) (compile-mode :fastest) (fuse-ops t)) Compiles the given computation node starting from toplevel . The docstring of Compiled-Composite describes how this function are used in practical. Inputs toplevel [AbstractTensor] The end of node. Any shapes could be OK even when constructing backward. inputs[list] Set a list of argument keywords here so that the method forward can receive arguments that have been lazily evaluated. The order is taken into account. (e.g.: Set to (:A :B) and forward can receive this: (forward compiled-model (randn (3 3)) (randn (3 3))) ) construct-backward? [boolean] Set t to build backward. compile-mode [compile-mode-t] an keyword indicating the compiling option. (No significant impact on execution speed but compile speed. for any case :fastest is the best solution.) fuse-ops[boolean] Set to enable FusionOps declared by defpath . Example REPL: > (setq out (!add (make-input `(a 10) :X) (make-input `(a 10) :Y))) {CPUTENSOR[float] :shape (A 10) :id TID1878 :vec-state [maybe-not-computed] <<Not allocated: size=(A 10)>> :facet :input :belongs-to :memory-pool :requires-grad NIL :backward <Node: ADDNODE-CPUTENSOR (A[~] B[~] -> A[~])>} REPL: > (with-no-grad (build out :inputs `(:X :Y))) <Compiled-Composite(allocated-p=NIL) forward : forward(model X Y) -> CPUTENSOR{FLOAT}(A 10) backward : nil memory-pool : one tensor(s) L {4.0e-5+((A) x 4.0e-6)}MB inputs: X -> (A 10) Y -> (A 10) > [method] set-input (set-input (model Compiled-Composite) input-name actual-value) Embodies an InputTensor in the model. All unembodied tensors in the model can be accessed by printing the model. input-name could be a keyword indicating input-tensor, actual-value is a AbstractTensor whose facet = :exist (created by make-tensor ). [method] get-input (get-input (model Compiled-Composite) input-name) Reading all variables in the computation node, the method get-input returns an corresponding InputTensor of model. Creating a ranked function with computing views [function] call-with-view A principle operator to extend your functions to higher arrays. (call-with-view function tensors &key (at-least-dim 1) (force-order nil) (lparallel nil) (fuse nil)) The function call-with-view generates a lisp code of (loop for ...) iteration for nd-arrays, which follows the optimal route, is parallelized, and later composable. Since generating an optimal for(int i=0;i<size;i++){...} route according to the given rank of tensors is one of the main concerns of JIT Compiler for Deep Learning Framework, this function is usually combined with the forward definition of define-impl macro. It is later compiled to lambda functions and used as nodes in cl-waffe2 IR. In the simplest case, call-with-view first deploys (loop for...) until the rank of given tensors reaches the given at-least-dim . After reaching at-least-dim , the function places the result of calling the given function . (call-with-view #'(lambda (x-view) `(+ 1 1)) (list (randn `(100 100 100))) :at-least-dim 2) ;; will return: (CL-WAFFE2/VM.GENERIC-TENSOR::LET*-IGNORABLE ((#:G312057 0)) (LOCALLY (DECLARE (TYPE FIXNUM #:G312057)) (CL-WAFFE2/VM.GENERIC-TENSOR::LET*-IGNORABLE ((#:G312058 #:G312057)) (LOCALLY (DECLARE (TYPE FIXNUM #:G312058)) (LET* ((#:G312059 (NTH 0 (LIST 10000 100 1))) (#:G25 100) (#:G25 (CL-WAFFE2/VM.GENERIC-TENSOR::READ-ADJUSTABLE-SYMBOL #:G25))) (INCF #:G312058 (CL-WAFFE2/VM.GENERIC-TENSOR::%* 0 #:G312059)) (LOOP CL-WAFFE2/VM.GENERIC-TENSOR::FOR #:G312060 FIXNUM CL-WAFFE2/VM.GENERIC-TENSOR::UPFROM 0 CL-WAFFE2/VM.GENERIC-TENSOR::BELOW #:G25 DO (PROGN (CL-WAFFE2/VM.GENERIC-TENSOR::LET*-IGNORABLE ((#:G312061 #:G312058)) (LOCALLY (DECLARE (TYPE FIXNUM #:G312061)) (LET ((#:G312062 (THE FIXNUM (NTH 1 (LIST 10000 100 1))))) (INCF #:G312061 (CL-WAFFE2/VM.GENERIC-TENSOR::%* 0 #:G312062)) (+ 1 1))))) UNLESS (= #:G312060 (1- #:G25)) DO (PROGN (INCF (THE FIXNUM #:G312058) (THE FIXNUM #:G312059))))))))) Here, the number of tensors corresponds with the number of arguments function receive. Usually, the function receives information on the view of the tensor at the corresponding position: (size-of x-view) to get the number of iteration, (stride-of x-view) to get the number of increment, and, (offset-of x-view) to get the offset of tensor. (Sometimes they return s-expression because the shapes of tensors are not necessary number, but symbols.) function [function] should return a list which corresponds with invoking user-defined operation given views. tensors[a list of abstracttensor] tensors to be called with. at-least-dim [fixnum] at-least-dim is minimum rank value required by the operation. set 1 to define element-wise operation, set 2 to define gemm` for example. force-order[boolean] On some conditions, call-with-view shuffles the order of ranks, or flattens given tensors (e.g.: 100x100 tensors is the equivalent to just 10000x1 tensor on the memory). If you want to disable this behaviour, set force-order =t. lparallel[boolean] Set t to use lparallel. This should be denoted that under lparallel execution, the parameter cl-waffe2/threads:*under-multi-thread* becomes t. Use this parameter for the lowest rank operation to decide whether to parallelise. Return: Expanded Lisp Codes Note that call-with-view should be used at once or zero in the one define-impl forward. If you need twice times to call it, the general definition of AbstractNode should be split. See also: with-ranked-loop to the more elegant wrapping macro. [macro] with-ranked-loop (with-ranked-loop (((op-function &rest variables) &key (kernel-size 1) (shuffle-rank t) (lparallel nil) (fuse nil)) &body body)) Just an alias of call-with-view with this form: `(,@(call-with-view op-function variables :at-least-dim kernel-size :force-order (not shuffle-rank) :lparallel lparallel :fuse fuse) ,@body)","title":"cl-waffe2/vm.generic-tensor"},{"location":"generic-tensor/#abstracttensor","text":"","title":"AbstractTensor"},{"location":"generic-tensor/#working-with-abstracttensor","text":"","title":"Working with AbstractTensor"},{"location":"generic-tensor/#class-abstracttensor","text":"AbstractTensor is a CLOS class that Wraps existing data structures such as matrices in an abstract class in automatic differential programming using cl-waffe2, and further adds information about computation nodes, gradients, etc. Tensors can be created by the make-tensor function. (make-tensor `(3 3)) Plus, InputTensors (lazy-evaluated tensors), which is used to delay allocation timing, to use dynamic shaping, and to store the result, can be created by the make-input function. (make-input `(3 3) :A) ;; Set :A=nil to register as a temporary space. As an applied use, users can create new AbstractTensor that inherit from AbstractTensor. In addition, inheriting existing AbstractTensors (e.g.: LispTensor for CL Standard Array) allows reusing descriptions such as allocations. (defclass MyOriginalTensor (AbstractTensor) nil) (defclass MyCPUTensor (LispTensor) nil) Declare the priority of the device to be used with the with-devices macro. ;; Higher <-> Lower (with-devices (MyCPUTensor MyOriginalTensor CPUTensor) (make-tensor `(10 10))) All available devices can be accessed with the (show-backends) function, and they can only be used as devices together if they are shown to have an inheritance relationship. If a completely new Tensor is defined from AbstractTensor, cl-waffe2 can handle it completely in a fast form by writing the following additional information. Allocator: initialize-instance :before method Storage Accessor: vref and (setf vref) method Finalizer: tensor-finalizer method (Optional) Backend State: current-backend-state method (Optional) a cl-waffe2/vm:defpath macro to enable device-specific optimization. This is the simplest case of MyTensor which works on CL Standard Array. (defclass MyTensor (AbstractTensor) nil) ;; Allocators satisfy the following properties ;; 1. When facet is not `:exist`, do nothing. ;; 2. If `vec` is specified as an argument, use this, and do not allocate any tensors. ;; 3. Otherwise, allocate the tensor with: ;; 1. Dtype -> :dtype ;; 2. Size -> :shape (must be 1D on the memory) ;; 3. initial-element -> :initial-element (defmethod initialize-instance :before ((tensor MyTensor) &rest initargs &key &allow-other-keys) (let* ((shape (getf initargs :shape)) (dtype (dtype->lisp-type (getf initargs :dtype))) (vec (getf initargs :vec)) (facet (getf initargs :facet)) (initial-element (coerce (or (getf initargs :initial-element) 0) dtype))) (when (eql facet :exist) (if vec (setf (tensor-vec tensor) vec) (setf (tensor-vec tensor) (make-array (apply #'* shape) :element-type dtype :initial-element initial-element)))))) ;; vref reads the index th element of storage vec, this is must be a setfable. ;; Leave the annoying and complicated stride/offset computations to cl-waffe2! (defmethod vref ((tensor MyTensor) index) (declare (type fixnum index)) (aref (tensor-vec tensor) index)) (defmethod (setf vref) (new-value (tensor MyTensor) index) (declare (type fixnum index)) (setf (aref (tensor-vec tensor) index) new-value)) ;; The method should return a lambda function, if its storage vector isn't gc-reachable. ;; Finalizers are called when quitting (with-memory-pool ...) macro. (defmethod tensor-finalizer ((tensor MyTensor)) ;; Returning a dummy finalizer #'(lambda ())) ;; The function (show-backends) will display all devices and their information ;; If you want to put something, override this method and return a string. (defmethod current-backend-state ((backend-name (eql 'MyTensor))) \"Hello This is an demo\") ;; For FusionOp and defpath macro usage, see the :cl-waffe2/vm docs. MyTensor is now recognised as a usable device, so operations can be defined using the define-impl and define-impl-op macros.","title":"[class] AbstractTensor"},{"location":"generic-tensor/#function-shape","text":"(shape tensor) returns a visible shape of the given tensor.","title":"[function] shape"},{"location":"generic-tensor/#function-dims","text":"(dims tensor) returns a rank of the given tensor.","title":"[function] dims"},{"location":"generic-tensor/#function-total","text":"(total tensor) returns the number of total visible elements of the giventensor.","title":"[function] total"},{"location":"generic-tensor/#slot-orig-shape-list","text":"stores the shape of storage vec.","title":"[slot] orig-shape (List)"},{"location":"generic-tensor/#accessor-initial-offset-fixnum","text":"stores the offset of the tensor. In default, set to 0. Shape testing, for example, does not work, so use with caution. (tensor-initial-offset tensor)","title":"[accessor] initial-offset (fixnum)"},{"location":"generic-tensor/#slot-stride-list","text":"(tensor-stride tensor) stores the stride of tensor.","title":"[slot] stride (list)"},{"location":"generic-tensor/#slot-visible-shape-list","text":"(shape tensor)","title":"[slot] visible-shape (list)"},{"location":"generic-tensor/#slot-view-list","text":"Returns a list of ViewInstruction, created by the function (view tensor ...) or (!view tensor ...) to create a backward. (tensor-view tensor)","title":"[slot] view (list)"},{"location":"generic-tensor/#slot-projected-p-boolean","text":"Set t if (apply #'* orig-shape) == (apply #'* visible-shape) otherwise set nil. If t, the tensor is created by !view or view functions.","title":"[slot] projected-p (boolean)"},{"location":"generic-tensor/#slot-scalar-p","text":"Set t if the tensor should be represented as a scalar. In cl-waffe2, it's not a pretty thing but scalars are represented as a (apply #'* shape)=1 tensors. ranks are anything but for the most case, returns 1.","title":"[slot] scalar-p"},{"location":"generic-tensor/#slot-detach-p","text":"Set T to detach the tensor at a certain position.","title":"[slot] detach-p"},{"location":"generic-tensor/#slot-state","text":"(tensor-state tensor) stores StateContainer .","title":"[slot] state"},{"location":"generic-tensor/#slot-variables","text":"(tensor-variables tensor) stores the previous variables if the tensor is created by any operation.","title":"[slot] variables"},{"location":"generic-tensor/#slot-tensor-id-symbol","text":"Indicates where the Tensor is stored, (e.g. in a virtual machine). In-place operations inherit tensor-id from variables called with, and should not be used for topological sorting.","title":"[slot] tensor-id (symbol)"},{"location":"generic-tensor/#slot-tensor-iid-symbol","text":"It holds an ID that is guaranteed to be absolutely unique to the processing system generated by gensym. Used for topological sorting.","title":"[slot] tensor-iid (symbol)"},{"location":"generic-tensor/#slot-grad-abstracttensor","text":"If the tensor is created by (parameter ...) or with :requires-grad=t , (grad tensor) will return a gradient.","title":"[slot] grad (AbstractTensor)"},{"location":"generic-tensor/#slot-backward-abstractnode","text":"(tensor-backward tensor) returns a abstractnode if the tensor is created by any operation.","title":"[slot] backward (AbstractNode)"},{"location":"generic-tensor/#slot-requires-grad-boolean","text":"Set T to hold the gradients.","title":"[slot] requires-grad (Boolean)"},{"location":"generic-tensor/#slot-ancestor-param-p-boolean","text":"Set T if compilers can reach any tensors with :requires-grad=t , by tracing the tensor.","title":"[slot] ancestor-param-p (Boolean)"},{"location":"generic-tensor/#slot-flexible-p-fixnum-or-null","text":"Indicates the position of broadcastable axis.","title":"[slot] flexible-p (Fixnum or Null)"},{"location":"generic-tensor/#slot-facet-keyword","text":"AbstractTensors in cl-waffe2 has a two state: ExistTensor and InputTensor . ExistTensor is a just tensor with allocated storage vec, made by make-tensor function. On the other hand InputTensor is a lazy-evaluated tensor, allocation won't be done until it is needed. :exist to ExitTensor, :input to InputTensor.","title":"[slot] facet (keyword)"},{"location":"generic-tensor/#method-mref","text":"(mref tensor &rest subscripts) will reads a cetrain position of storage vec. This is setfable. In terms of performance, it is much faster way to edit a storage vec that using (change-facet) function and convert into other forms.","title":"[method] mref"},{"location":"generic-tensor/#hooking-optimizers-and-optimizing-parameters","text":"(TODO)","title":"Hooking Optimizers and Optimizing Parameters"},{"location":"generic-tensor/#function-hook-optimizer","text":"(hook-optimizer! tensor optimizer) Hooks the optimizer to the tensor.","title":"[function] hook-optimizer!"},{"location":"generic-tensor/#inputs","text":"tensor[AbstractTensor] optimizer[AbstractOptimizer]","title":"Inputs"},{"location":"generic-tensor/#function-call-optimizer","text":"(call-optimizer! tensor) Reading the (grad tensor) , the function invokes the optimizer hooked to the tensor.","title":"[function] call-optimizer!"},{"location":"generic-tensor/#function-reset-grad","text":"Resets the gradient of the tensor with zero with retain-grad=t .","title":"[function] reset-grad!"},{"location":"generic-tensor/#function-tensor-vec","text":"(tensor-vec tensor) If the given tensor is a ExistTensor, returns its storage vec. If the given tensor is a InputTensor, allocates the area for tensor and return its storage vec. This function is setfable and inlined.","title":"[function] tensor-vec"},{"location":"generic-tensor/#function-make-tensor","text":"(make-tensor shape-or-scalar &key (requires-grad nil) (dtype *default-dtype*) (view nil) (order *default-order*) (initial-element nil) (device nil)) Created a new ExistTensor of a device of (car *using-backend*) .","title":"[function] make-tensor"},{"location":"generic-tensor/#inputs_1","text":"shape-or-scalar [Anything] If set to list, creates a new matrix. Otherwise (e.g.: set to fixnum), creates a ScalarTensor. In that case, cl-waffe2 uses the highest priority device from *using-backends* parameter that inherits from the ScalarTensor class. requires-grad [Boolean] Set t to holds a gradients. (parameter tensor) will also do the same work. Under (with-no-grad ...) macro. This is set to nil forcibly. dtype [keyword] Set keyword indicating a type of elements. order [keyword] set keyword indicating the order of elments from :column or :row . in default set to :column . initial-element [Anything] Set anything which you want to set as a initial element. device[symbol or null] If set to symbol, the function returns with making a tensor of device.","title":"Inputs"},{"location":"generic-tensor/#example","text":"(make-tensor `(10 10) :initial-element 1.0) {CPUTENSOR[float] :shape (10 10) ((1.0 1.0 1.0 ~ 1.0 1.0 1.0) (1.0 1.0 1.0 ~ 1.0 1.0 1.0) ... (1.0 1.0 1.0 ~ 1.0 1.0 1.0) (1.0 1.0 1.0 ~ 1.0 1.0 1.0)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"generic-tensor/#function-make-input","text":"(make-input shape named &key (created-from nil) (scalar-p nil) (dtype *default-dtype*) (order *default-order*)) Creates a new InputTensor. The allocation won't be done until the function (tensor-vec tensor) is called. In cl-waffe2, InputTensors can be applied for various things, for example, tracing the structure of computation node, used as a temporary tensor which can be pruned later by a compiler, as an argument of the computation node compiled by the build function.","title":"[function] make-input"},{"location":"generic-tensor/#inputs_2","text":"Shape [list] Set the shape of tensor. You can also use symbols if shapes can be changed later. The function set-input will update all symbols declared in the computation node, and accordingly, strides/shapes etc... will be also updated to minimise compiling-time overhead (use build and forward to do this). ScalarTensors aren't created by setting it= <<Something but not a list>> . Instead, set scalar-p=t . Named [keyword or null] Indicates the name of tensor. If set to keyword, This means the name of the argument when compiled into a function, which can be changed later. If set to nil, the name is filled with gensym indicating the index in the memory-pool. scalar-p [boolean] Set t to create a scalar. dtype [keyword] Set dtype. order [keyword] Set order. create-from[nil or AbstractTensor] The returned InputTensor will extend Permutions/Strides and so on from create-from if any.","title":"Inputs"},{"location":"generic-tensor/#example_1","text":"(make-input `(a 10) :train-x) {CPUTENSOR[float] :shape (A 10) :named :TRAIN-X <<Not allocated: size=(A 10)>> :facet :input :requires-grad NIL :backward NIL}","title":"Example"},{"location":"generic-tensor/#manipulating-gradients","text":"","title":"Manipulating Gradients"},{"location":"generic-tensor/#parameter-no-grad","text":"Ensures that back-propagation is not invoked inside the scope for which this parameter is set to T, with the following effects: Save For Backward is forcibly ignored. Computational nodes for back propagation are not compiled. In default, set to nil. See also the with-no-grad macro to explict this state.","title":"[parameter] *no-grad*"},{"location":"generic-tensor/#macro-with-no-grad","text":"(with-no-grad &body body) Set T to *no-grad* during the execution of body.","title":"[macro] with-no-grad"},{"location":"generic-tensor/#function-parameter","text":"(parameter tensor) Creates a new tensor with :requires-grad=t from the given tensor. If the tensor is remained to be computed, parameter will use the result from proceed .","title":"[function] parameter"},{"location":"generic-tensor/#example_2","text":"(parameter (randn `(3 3)))","title":"Example"},{"location":"generic-tensor/#building-functions-from-abstracttensor","text":"","title":"Building functions from AbstractTensor"},{"location":"generic-tensor/#class-compiled-composite","text":"Stores information on computation nodes compiled by the build function. The user has to guarantee that this point is the end of the computation node. Therefore, it is not possible in principle to continue the computation node after this point. Forward and backward propagation can be invoked using the forward and backward methods respectively. ;; Example (let ((model (build (!add 1 1)))) (forward model) (backward model)) This class furthermore records information on lazy-evaluated tensors. The tensor is an argument to the function, which can change the input via the set-input method. (let ((lazy-tensor (make-input `(10 10) :A))) (let ((model (build (!sum lazy-tensor)))) (set-input model :A (randn `(10 10))) ;; :A = (randn `(10 10)) (get-input model :A) (forward model))) By passing argument information to the compiler at build time, arguments can be given together when the forward method is called. (let ((a (make-input `(A B) :A)) (b (make-input `(A B) :B))) (let ((model (build (!mul a b) :inputs `(:A :B)))) (forward model (randn `(3 3)) (randn `(3 3))))) All tensors with :requires-grad=t , can be accessed by the (model-parameters model) method.","title":"[class] Compiled-Composite"},{"location":"generic-tensor/#function-build","text":"(build toplevel &key (inputs nil) (construct-backward? (not *no-grad*)) (compile-mode :fastest) (fuse-ops t)) Compiles the given computation node starting from toplevel . The docstring of Compiled-Composite describes how this function are used in practical.","title":"[function] build"},{"location":"generic-tensor/#inputs_3","text":"toplevel [AbstractTensor] The end of node. Any shapes could be OK even when constructing backward. inputs[list] Set a list of argument keywords here so that the method forward can receive arguments that have been lazily evaluated. The order is taken into account. (e.g.: Set to (:A :B) and forward can receive this: (forward compiled-model (randn (3 3)) (randn (3 3))) ) construct-backward? [boolean] Set t to build backward. compile-mode [compile-mode-t] an keyword indicating the compiling option. (No significant impact on execution speed but compile speed. for any case :fastest is the best solution.) fuse-ops[boolean] Set to enable FusionOps declared by defpath .","title":"Inputs"},{"location":"generic-tensor/#example_3","text":"REPL: > (setq out (!add (make-input `(a 10) :X) (make-input `(a 10) :Y))) {CPUTENSOR[float] :shape (A 10) :id TID1878 :vec-state [maybe-not-computed] <<Not allocated: size=(A 10)>> :facet :input :belongs-to :memory-pool :requires-grad NIL :backward <Node: ADDNODE-CPUTENSOR (A[~] B[~] -> A[~])>} REPL: > (with-no-grad (build out :inputs `(:X :Y))) <Compiled-Composite(allocated-p=NIL) forward : forward(model X Y) -> CPUTENSOR{FLOAT}(A 10) backward : nil memory-pool : one tensor(s) L {4.0e-5+((A) x 4.0e-6)}MB inputs: X -> (A 10) Y -> (A 10) >","title":"Example"},{"location":"generic-tensor/#method-set-input","text":"(set-input (model Compiled-Composite) input-name actual-value) Embodies an InputTensor in the model. All unembodied tensors in the model can be accessed by printing the model. input-name could be a keyword indicating input-tensor, actual-value is a AbstractTensor whose facet = :exist (created by make-tensor ).","title":"[method] set-input"},{"location":"generic-tensor/#method-get-input","text":"(get-input (model Compiled-Composite) input-name) Reading all variables in the computation node, the method get-input returns an corresponding InputTensor of model.","title":"[method] get-input"},{"location":"generic-tensor/#creating-a-ranked-function-with-computing-views","text":"","title":"Creating a ranked function with computing views"},{"location":"generic-tensor/#function-call-with-view","text":"A principle operator to extend your functions to higher arrays. (call-with-view function tensors &key (at-least-dim 1) (force-order nil) (lparallel nil) (fuse nil)) The function call-with-view generates a lisp code of (loop for ...) iteration for nd-arrays, which follows the optimal route, is parallelized, and later composable. Since generating an optimal for(int i=0;i<size;i++){...} route according to the given rank of tensors is one of the main concerns of JIT Compiler for Deep Learning Framework, this function is usually combined with the forward definition of define-impl macro. It is later compiled to lambda functions and used as nodes in cl-waffe2 IR. In the simplest case, call-with-view first deploys (loop for...) until the rank of given tensors reaches the given at-least-dim . After reaching at-least-dim , the function places the result of calling the given function . (call-with-view #'(lambda (x-view) `(+ 1 1)) (list (randn `(100 100 100))) :at-least-dim 2) ;; will return: (CL-WAFFE2/VM.GENERIC-TENSOR::LET*-IGNORABLE ((#:G312057 0)) (LOCALLY (DECLARE (TYPE FIXNUM #:G312057)) (CL-WAFFE2/VM.GENERIC-TENSOR::LET*-IGNORABLE ((#:G312058 #:G312057)) (LOCALLY (DECLARE (TYPE FIXNUM #:G312058)) (LET* ((#:G312059 (NTH 0 (LIST 10000 100 1))) (#:G25 100) (#:G25 (CL-WAFFE2/VM.GENERIC-TENSOR::READ-ADJUSTABLE-SYMBOL #:G25))) (INCF #:G312058 (CL-WAFFE2/VM.GENERIC-TENSOR::%* 0 #:G312059)) (LOOP CL-WAFFE2/VM.GENERIC-TENSOR::FOR #:G312060 FIXNUM CL-WAFFE2/VM.GENERIC-TENSOR::UPFROM 0 CL-WAFFE2/VM.GENERIC-TENSOR::BELOW #:G25 DO (PROGN (CL-WAFFE2/VM.GENERIC-TENSOR::LET*-IGNORABLE ((#:G312061 #:G312058)) (LOCALLY (DECLARE (TYPE FIXNUM #:G312061)) (LET ((#:G312062 (THE FIXNUM (NTH 1 (LIST 10000 100 1))))) (INCF #:G312061 (CL-WAFFE2/VM.GENERIC-TENSOR::%* 0 #:G312062)) (+ 1 1))))) UNLESS (= #:G312060 (1- #:G25)) DO (PROGN (INCF (THE FIXNUM #:G312058) (THE FIXNUM #:G312059))))))))) Here, the number of tensors corresponds with the number of arguments function receive. Usually, the function receives information on the view of the tensor at the corresponding position: (size-of x-view) to get the number of iteration, (stride-of x-view) to get the number of increment, and, (offset-of x-view) to get the offset of tensor. (Sometimes they return s-expression because the shapes of tensors are not necessary number, but symbols.) function [function] should return a list which corresponds with invoking user-defined operation given views. tensors[a list of abstracttensor] tensors to be called with. at-least-dim [fixnum] at-least-dim is minimum rank value required by the operation. set 1 to define element-wise operation, set 2 to define gemm` for example. force-order[boolean] On some conditions, call-with-view shuffles the order of ranks, or flattens given tensors (e.g.: 100x100 tensors is the equivalent to just 10000x1 tensor on the memory). If you want to disable this behaviour, set force-order =t. lparallel[boolean] Set t to use lparallel. This should be denoted that under lparallel execution, the parameter cl-waffe2/threads:*under-multi-thread* becomes t. Use this parameter for the lowest rank operation to decide whether to parallelise. Return: Expanded Lisp Codes Note that call-with-view should be used at once or zero in the one define-impl forward. If you need twice times to call it, the general definition of AbstractNode should be split. See also: with-ranked-loop to the more elegant wrapping macro.","title":"[function] call-with-view"},{"location":"generic-tensor/#macro-with-ranked-loop","text":"(with-ranked-loop (((op-function &rest variables) &key (kernel-size 1) (shuffle-rank t) (lparallel nil) (fuse nil)) &body body)) Just an alias of call-with-view with this form: `(,@(call-with-view op-function variables :at-least-dim kernel-size :force-order (not shuffle-rank) :lparallel lparallel :fuse fuse) ,@body)","title":"[macro] with-ranked-loop"},{"location":"install/","text":"Setting up Environments If you're new to Common Lisp: 1. Installing Roswell Roswell is an environment manager for Common Lisp. https://github.com/roswell/roswell See the Readme.md and install Roswell 2. Installing Common Lisp Common Lisp has several implementations, but I personally recommended SBCL for its performance. If you've installed Roswell: $ ros install sbcl $ ros use <Installed SBCL Version> $ ros run # REPL is launched. should work and everything is done. 3. Setting up IDE (optional) The following editors are recommended as we're working with REPL: Emacs + Slime Lem Lem is an emacs-like text editor specialized on Common Lisp. Installing cl-waffe2 As of this writing(2023/9/13), cl-waffe2 is not yet available on Quicklisp. So I'm sorry but you have to install it manually. With roswell, the latest cl-wafe2 repository can be fetch like: $ ros install hikettei/cl-waffe2 In this case, you have to note that SBCL also needs to be started via Roswell. Another valid option would be loading cl-waffe2.asd file manually after cloning cl-waffe2 github repos: $ git clone https://github.com/hikettei/cl-waffe2.git $ cd ./cl-waffe2 $ ros run # start repl $ (load \"cl-waffe2.asd\") $ (ql:quickload :cl-waffe2) $ (in-pacakge :cl-waffe2-repl) # or make repl After you ensured it should work, move the ./cl-waffe2 directory to ~/quicklisp/local-projects/ and quicklisp can find the project! The get the full performance of cl-waffe2, you also have to do the following steps: Setting BLAS cl-waffe2 searches for and reads the libblas file by default. The following steps are only necessary if you get a warning when loading the library First, install the libopenblas # with ubuntu for example $ apt install libopenblas # With macOS $ brew instlal libopenblas Load the package again: $ ros run $ (load \"cl-waffe2.asd\") $ (ql:quickload :cl-waffe2) If you've got no warning after loading cl-waffe2, CPUTensor is successfully enabled and can recognize the OpenBLAS. If you still get warnings, you have to step an additional configs because cl-waffe2 couldn't find out the location. So, In your init file, (e.g.: ~/.roswell/init.lisp or ~/.sbclrc ), add the code below for example. (Change the path depending on your environment. You can find where you've installed the library with $ locate libblas for example of macOS). ;; In ~~/.sbclrc for example: (defparameter *cl-waffe-config* `((:libblas \\\"libblas.dylib for example\\\"))) It should work. If you still get warnings or encountered some problems, feel free to make an issue . Building SIMD Extension SIMD Extension is an extension for speeding up the execution of mathematical functions and some instructions (including sparse matrix) on the CPU. $ make build_simd_extension and everything is ok. Ensure that no warnings are displayed in your terminal after loaded cl-waffe2. Is GPU(CUDA/Metal/OpenCL etc...) supported? Currently, No. But cl-waffe2 is designed to be independent of which devices work on, and writing extension is easy. Personally, I don't have enough environment and equipment to do the test, so I plan to do it one day when I save up the money.","title":"Install"},{"location":"install/#setting-up-environments","text":"","title":"Setting up Environments"},{"location":"install/#if-youre-new-to-common-lisp","text":"","title":"If you're new to Common Lisp:"},{"location":"install/#1-installing-roswell","text":"Roswell is an environment manager for Common Lisp. https://github.com/roswell/roswell See the Readme.md and install Roswell","title":"1. Installing Roswell"},{"location":"install/#2-installing-common-lisp","text":"Common Lisp has several implementations, but I personally recommended SBCL for its performance. If you've installed Roswell: $ ros install sbcl $ ros use <Installed SBCL Version> $ ros run # REPL is launched. should work and everything is done.","title":"2. Installing Common Lisp"},{"location":"install/#3-setting-up-ide-optional","text":"The following editors are recommended as we're working with REPL: Emacs + Slime Lem Lem is an emacs-like text editor specialized on Common Lisp.","title":"3. Setting up IDE (optional)"},{"location":"install/#installing-cl-waffe2","text":"As of this writing(2023/9/13), cl-waffe2 is not yet available on Quicklisp. So I'm sorry but you have to install it manually. With roswell, the latest cl-wafe2 repository can be fetch like: $ ros install hikettei/cl-waffe2 In this case, you have to note that SBCL also needs to be started via Roswell. Another valid option would be loading cl-waffe2.asd file manually after cloning cl-waffe2 github repos: $ git clone https://github.com/hikettei/cl-waffe2.git $ cd ./cl-waffe2 $ ros run # start repl $ (load \"cl-waffe2.asd\") $ (ql:quickload :cl-waffe2) $ (in-pacakge :cl-waffe2-repl) # or make repl After you ensured it should work, move the ./cl-waffe2 directory to ~/quicklisp/local-projects/ and quicklisp can find the project! The get the full performance of cl-waffe2, you also have to do the following steps:","title":"Installing cl-waffe2"},{"location":"install/#setting-blas","text":"cl-waffe2 searches for and reads the libblas file by default. The following steps are only necessary if you get a warning when loading the library First, install the libopenblas # with ubuntu for example $ apt install libopenblas # With macOS $ brew instlal libopenblas Load the package again: $ ros run $ (load \"cl-waffe2.asd\") $ (ql:quickload :cl-waffe2) If you've got no warning after loading cl-waffe2, CPUTensor is successfully enabled and can recognize the OpenBLAS. If you still get warnings, you have to step an additional configs because cl-waffe2 couldn't find out the location. So, In your init file, (e.g.: ~/.roswell/init.lisp or ~/.sbclrc ), add the code below for example. (Change the path depending on your environment. You can find where you've installed the library with $ locate libblas for example of macOS). ;; In ~~/.sbclrc for example: (defparameter *cl-waffe-config* `((:libblas \\\"libblas.dylib for example\\\"))) It should work. If you still get warnings or encountered some problems, feel free to make an issue .","title":"Setting BLAS"},{"location":"install/#building-simd-extension","text":"SIMD Extension is an extension for speeding up the execution of mathematical functions and some instructions (including sparse matrix) on the CPU. $ make build_simd_extension and everything is ok. Ensure that no warnings are displayed in your terminal after loaded cl-waffe2.","title":"Building SIMD Extension"},{"location":"install/#is-gpucudametalopencl-etc-supported","text":"Currently, No. But cl-waffe2 is designed to be independent of which devices work on, and writing extension is easy. Personally, I don't have enough environment and equipment to do the test, so I plan to do it one day when I save up the money.","title":"Is GPU(CUDA/Metal/OpenCL etc...) supported?"},{"location":"lisp-tensor-backend/","text":"[package] :cl-waffe2/backends.lisp The package :cl-waffe2/backends.lisp provides an AbstractTensor LispTensor as an external backend, and designed with the aim of portalibity, not performance. Therefore, most implementations of this follow ANSI Common Lisp, so it will work in any environment but concerns remain about speed. It is recommended that LispTensor are installed in the lowest priority of *using-backend* , and Couldnt find any implementation for ... error will never occurs. [AbstractTensor] LispTensor","title":"cl-waffe2/backends.lisp"},{"location":"lisp-tensor-backend/#package-cl-waffe2backendslisp","text":"The package :cl-waffe2/backends.lisp provides an AbstractTensor LispTensor as an external backend, and designed with the aim of portalibity, not performance. Therefore, most implementations of this follow ANSI Common Lisp, so it will work in any environment but concerns remain about speed. It is recommended that LispTensor are installed in the lowest priority of *using-backend* , and Couldnt find any implementation for ... error will never occurs.","title":"[package] :cl-waffe2/backends.lisp"},{"location":"lisp-tensor-backend/#abstracttensor-lisptensor","text":"","title":"[AbstractTensor] LispTensor"},{"location":"nn/","text":".katex img { object-fit: fill; padding: unset; display: block; position: absolute; width: 100%; height: inherit; } cl-waffe2/nn [Non Linear Activations] [function] !relu (!relu x) Computes ReLU to the given tensor. R e L U ( x ) = m a x ( x , 0 ) ReLU(x) = max(x, 0) R e LU ( x ) = ma x ( x , 0 ) Example (proceed (!relu (randn `(10 10)))) {CPUTENSOR[float] :shape (10 10) :id TID1999 :vec-state [computed] ((0.43108767 0.41316718 -0.0 ~ 0.09826224 0.67178464 0.2550704) (0.3619366 1.0882165 0.1617515 ~ -0.0 -0.0 0.33450902) ... (1.0800804 0.7311244 -0.0 ~ -0.0 -0.0 0.004279087) (2.8531516 -0.0 0.24354811 ~ -0.0 -0.0 0.970088)) :facet :input :belongs-to :memory-pool :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} [function] !gelu (!gelu x) Applies the Gaussian Error Linear Units function approximated with: G e L U ( x ) = 0.5 \u00d7 x \u00d7 ( 1 + T a n h ( 2 \u03c0 \u00d7 ( x + 0.44715 \u00d7 x 3 ) ) ) GeLU(x) = 0.5\\times{x}\\times{(1 + Tanh(\\sqrt{\\frac{2}{\u03c0}}\\times{(x + 0.44715\\times{x^3})}))} G e LU ( x ) = 0.5 \u00d7 x \u00d7 ( 1 + T anh ( \u03c0 2 \u200b \u200b \u00d7 ( x + 0.44715 \u00d7 x 3 ) )) Example (proceed (!relu (randn `(10 10)))) {CPUTENSOR[float] :shape (10 10) :id TID2097 :vec-state [computed] ((-0.0 -0.0 -0.0 ~ 0.46306393 0.21459135 2.1423104) (-0.0 -0.0 -0.0 ~ -0.0 0.99525183 -0.0) ... (0.93948185 2.7282104 0.947536 ~ 1.4903362 0.11486413 0.42094356) (-0.0 0.35961413 0.40555617 ~ 0.78753096 0.10287541 1.0347279)) :facet :input :belongs-to :memory-pool :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} [function] !sigmoid (!sigmoid x) Computes sigmoid function to the given tensor. S i g m o i d ( x ) = 1 1 + e x p ( \u2212 x ) Sigmoid(x) = \\frac{1}{1 + exp(-x)} S i g m o i d ( x ) = 1 + e x p ( \u2212 x ) 1 \u200b Example (proceed (!sigmoid (randn `(10 10)))) {CPUTENSOR[float] :shape (10 10) :id TID2186 :vec-state [computed] ((0.1961688 0.82670623 0.92200094 ~ 0.62830925 0.5077358 0.89164513) (0.58502465 0.8223902 0.77587825 ~ 0.54009235 0.32129353 0.72657025) ... (0.25196192 0.6034778 0.47505894 ~ 0.40410277 0.28302354 0.73229575) (0.55372566 0.40291837 0.685333 ~ 0.682953 0.24996755 0.40710145)) :facet :input :belongs-to :memory-pool :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} [function] !leakey-relu (!leakey-relu x &key (negative-slope 0.01)) Applies the element-wise function: L e a k e y R e L U ( x ) = m a x ( x , 0 ) + n e g a t i v e \u2212 s l o p e \u00d7 m i n ( 0 , x ) LeakeyReLU(x) = max(x, 0) + negative-slope\\times{min(0, x)} L e ak ey R e LU ( x ) = ma x ( x , 0 ) + n e g a t i v e \u2212 s l o p e \u00d7 min ( 0 , x ) Inputs x[AbstractTensor] negative-slope[single-float] Example (proceed (!leakey-relu (randn `(10 10)))) {CPUTENSOR[float] :shape (10 10) :id TID2357 :vec-state [computed] ((0.11885163 0.18296687 -0.008493589 ~ -0.0044549606 -0.012962578 0.121423334) (-0.006803466 0.7299773 0.14457674 ~ 1.3988262 -0.0037990229 0.28851673) ... (1.3290814 -0.0049352995 -0.0063573807 ~ -0.009075811 1.2731968 -0.011155452) (-1.3661921e-5 1.6553823 -0.005161251 ~ -0.0032047757 0.13979843 0.5125472)) :facet :input :belongs-to :memory-pool :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} [function] !elu (!elu x &key (alpha 1.0)) Applies the Expotential Linear Units Function (ELUs) element-wise as described in this paper E L U ( x ) = { x i f x > 0 \u03b1*(exp(x)-1) otherwise \\begin{equation} ELU(x)= \\begin{cases} \\text{x} & if x>0 \\\\ \\text{\u03b1*(exp(x)-1)} & \\text{otherwise} \\end{cases} \\end{equation} E LU ( x ) = { x \u03b1*(exp(x)-1) \u200b i f x > 0 otherwise \u200b \u200b \u200b Example (proceed (!leakey-relu (randn `(10 10)))) {CPUTENSOR[float] :shape (10 10) :id TID2455 :vec-state [computed] ((-0.019835515 -0.0015778928 0.21411653 ~ -0.011082122 -0.0012673637 -0.009207103) (-0.014301519 0.8298885 -2.591463e-4 ~ -0.0024188054 0.28661168 -0.015332591) ... (0.41514573 0.43301943 -0.009638567 ~ 1.8616861 -0.0058236853 -0.0016050143) (-0.0034687081 0.9162214 -0.007410889 ~ -0.02770405 0.59393775 0.49283808)) :facet :input :belongs-to :memory-pool :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} [function] !softmax (!softmax x &key (avoid-overflow t) (axis 1)) Returns a tensor that applied Softmax function along the given axis. Softmax(x_i) = exp(x_i)\\div{sum(x_j, axis)} If avoid-overflow is set to t: x_i = x_i - mean(x) Inputs avoid-overflow[boolean] If t, exp(x_i) is substracted by the mean value of x . axis[fixnum or list or t] The axis to be reducted. Example (proceed (!softmax (randn `(3 3)))) {CPUTENSOR[float] :shape (3 3) :id TID2627 :vec-state [computed] ((0.16332655 0.33568394 0.5009895) (0.19607598 0.5174468 0.28647718) (0.17317529 0.6267547 0.20007004)) :facet :input :belongs-to :memory-pool :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} [Normalization Layers] [model] LAYERNORM (layernorm NORMALIZED-SHAPE &KEY (EPS 1.0e-5) (AFFINE T)) which transformation of shapes are defined as: (X[~ NORMALIZED-SHAPE] -> OUT[~ NORMALIZED-SHAPE]) Description Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization L a y e r N o r m ( x ) = x \u2212 E [ x ] V a r [ x ] + \u03b5 \u00d7 \u03b3 + \u03b2 LayerNorm(x) = \\frac{x - E[x]}{\\sqrt{Var[x] + \u03b5}}\\times{\u03b3}+\u03b2 L a yer N or m ( x ) = Va r [ x ] + \u03b5 \u200b x \u2212 E [ x ] \u200b \u00d7 \u03b3 + \u03b2 The mean and standard-deviation are calculated over the last D dimension where D = (length normalized-shape) . The parameters \u03b2 and \u03b3 are trainable affine transforms created if affine is set to T. Inputs normalized-shape [list or fixnum] the size of kernel eps[single-float] a value added to the denominator for the numerical stability. affine[boolean] Set T to use affine transformation. Parameters alpha (normalized-shape) is a trainable tensor filled with 1.0 . accessor: alpha-of beta (normalized-shape) is a trainable tensor filled with 0.0 . accessor: beta-of [Loss Functions] Tips: Utility Function The :reduction keyword for all loss functions is set to T by default. If you want to compose several functions for reduction (e.g. ->scal and !sum), it is recommended to define utilities such as: (defun criterion (criterion X Y &key (reductions nil)) (apply #'call-> (funcall criterion X Y) (map 'list #'asnode reductions))) ;; Without criterion: (->scal (MSE x y :reduction :sum)) ;; With criterion for example: (criterion #'MSE x y :reductions `(#'!sum #'->scal)) [function] L1Norm (L1Norm x p &key (:reduction t)) Returns a tensor that measures L1 Norm between each element in the input x and y . l ( x , y ) = L = l 1 , . . . , l n \u22ba , l n = a b s ( x n \u2212 y n ) l(x, y) = L = {l_1, ..., l_n}^\\intercal, l_n = abs(x_n - y_n) l ( x , y ) = L = l 1 \u200b , ... , l n \u200b \u22ba , l n \u200b = ab s ( x n \u200b \u2212 y n \u200b ) where N is a batch-size. In addition, reading the value of a :reduction keyword (one of :mean :sum t ), the result of L is reducted. (If t, reduction is ignored.) Example (proceed (L1Norm (randn `(10 10)) (randn `(10 10)))) {CPUTENSOR[float] :shape (10 10) :id TID2776 :vec-state [computed] ((1.0513756 0.1518357 0.8683155 ~ 0.5599009 0.27099022 0.44716904) (0.041606337 2.3360872 0.33524698 ~ 0.70283914 1.2479284 1.8440816) ... (0.24704999 1.5345023 1.0580499 ~ 0.07190052 1.2126788 0.17113328) (0.78247666 0.49038488 0.481458 ~ 2.7227192 1.29865 0.044514775)) :facet :input :belongs-to :memory-pool :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} [function] mse (mse x p &key (:reduction T)) Returns a tensor that measures the MSE error (i.e.: L2Norm) between each element in the input x and y . l ( x , y ) = L = l 1 , . . . , l n \u22ba , l n = ( x n \u2212 y n ) 2 l(x, y) = L = {l_1, ..., l_n}^\\intercal, l_n = (x_n - y_n)^2 l ( x , y ) = L = l 1 \u200b , ... , l n \u200b \u22ba , l n \u200b = ( x n \u200b \u2212 y n \u200b ) 2 where N is a batch-size. In addition, reading the value of a :reduction keyword (one of :mean :sum t ), the result of L is reducted. (If t, this operation is ignored.) Example (proceed (MSE (randn `(10 10)) (randn `(10 10)))) {CPUTENSOR[float] :shape (10 10) :id TID2882 :vec-state [computed] ((0.8048076 0.48694885 6.6639085 ~ 2.6626682 3.6498027 1.0817418) (0.661556 0.12334199 0.21250762 ~ 0.043855775 0.002835647 1.4583211) ... (1.3160336 0.45490843 0.20014359 ~ 6.1790547 0.24884623 0.116668575) (0.1191209 1.1257133 0.30800548 ~ 1.3009849 0.038676202 1.5365238)) :facet :input :belongs-to :memory-pool :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} [fucntion] cross-entropy-loss (cross-entropy-loss x labels &key (delta 1e-7) (reduction t)) Returns a tensor that measures the Cross-Entropy-Error between each element in the x and labels. L i = \u2212 p i l o g ( x i + d e l t a ) L_i = -p_ilog(x_i + delta) L i \u200b = \u2212 p i \u200b l o g ( x i \u200b + d e lt a ) o u t i = { s u m ( L ) reduction = sum m e a n ( L ) reduction = mean L otherwise \\begin{equation} out_i= \\begin{cases} sum(L) & \\text{reduction = sum} \\\\ mean(L) & \\text{reduction = mean} \\\\ L & \\text{otherwise} \\end{cases} \\end{equation} o u t i \u200b = \u23a9 \u23a8 \u23a7 \u200b s u m ( L ) m e an ( L ) L \u200b reduction = sum reduction = mean otherwise \u200b \u200b \u200b Inputs x[AbstractTensor] labels[AbstractTensor] one-hot encoding. reduction one of :sum :mean t [function] softmax-cross-entropy (softmax-cross-entropy x labels &key (axis 1) (delta 1e-7) (avoid-overflow nil) (reduction t)) Returns a tensor that measures the Softmax-Cross-Entropy-Error between each element in the x and labels. o u t = C r o s s E n t r o p y L o s s ( S o f t m a x ( x ) , l a b e l s ) out = CrossEntropyLoss(Softmax(x), labels) o u t = C ross E n t ro p y L oss ( S o f t ma x ( x ) , l ab e l s ) Inputs x[AbstractTensor] distribution to measure labels[AbstractTensor] answer labels with one-hot encoding. [Linear Layers] [model] LINEARLAYER (linearlayer IN-FEATURES OUT-FEATURES &OPTIONAL (USE-BIAS? T)) which transformation of shapes are defined as: ([~ BATCH-SIZE IN-FEATURES] -> [~ BATCH-SIZE OUT-FEATURES]) Description Applies a linear transformation to the incoming data. y = x A \u22ba + b y = xA^\\intercal + b y = x A \u22ba + b Inputs in-features[fixnum] size of each input size. out-features[fixnum] size of each output size. bias[boolean] If set to nil, the layer won't learn an additive bias. default: t Parameters (linear-weight self) the trainable value of the model of shape out_features * in_features . The values are sampled from xavier-uniform . (linear-bias self) the tranable value of bias of shape out_features . If bias is t, the initial values are sampled from uniform distribution: U(-k, k) where k = sqrt(1/out-features) . Example (LinearLayer 10 5) <Composite: LINEARLAYER{W2983}( <Input : ((~ BATCH-SIZE 10)) -> Output: ((~ BATCH-SIZE 5))> WEIGHTS -> (5 10) BIAS -> (5) )> [Dropout Layers] [Sparse Layers] [Recurrent Layers] [Convolutional Layers] [model] CONV2D (conv2d IN-CHANNELS OUT-CHANNELS KERNEL-SIZE &KEY (STRIDE 1) (PADDING 0) (DILATION 1) (GROUPS 1) (BIAS T) &AUX (STRIDE (MAYBE-TUPLE STRIDE 'STRIDE)) (KERNEL-SIZE (MAYBE-TUPLE KERNEL-SIZE 'KERNEL-SIZE)) (PADDING (MAYBE-TUPLE PADDING 'PADDING)) (DILATION (MAYBE-TUPLE DILATION 'DILATION))) which transformation of shapes are defined as: (INPUT[N C_IN H_IN W_IN] -> OUTPUT[N C_OUT H_OUT W_OUT] WHERE C_IN = IN-CHANNELS C_OUT = OUT-CHANNELS H_OUT = (IF (NUMBERP H_IN) (FLOOR (+ 1 (/ (+ H_IN (* 2 (CAR PADDING)) (* (- (CAR DILATION)) (- (CAR KERNEL-SIZE) 1)) -1) (CAR STRIDE)))) -1) W_OUT = (IF (NUMBERP W_IN) (FLOOR (+ 1 (/ (+ W_IN (* 2 (SECOND PADDING)) (* (- (SECOND DILATION)) (- (SECOND KERNEL-SIZE) 1)) -1) (SECOND STRIDE)))) -1)) Description Applies a 2D convolution over an input signal composed of several input planes. Inputs in-channels[fixnum] out-channels[fixnum] the number of channels. For example, if the input image is RGB, in-channels=3 . kernel-size[list (kernel-x kernel-y)] controls the size of kernel (e.g.: '(3 3) ). padding[fixnum or list] controls the amount of padding applies to the coming input. pads in X and Y direction when an integer value is entered. set a list of (pad-x pad-y) and pads in each direction. stride[fixnum or list] controls the stride for cross-correlation. As with padding , this parameter can be applied for each x/y axis. dilation[fixnum or list] controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. (currently not working, please set 1.) bias[boolean] Set t to use bias. Parameters Let k be (/ groups (* in-channels (apply #'* kernel-size))) . weight is a trainable parameter of (out-channels, in-channels / groups, kernel-size[0] kernel-size[1]) sampled from U(-sqrt(k), sqrt(k)) distribution, and can be accessed with weight-of . bias is a trainable parameter of (out-channels) sampled from U(-sqrt(k), sqrt(k)) distribution, and can be accessed with bias-of . Note: When Conv2D is initialised, the output is displayed as -1. This is because the calculation of the output is complicated and has been omitted. Once call is invoked, the output is recorded. Example (Conv2D 3 5 '(3 3)) <Composite: CONV2D{W2993}( <Input : ((N 3 H_IN W_IN)) -> Output: ((N 5 -1 -1))> WEIGHT -> (5 3 3 3) BIAS -> (5) )> Pooling Layers [model] MAXPOOL2D (maxpool2d KERNEL-SIZE &KEY (STRIDE KERNEL-SIZE) (PADDING 0) &AUX (STRIDE (MAYBE-TUPLE STRIDE 'STRIDE)) (PADDING (MAYBE-TUPLE PADDING 'PADDING))) which transformation of shapes are defined as: (INPUT[N C H_IN W_IN] -> OUTPUT[N C H_OUT W_OUT] WHERE H_OUT = (IF (NUMBERP H_IN) (POOL-OUT-SIZE H_IN (CAR PADDING) (CAR KERNEL-SIZE) (CAR STRIDE)) -1) W_OUT = (IF (NUMBERP W_OUT) (POOL-OUT-SIZE W_IN (SECOND PADDING) (SECOND KERNEL-SIZE) (SECOND STRIDE)) -1)) Description Applies a 2D max pooling over an input signal composed of several input planes. Inputs kernel-size[list] the size of window stride[fixnum or list] the stride of window padding[fixnum or list] adds 0 padding Likewise Conv2D , these parameters can be set for both X and Y axis directions. [model] AVGPOOL2D (avgpool2d KERNEL-SIZE &KEY (STRIDE KERNEL-SIZE) (PADDING 0) &AUX (STRIDE (MAYBE-TUPLE STRIDE 'STRIDE)) (PADDING (MAYBE-TUPLE PADDING 'PADDING))) which transformation of shapes are defined as: (INPUT[N C H_IN W_IN] -> OUTPUT[N C H_OUT W_OUT] WHERE H_OUT = (IF (NUMBERP H_IN) (POOL-OUT-SIZE H_IN (CAR PADDING) (CAR KERNEL-SIZE) (CAR STRIDE)) -1) W_OUT = (IF (NUMBERP W_OUT) (POOL-OUT-SIZE W_IN (SECOND PADDING) (SECOND KERNEL-SIZE) (SECOND STRIDE)) -1)) Description Applies a 2D average pooling over an input signal composed of several input planes. Inputs kernel-size[list] the size of window stride[fixnum or list] the stride of window padding[fixnum or list] adds 0 padding Likewise Conv2D , these parameters can be set for both X and Y axis directions. [function] unfold (unfold input dilation kernel-size stride padding) Extracts sliding local blocks from a batched input tensor. The detailed specifications follow PyTorch: nn.Unfold . As of this writing, input must be a 4D Tensor even when N=batch-size=1 . Corresponding nodes: cl-waffe2/base-impl:Im2ColNode , cl-waffe2/base-impl:Col2ImNode Inputs Note that dilation , kernel-size , stride , and padding are given in this form: (list y-direction(Height) x-direction(Width)) input[AbstractTensor] the tensor to be unfold. dilation[list] a parameter that controls the stride of elements within the neighborhood. kernel-size[list] the size of sliding blocks. padding[list] implicts the number of zero-padding to be added on both sides of input. stride[list] the number of stride of the sliding blocks.","title":"cl-waffe2/nn"},{"location":"nn/#cl-waffe2nn","text":"","title":"cl-waffe2/nn"},{"location":"nn/#non-linear-activations","text":"","title":"[Non Linear Activations]"},{"location":"nn/#function-relu","text":"(!relu x) Computes ReLU to the given tensor. R e L U ( x ) = m a x ( x , 0 ) ReLU(x) = max(x, 0) R e LU ( x ) = ma x ( x , 0 )","title":"[function] !relu"},{"location":"nn/#example","text":"(proceed (!relu (randn `(10 10)))) {CPUTENSOR[float] :shape (10 10) :id TID1999 :vec-state [computed] ((0.43108767 0.41316718 -0.0 ~ 0.09826224 0.67178464 0.2550704) (0.3619366 1.0882165 0.1617515 ~ -0.0 -0.0 0.33450902) ... (1.0800804 0.7311244 -0.0 ~ -0.0 -0.0 0.004279087) (2.8531516 -0.0 0.24354811 ~ -0.0 -0.0 0.970088)) :facet :input :belongs-to :memory-pool :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>}","title":"Example"},{"location":"nn/#function-gelu","text":"(!gelu x) Applies the Gaussian Error Linear Units function approximated with: G e L U ( x ) = 0.5 \u00d7 x \u00d7 ( 1 + T a n h ( 2 \u03c0 \u00d7 ( x + 0.44715 \u00d7 x 3 ) ) ) GeLU(x) = 0.5\\times{x}\\times{(1 + Tanh(\\sqrt{\\frac{2}{\u03c0}}\\times{(x + 0.44715\\times{x^3})}))} G e LU ( x ) = 0.5 \u00d7 x \u00d7 ( 1 + T anh ( \u03c0 2 \u200b \u200b \u00d7 ( x + 0.44715 \u00d7 x 3 ) ))","title":"[function] !gelu"},{"location":"nn/#example_1","text":"(proceed (!relu (randn `(10 10)))) {CPUTENSOR[float] :shape (10 10) :id TID2097 :vec-state [computed] ((-0.0 -0.0 -0.0 ~ 0.46306393 0.21459135 2.1423104) (-0.0 -0.0 -0.0 ~ -0.0 0.99525183 -0.0) ... (0.93948185 2.7282104 0.947536 ~ 1.4903362 0.11486413 0.42094356) (-0.0 0.35961413 0.40555617 ~ 0.78753096 0.10287541 1.0347279)) :facet :input :belongs-to :memory-pool :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>}","title":"Example"},{"location":"nn/#function-sigmoid","text":"(!sigmoid x) Computes sigmoid function to the given tensor. S i g m o i d ( x ) = 1 1 + e x p ( \u2212 x ) Sigmoid(x) = \\frac{1}{1 + exp(-x)} S i g m o i d ( x ) = 1 + e x p ( \u2212 x ) 1 \u200b","title":"[function] !sigmoid"},{"location":"nn/#example_2","text":"(proceed (!sigmoid (randn `(10 10)))) {CPUTENSOR[float] :shape (10 10) :id TID2186 :vec-state [computed] ((0.1961688 0.82670623 0.92200094 ~ 0.62830925 0.5077358 0.89164513) (0.58502465 0.8223902 0.77587825 ~ 0.54009235 0.32129353 0.72657025) ... (0.25196192 0.6034778 0.47505894 ~ 0.40410277 0.28302354 0.73229575) (0.55372566 0.40291837 0.685333 ~ 0.682953 0.24996755 0.40710145)) :facet :input :belongs-to :memory-pool :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>}","title":"Example"},{"location":"nn/#function-leakey-relu","text":"(!leakey-relu x &key (negative-slope 0.01)) Applies the element-wise function: L e a k e y R e L U ( x ) = m a x ( x , 0 ) + n e g a t i v e \u2212 s l o p e \u00d7 m i n ( 0 , x ) LeakeyReLU(x) = max(x, 0) + negative-slope\\times{min(0, x)} L e ak ey R e LU ( x ) = ma x ( x , 0 ) + n e g a t i v e \u2212 s l o p e \u00d7 min ( 0 , x )","title":"[function] !leakey-relu"},{"location":"nn/#inputs","text":"x[AbstractTensor] negative-slope[single-float]","title":"Inputs"},{"location":"nn/#example_3","text":"(proceed (!leakey-relu (randn `(10 10)))) {CPUTENSOR[float] :shape (10 10) :id TID2357 :vec-state [computed] ((0.11885163 0.18296687 -0.008493589 ~ -0.0044549606 -0.012962578 0.121423334) (-0.006803466 0.7299773 0.14457674 ~ 1.3988262 -0.0037990229 0.28851673) ... (1.3290814 -0.0049352995 -0.0063573807 ~ -0.009075811 1.2731968 -0.011155452) (-1.3661921e-5 1.6553823 -0.005161251 ~ -0.0032047757 0.13979843 0.5125472)) :facet :input :belongs-to :memory-pool :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>}","title":"Example"},{"location":"nn/#function-elu","text":"(!elu x &key (alpha 1.0)) Applies the Expotential Linear Units Function (ELUs) element-wise as described in this paper E L U ( x ) = { x i f x > 0 \u03b1*(exp(x)-1) otherwise \\begin{equation} ELU(x)= \\begin{cases} \\text{x} & if x>0 \\\\ \\text{\u03b1*(exp(x)-1)} & \\text{otherwise} \\end{cases} \\end{equation} E LU ( x ) = { x \u03b1*(exp(x)-1) \u200b i f x > 0 otherwise \u200b \u200b \u200b","title":"[function] !elu"},{"location":"nn/#example_4","text":"(proceed (!leakey-relu (randn `(10 10)))) {CPUTENSOR[float] :shape (10 10) :id TID2455 :vec-state [computed] ((-0.019835515 -0.0015778928 0.21411653 ~ -0.011082122 -0.0012673637 -0.009207103) (-0.014301519 0.8298885 -2.591463e-4 ~ -0.0024188054 0.28661168 -0.015332591) ... (0.41514573 0.43301943 -0.009638567 ~ 1.8616861 -0.0058236853 -0.0016050143) (-0.0034687081 0.9162214 -0.007410889 ~ -0.02770405 0.59393775 0.49283808)) :facet :input :belongs-to :memory-pool :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>}","title":"Example"},{"location":"nn/#function-softmax","text":"(!softmax x &key (avoid-overflow t) (axis 1)) Returns a tensor that applied Softmax function along the given axis. Softmax(x_i) = exp(x_i)\\div{sum(x_j, axis)} If avoid-overflow is set to t: x_i = x_i - mean(x)","title":"[function] !softmax"},{"location":"nn/#inputs_1","text":"avoid-overflow[boolean] If t, exp(x_i) is substracted by the mean value of x . axis[fixnum or list or t] The axis to be reducted.","title":"Inputs"},{"location":"nn/#example_5","text":"(proceed (!softmax (randn `(3 3)))) {CPUTENSOR[float] :shape (3 3) :id TID2627 :vec-state [computed] ((0.16332655 0.33568394 0.5009895) (0.19607598 0.5174468 0.28647718) (0.17317529 0.6267547 0.20007004)) :facet :input :belongs-to :memory-pool :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>}","title":"Example"},{"location":"nn/#normalization-layers","text":"","title":"[Normalization Layers]"},{"location":"nn/#model-layernorm","text":"(layernorm NORMALIZED-SHAPE &KEY (EPS 1.0e-5) (AFFINE T)) which transformation of shapes are defined as: (X[~ NORMALIZED-SHAPE] -> OUT[~ NORMALIZED-SHAPE])","title":"[model] LAYERNORM"},{"location":"nn/#description","text":"Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization L a y e r N o r m ( x ) = x \u2212 E [ x ] V a r [ x ] + \u03b5 \u00d7 \u03b3 + \u03b2 LayerNorm(x) = \\frac{x - E[x]}{\\sqrt{Var[x] + \u03b5}}\\times{\u03b3}+\u03b2 L a yer N or m ( x ) = Va r [ x ] + \u03b5 \u200b x \u2212 E [ x ] \u200b \u00d7 \u03b3 + \u03b2 The mean and standard-deviation are calculated over the last D dimension where D = (length normalized-shape) . The parameters \u03b2 and \u03b3 are trainable affine transforms created if affine is set to T.","title":"Description"},{"location":"nn/#inputs_2","text":"normalized-shape [list or fixnum] the size of kernel eps[single-float] a value added to the denominator for the numerical stability. affine[boolean] Set T to use affine transformation.","title":"Inputs"},{"location":"nn/#parameters","text":"alpha (normalized-shape) is a trainable tensor filled with 1.0 . accessor: alpha-of beta (normalized-shape) is a trainable tensor filled with 0.0 . accessor: beta-of","title":"Parameters"},{"location":"nn/#loss-functions","text":"","title":"[Loss Functions]"},{"location":"nn/#tips-utility-function","text":"The :reduction keyword for all loss functions is set to T by default. If you want to compose several functions for reduction (e.g. ->scal and !sum), it is recommended to define utilities such as: (defun criterion (criterion X Y &key (reductions nil)) (apply #'call-> (funcall criterion X Y) (map 'list #'asnode reductions))) ;; Without criterion: (->scal (MSE x y :reduction :sum)) ;; With criterion for example: (criterion #'MSE x y :reductions `(#'!sum #'->scal))","title":"Tips: Utility Function"},{"location":"nn/#function-l1norm","text":"(L1Norm x p &key (:reduction t)) Returns a tensor that measures L1 Norm between each element in the input x and y . l ( x , y ) = L = l 1 , . . . , l n \u22ba , l n = a b s ( x n \u2212 y n ) l(x, y) = L = {l_1, ..., l_n}^\\intercal, l_n = abs(x_n - y_n) l ( x , y ) = L = l 1 \u200b , ... , l n \u200b \u22ba , l n \u200b = ab s ( x n \u200b \u2212 y n \u200b ) where N is a batch-size. In addition, reading the value of a :reduction keyword (one of :mean :sum t ), the result of L is reducted. (If t, reduction is ignored.)","title":"[function] L1Norm"},{"location":"nn/#example_6","text":"(proceed (L1Norm (randn `(10 10)) (randn `(10 10)))) {CPUTENSOR[float] :shape (10 10) :id TID2776 :vec-state [computed] ((1.0513756 0.1518357 0.8683155 ~ 0.5599009 0.27099022 0.44716904) (0.041606337 2.3360872 0.33524698 ~ 0.70283914 1.2479284 1.8440816) ... (0.24704999 1.5345023 1.0580499 ~ 0.07190052 1.2126788 0.17113328) (0.78247666 0.49038488 0.481458 ~ 2.7227192 1.29865 0.044514775)) :facet :input :belongs-to :memory-pool :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>}","title":"Example"},{"location":"nn/#function-mse","text":"(mse x p &key (:reduction T)) Returns a tensor that measures the MSE error (i.e.: L2Norm) between each element in the input x and y . l ( x , y ) = L = l 1 , . . . , l n \u22ba , l n = ( x n \u2212 y n ) 2 l(x, y) = L = {l_1, ..., l_n}^\\intercal, l_n = (x_n - y_n)^2 l ( x , y ) = L = l 1 \u200b , ... , l n \u200b \u22ba , l n \u200b = ( x n \u200b \u2212 y n \u200b ) 2 where N is a batch-size. In addition, reading the value of a :reduction keyword (one of :mean :sum t ), the result of L is reducted. (If t, this operation is ignored.)","title":"[function] mse"},{"location":"nn/#example_7","text":"(proceed (MSE (randn `(10 10)) (randn `(10 10)))) {CPUTENSOR[float] :shape (10 10) :id TID2882 :vec-state [computed] ((0.8048076 0.48694885 6.6639085 ~ 2.6626682 3.6498027 1.0817418) (0.661556 0.12334199 0.21250762 ~ 0.043855775 0.002835647 1.4583211) ... (1.3160336 0.45490843 0.20014359 ~ 6.1790547 0.24884623 0.116668575) (0.1191209 1.1257133 0.30800548 ~ 1.3009849 0.038676202 1.5365238)) :facet :input :belongs-to :memory-pool :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>}","title":"Example"},{"location":"nn/#fucntion-cross-entropy-loss","text":"(cross-entropy-loss x labels &key (delta 1e-7) (reduction t)) Returns a tensor that measures the Cross-Entropy-Error between each element in the x and labels. L i = \u2212 p i l o g ( x i + d e l t a ) L_i = -p_ilog(x_i + delta) L i \u200b = \u2212 p i \u200b l o g ( x i \u200b + d e lt a ) o u t i = { s u m ( L ) reduction = sum m e a n ( L ) reduction = mean L otherwise \\begin{equation} out_i= \\begin{cases} sum(L) & \\text{reduction = sum} \\\\ mean(L) & \\text{reduction = mean} \\\\ L & \\text{otherwise} \\end{cases} \\end{equation} o u t i \u200b = \u23a9 \u23a8 \u23a7 \u200b s u m ( L ) m e an ( L ) L \u200b reduction = sum reduction = mean otherwise \u200b \u200b \u200b","title":"[fucntion] cross-entropy-loss"},{"location":"nn/#inputs_3","text":"x[AbstractTensor] labels[AbstractTensor] one-hot encoding. reduction one of :sum :mean t","title":"Inputs"},{"location":"nn/#function-softmax-cross-entropy","text":"(softmax-cross-entropy x labels &key (axis 1) (delta 1e-7) (avoid-overflow nil) (reduction t)) Returns a tensor that measures the Softmax-Cross-Entropy-Error between each element in the x and labels. o u t = C r o s s E n t r o p y L o s s ( S o f t m a x ( x ) , l a b e l s ) out = CrossEntropyLoss(Softmax(x), labels) o u t = C ross E n t ro p y L oss ( S o f t ma x ( x ) , l ab e l s )","title":"[function] softmax-cross-entropy"},{"location":"nn/#inputs_4","text":"x[AbstractTensor] distribution to measure labels[AbstractTensor] answer labels with one-hot encoding.","title":"Inputs"},{"location":"nn/#linear-layers","text":"","title":"[Linear Layers]"},{"location":"nn/#model-linearlayer","text":"(linearlayer IN-FEATURES OUT-FEATURES &OPTIONAL (USE-BIAS? T)) which transformation of shapes are defined as: ([~ BATCH-SIZE IN-FEATURES] -> [~ BATCH-SIZE OUT-FEATURES])","title":"[model] LINEARLAYER"},{"location":"nn/#description_1","text":"Applies a linear transformation to the incoming data. y = x A \u22ba + b y = xA^\\intercal + b y = x A \u22ba + b","title":"Description"},{"location":"nn/#inputs_5","text":"in-features[fixnum] size of each input size. out-features[fixnum] size of each output size. bias[boolean] If set to nil, the layer won't learn an additive bias. default: t","title":"Inputs"},{"location":"nn/#parameters_1","text":"(linear-weight self) the trainable value of the model of shape out_features * in_features . The values are sampled from xavier-uniform . (linear-bias self) the tranable value of bias of shape out_features . If bias is t, the initial values are sampled from uniform distribution: U(-k, k) where k = sqrt(1/out-features) .","title":"Parameters"},{"location":"nn/#example_8","text":"(LinearLayer 10 5) <Composite: LINEARLAYER{W2983}( <Input : ((~ BATCH-SIZE 10)) -> Output: ((~ BATCH-SIZE 5))> WEIGHTS -> (5 10) BIAS -> (5) )>","title":"Example"},{"location":"nn/#dropout-layers","text":"","title":"[Dropout Layers]"},{"location":"nn/#sparse-layers","text":"","title":"[Sparse Layers]"},{"location":"nn/#recurrent-layers","text":"","title":"[Recurrent Layers]"},{"location":"nn/#convolutional-layers","text":"","title":"[Convolutional Layers]"},{"location":"nn/#model-conv2d","text":"(conv2d IN-CHANNELS OUT-CHANNELS KERNEL-SIZE &KEY (STRIDE 1) (PADDING 0) (DILATION 1) (GROUPS 1) (BIAS T) &AUX (STRIDE (MAYBE-TUPLE STRIDE 'STRIDE)) (KERNEL-SIZE (MAYBE-TUPLE KERNEL-SIZE 'KERNEL-SIZE)) (PADDING (MAYBE-TUPLE PADDING 'PADDING)) (DILATION (MAYBE-TUPLE DILATION 'DILATION))) which transformation of shapes are defined as: (INPUT[N C_IN H_IN W_IN] -> OUTPUT[N C_OUT H_OUT W_OUT] WHERE C_IN = IN-CHANNELS C_OUT = OUT-CHANNELS H_OUT = (IF (NUMBERP H_IN) (FLOOR (+ 1 (/ (+ H_IN (* 2 (CAR PADDING)) (* (- (CAR DILATION)) (- (CAR KERNEL-SIZE) 1)) -1) (CAR STRIDE)))) -1) W_OUT = (IF (NUMBERP W_IN) (FLOOR (+ 1 (/ (+ W_IN (* 2 (SECOND PADDING)) (* (- (SECOND DILATION)) (- (SECOND KERNEL-SIZE) 1)) -1) (SECOND STRIDE)))) -1))","title":"[model] CONV2D"},{"location":"nn/#description_2","text":"Applies a 2D convolution over an input signal composed of several input planes.","title":"Description"},{"location":"nn/#inputs_6","text":"in-channels[fixnum] out-channels[fixnum] the number of channels. For example, if the input image is RGB, in-channels=3 . kernel-size[list (kernel-x kernel-y)] controls the size of kernel (e.g.: '(3 3) ). padding[fixnum or list] controls the amount of padding applies to the coming input. pads in X and Y direction when an integer value is entered. set a list of (pad-x pad-y) and pads in each direction. stride[fixnum or list] controls the stride for cross-correlation. As with padding , this parameter can be applied for each x/y axis. dilation[fixnum or list] controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. (currently not working, please set 1.) bias[boolean] Set t to use bias.","title":"Inputs"},{"location":"nn/#parameters_2","text":"Let k be (/ groups (* in-channels (apply #'* kernel-size))) . weight is a trainable parameter of (out-channels, in-channels / groups, kernel-size[0] kernel-size[1]) sampled from U(-sqrt(k), sqrt(k)) distribution, and can be accessed with weight-of . bias is a trainable parameter of (out-channels) sampled from U(-sqrt(k), sqrt(k)) distribution, and can be accessed with bias-of . Note: When Conv2D is initialised, the output is displayed as -1. This is because the calculation of the output is complicated and has been omitted. Once call is invoked, the output is recorded.","title":"Parameters"},{"location":"nn/#example_9","text":"(Conv2D 3 5 '(3 3)) <Composite: CONV2D{W2993}( <Input : ((N 3 H_IN W_IN)) -> Output: ((N 5 -1 -1))> WEIGHT -> (5 3 3 3) BIAS -> (5) )>","title":"Example"},{"location":"nn/#pooling-layers","text":"","title":"Pooling Layers"},{"location":"nn/#model-maxpool2d","text":"(maxpool2d KERNEL-SIZE &KEY (STRIDE KERNEL-SIZE) (PADDING 0) &AUX (STRIDE (MAYBE-TUPLE STRIDE 'STRIDE)) (PADDING (MAYBE-TUPLE PADDING 'PADDING))) which transformation of shapes are defined as: (INPUT[N C H_IN W_IN] -> OUTPUT[N C H_OUT W_OUT] WHERE H_OUT = (IF (NUMBERP H_IN) (POOL-OUT-SIZE H_IN (CAR PADDING) (CAR KERNEL-SIZE) (CAR STRIDE)) -1) W_OUT = (IF (NUMBERP W_OUT) (POOL-OUT-SIZE W_IN (SECOND PADDING) (SECOND KERNEL-SIZE) (SECOND STRIDE)) -1))","title":"[model] MAXPOOL2D"},{"location":"nn/#description_3","text":"Applies a 2D max pooling over an input signal composed of several input planes.","title":"Description"},{"location":"nn/#inputs_7","text":"kernel-size[list] the size of window stride[fixnum or list] the stride of window padding[fixnum or list] adds 0 padding Likewise Conv2D , these parameters can be set for both X and Y axis directions.","title":"Inputs"},{"location":"nn/#model-avgpool2d","text":"(avgpool2d KERNEL-SIZE &KEY (STRIDE KERNEL-SIZE) (PADDING 0) &AUX (STRIDE (MAYBE-TUPLE STRIDE 'STRIDE)) (PADDING (MAYBE-TUPLE PADDING 'PADDING))) which transformation of shapes are defined as: (INPUT[N C H_IN W_IN] -> OUTPUT[N C H_OUT W_OUT] WHERE H_OUT = (IF (NUMBERP H_IN) (POOL-OUT-SIZE H_IN (CAR PADDING) (CAR KERNEL-SIZE) (CAR STRIDE)) -1) W_OUT = (IF (NUMBERP W_OUT) (POOL-OUT-SIZE W_IN (SECOND PADDING) (SECOND KERNEL-SIZE) (SECOND STRIDE)) -1))","title":"[model] AVGPOOL2D"},{"location":"nn/#description_4","text":"Applies a 2D average pooling over an input signal composed of several input planes.","title":"Description"},{"location":"nn/#inputs_8","text":"kernel-size[list] the size of window stride[fixnum or list] the stride of window padding[fixnum or list] adds 0 padding Likewise Conv2D , these parameters can be set for both X and Y axis directions.","title":"Inputs"},{"location":"nn/#function-unfold","text":"(unfold input dilation kernel-size stride padding) Extracts sliding local blocks from a batched input tensor. The detailed specifications follow PyTorch: nn.Unfold . As of this writing, input must be a 4D Tensor even when N=batch-size=1 . Corresponding nodes: cl-waffe2/base-impl:Im2ColNode , cl-waffe2/base-impl:Col2ImNode","title":"[function] unfold"},{"location":"nn/#inputs_9","text":"Note that dilation , kernel-size , stride , and padding are given in this form: (list y-direction(Height) x-direction(Width)) input[AbstractTensor] the tensor to be unfold. dilation[list] a parameter that controls the stride of elements within the neighborhood. kernel-size[list] the size of sliding blocks. padding[list] implicts the number of zero-padding to be added on both sides of input. stride[list] the number of stride of the sliding blocks.","title":"Inputs"},{"location":"nodes/","text":"Formulating Computation Nodes The package :cl-waffe2/vm.nodes provides a features on AbstractNode and Composite , which is a fundamental data structure to represent computation node. AbstractNode is the smallest unit of the operation in the network, and Composite is a class which bundles several AbstractNodes ( Composite=nn.Module or Model in other frameworks). The role of node and model is completely different. To perform operations with AbstractNode we have to step a two steps: General Definition and Device Specific Implementations . AbstractNode is defined by the macro defnode with its specifications but forward implementation. The macro define-impl or define-impl-op will provide device-specific implementations for each AbstractTensor. The differences between define-impl and define-impl-op is that: The :forward definition is given by a macro or a function respectively. With define-impl macro and the call-with-view function, you can create a operation which optimizes, fuses, and collapses the order of iteration, schedules multi-threading, and computes view offsets in advance with a simple form. On the other hand, since Composite is user to bundle several nodes, it is not only used to juse represent Neural Network Model (e.g.: LinearLayer Conv2D ...) but compiled into function or AbstractNode with the defmodel-as macro by tracing its computation node declared in the :call-> method. However, to do this, you have to declare these information in advance: The rank/shape of tensors, the number of arguments, and which operations are In-place? . This is also true for AbstractNode and cl-waffe2 introduced an small DSL to represent this, Subscript DSL (:where ...) . In short word, Subscript DSL is used to: For AbstractNode, declares the transmission states of the operation (MUST) For Composite, in order to trace the network, it declares the transmission state of operation (Optional) Accordingly, this document is divided to three sections. The specification of Subscript DSL AbstractNode Composite And an overview of APIs is here: [AbstractNode] The fundamental unit of forward/backward propagations. defnode - Declares a general definition of AbstractNode L define-impl Implements a AbstractNode. Its forward definition is given as a macro (to inline/call-with-view), later (compile nil body) is called, and cached when :compile-when-cache=t. L define-impl-op Implements as a lambda function. define-op = defnode + define-impl-op [Composite] Bundles several AbstractNodes, defined by defmodel macro. defmodel - Defines a new Composite L defmodel-as Redefining the existing Composite as a function or AbstractNode to reduce compiling time, to use cl-waffe2 as a define-by-run library. cl-waffe2 VM sorts and compiles the network of AbstractNode into a cl-waffe2 IR (Extended Wengert List) and operations are performed. And, AbstractNode is used to represent an blueprint of lambda functions. Both of AbstractNode and Composite are the CLOS class. Representing shapes before and after the operation. When defining an operation in cl-waffe2 with a defnode macro, the shape of the matrix used in the operation must also be defined in the :where keyword. This is a Shaping API, and responsible for shape inspection of all operations, and tracing the network. Introducing Subscript DSL I assume you have already seen defnode macro. This macro takes a strange syntax language after :where keyword. (defnode (TransposeNode (myself) :where (A[~ i j] -> A[~ j i]) ...)) (defnode (ScalarAdd (myself) :where (A[~] Scal[scal] -> A[~] where scal = 1) ...)) (defnode (ReshapeNode (myself tensor after &aux (before (shape tensor))) :where (A[before] -> A[after]) ...)) This is a DSL (Domain Specific Language) called Subscript DSL , which is used to notate the pointer and shape to be handled before and after the operation. For example, TransposeNode is said to be: Before and after the operation, we use the same pointer. A is a tensor with more than two dimensions, and after the operation, transposed the last two axes. (i.e.: A=(10 5 2), (10 2 5) is returned) ScalarAdd is said to be: The first argument A can be anything. The second argument Scal is a scalar tensor. The returned tensor shares the pointer with the given A . ReshapeNode is: Before and after the operation, pointers are common. The shape of A will be transformed from before into after Basic Grammar Let's start with learning the grammar. One line code of Subscript DSL follows this format: [Before The Operation] -> [After The Operation] where [symbol = expression (Optional)] ... Note that: the pharse where [symbol = expression (Optional)] ... is Optional One Subscript DSL place can include one line of code. [Before The Operation] and [After The Operation] has the common grammar rule. Let <Arguments> be a grammar rule of [Before The Operation] and [After The Operation], <Arguments> can be defined as: <Arguments> ::= <Arguments> <Argument> <Argument> ::= <PointerName> [ <SubScripts> ] | NIL <PointerName> ::= Symbol // the same as CL's symbol. <SubScripts> ::= <Subscripts> <Subscript> <Subscript> ::= Symbol | NIL To put it bluntly, can be a sequence of: PointerName[SubScripts] // SubScripts can be one of: [A], [A B] [~ i j] etc... Assigned task A[a b] B[a b] -> B[a b] In the DSL above, A and B indicates the name of pointer, they're not needed to be defined in advance. On the other hand a and b inside [ ... ], indicates subscripts of A and B , DSL's assigned work is to inference these undetermined symbols from: determined symbol from where pharse and symbols in arguments of constructor. Shape of the given inputs at runtime. If any, DSL compiles and display a report on Shape-Error before performing the operation. (!add (randn `(3 2)) (randn `(2 4))) ;; will produce... [cl-waffe] Shaping-Error: Couldn't step forward because of shape-error. The operation was : <Node: ADDNODE-CPUTENSOR (A[~] B[~] -> A[~])> Input(s) : ((3 2) (2 4)) Predicted Output(s) : ((3 2)) Here's a list of reports. 1. Couldn't idenfity ~: ~ is determined as 3 butgot: 2. Excepted ~ = (3 2), butgot: (2 4) Also, these reports could be helpful for you (calculated ignoring the first errors.) 2. Couldn't idenfity ~: ~ is determined as 2 butgot: 4. Excepted ~ = (3 2), butgot: (2 4) Determine Rules (defnode (ExampleNode (myself) :where (A[~ i j] B[~ j k] C[~ k i] -> C[~ k i]) ...)) Symbols used in subscripts has a two state: Determined (those that can say i=1, j=2!) Undetermined (those that cannot say i=1, j=2) Before doing (call (ExampleNode) ...) , we create a table which stores determined/undetermined symbols and corresponding values. [TABLE] ~ -> ? // Undetermined before runtime i -> ? // Undetermined before runtime j -> ? // Undetermined before runtime k -> ? // Undetermined before runtime The moment we do (call (ExampleNode) TensorA TensorB TensorC) , we will be able to inference the value of i j k from the shape of given TensorA, TensorB, and TensorC. For Example, Let TensorA be a 2x3x4 Matrix, then the table become: [TABLE] ~ -> 2 i -> 3 j -> 4 k -> ? Then continue to do the same thing for TensorB. Let TensorB be a 2x4x9 Matrix, then the table become: [TABLE] ~ -> 2 i -> 3 j -> 4 k -> 9 Last, applying this operation into TensorC, but what if I gave the wrong shape to TensorC? Let TensorC be a 999x999x999 Matrix. (Obviously this is wrong). [TABLE] ~ -> 2 // \u2260999 i -> 3 // \u2260999 j -> 4 // \u2260999 k -> 9 // \u2260999 All subscripts in the table do not match with 999, resuting in shape-error. In that case, we can try again the operation with giving the correct shape to TensorC. Let TensorC be 2x9x3 Matrix. [TABLE] ~ -> 2 // =2 i -> 3 // = 3 j -> 4 // k -> 9 // = 9 All subscripts passed! (puts error If there's still undetermined symbol.) Using the determined table, we can also inference the shape of output tensor. The returned tensor is the shape of (~ k i) , that is, (2 9 3) . This operation can be done in a chain of lazy-evaluated nodes. Now, moving on to another topic, subscripts can be one of them. [TABLE] a = 1 // Fixnum b = `(1 2) // List consisted of fixnum ~ = `(1 2 3) // ~ is a special symbol which represents batched-input. DSL flattens the list in the subscript. (e.g.: b=(1 2) in A[b] is the equivalent to A[1 2] ) Note that ~ is a reserved word by cl-waffe2 and has a special rule: ~ is used to express dimensions from 0 to N ~ can only be used once for one input of subscript. In tables, ~ is interpreted as one of: NIL or List In addition, ~ has a three behaviour: If ~ never appears in [Before The Operation] and [After The Operation] parts, the length of ~ could be Any. If ~ appears more than once, the length of ~ and content should be common. If ~ appears only in [After The Operation], returns error because we can't determine ~. In conclusion, I believe introducing Subscript DSL produces two benefits: Rigorous Shape Inspection in all operations with small code, and produce better Shape-Error (Initially I'm inspired in: nalgebra ). JIT Compiler can use a shape of given arguments in advance. (If only CL has a const-generics like Rust, Subscript DSL isn't needed anymore!). Initial value of table In order to give a initial value to tables, you can declare symbols with initial value. Using where pharse in :where form Add this form to your :where form. ;; Syntax is that: Symbol-Name = Expression (defnode (... :where (A[i] B[j] -> C[k] where i = 1 j = 2 k = 3) .... will produce: [TABLE] i = 1 j = 2 k = 3 Using arguments declared in constructor . (defnode (ExampleNode (self i) :where (A[~] -> A[i])) ...) Arguments used in constructor, will automatically interpreted as initial value . (e.g.: i is a initial value.) [TABLE] ~ = ? i = i That is, when ExampleNode is initialized with (ExampleNode 3) , the table become: [TABLE] ~ = ? i = 3 arguments of constructor API: create-subscript-p (create-subscript-p subscripts &key macroexpand fixed return-body) Inputs: macroexpand[Boolean] If t, displays the generated program. fixed[Boolean] If t, ~ is ignored. return-body[Boolean] If t, the returned is S-exp. Outputs: (values compiled-function To-Refer-Pointer-Idx Broadcastable_List) Example: (TODO) AbstractNode [class] AbstractNode AbstractNode is a CLOS class to represent operations. Can be created by a function (AbstractName ...) declared by the defnode macro. In order to step the computation: (forward node arg1 arg2 ...) (using a call instead of forward is ok) And backward: (backward node prev-gradient arg1 arg2 ...) [macro] defnode (defnode (abstract-name (self &rest constructor-arguments) &key (where t) (out-scalar-p nil) (slots nil) (save-for-backward nil) (backward nil) (extends nil) (documentation \"\")) &body constructor-body) Declares a new AbstractNode . Effects defines a class (subclass of AbstractNode ) named abstract-name defines a fucntion which initializes the defined node. Inputs abstract-name [symbol] indicates the name of class, and constructor. extends[list] set a list of symbols, the class is defined with extending them. (self &rest constructor-arguments) declares the arguments of the constructor function, which cosntructor-body uses. slots[list] Describe the slots which node has as if defclass. Tips: In order to make it shorter to create a constructor, if initargs (i.e.: :initarg :XXX ) is the same as the keyword name of the argument, the initform is replaced with the argument. where[SubscriptDSL] Put here the Subscript DSL (MUST) out-scalar-p [Boolean] Set t if the node returns a ScalarTensor. backward [list] This form is optional. The backward receives arguments like: (dout var1 var2...) and return tensors which is lazy-evaluated. (See examples). You can set this form as nil, but in that case each define-impl and define-impl-op must have a backward slot. documentation [String] Example ;; Tips (defnode (ExampleNode (self arg) :slots ((arg :initarg :arg)))) (slot-value (ExampleNode 10) 'arg) ;; => 10 (defnode (MatMulNode-Revisit (myself dtype &key transpose-a transpose-b) :where (A[~ i j] B[~ j k] C[~ i k] -> C[~ i k]) :slots ((transpose-a :initarg :transpose-a :type boolean :reader trans-a?) (transpose-b :initarg :transpose-b :type boolean :reader trans-b?)) :backward ((self dout da db do) ;; dout=previous gradient, :save-for-backward is set to (t t nil). ;; so da/db is a copy of variable. (declare (ignore do)) ;; Set nil to the direction gradients aren't produced. (values (!matmul dout (!t db)) (!matmul (!t da) dout) nil)) :documentation \" C <- GEMM(1.0 A B 0.0 C) \")) You can invoke the forward/backward by using the method forward/backward. (forward node arg1 arg2...) (backward node dout1 arg1 arg2...) . [macro] define-impl (define-impl (abstract-name &key (device t) (extends nil) (cache-when-compiled t) (reject-p nil)) &key (save-for-backward nil) (forward nil) (backward nil)) Defines an implementation of abstract-name which is already declared by defnode macro, with :forward=macro and later compiled. Effects Defines a CLOS class named abstract-name-device extends abstract-name Inputs device [symbol or t] Set the name of AbstractTensor which the impl supports for. Set t to anything. extends [nil or list] In addition to extend abstract-name , the defined implementation will extends the given classses. cache-when-compiled [boolean] Set T to cache the forward definiton depending on dtypes, ranks, devices of arguments. You can set this to NIL but in terms of performance it is not recommended (runtime-compiling overhead is unignorable!) Instead, in that case, using define-impl-op would be nice. save-for-backward [list of boolean] For backward computation, the corresponding position of received variables will be produce a copy. You can check how it works with (disassemble-waffe2-ir toplevel) function and SV4BW(...) is exactly this. In backward, ((self dout x y) ...) will receive the copy. forward [body] Follows this format: ((self arg1 arg2 ...) <<macro-body>>) and the form must return S-expression later compiled by `(compile nil ...) backward [body] Follows this format: ((self prev-gradient arg1 arg2 ...) (values arg1.grad arg2.grad)) Note that the form is given by a function, and computation nodes are continuous. Not a macro. reject-p [nil or function] Set a lambda function returning nil or T. The function is called with arguments: (function constructor-args1 constructor-args2 ...) . In the case the function returned T, the method dispatching is ignored. You can use this method to ignore a certain dtype as a :forward arguments for example. [macro] define-impl-op Gives an implementation of abstract-name as a function form. (define-impl-op ((abstract-name &key (device t) (extends nil) (reject-p nil)) &key forward backward)) [macro] define-op define-op = defnode + define-impl-op Defines a differentiable AbstractNode which its definition is given by a function. (define-op (name (self &rest constructor-args) where slots out-scalar-p save-for-backward-names forward backward documentation extends) &body body) Effects This macro defines: two AbstractNodes named name and name-backward (if backward is given) Example (define-op (TestAdd-Scalar (self) :where (A[scal] B[scal] -> A[scal] where scal = 1) :out-scalar-p t :forward ((self a b) (make-tensor (+ (tensor-vec a) (tensor-vec b)))) :backward ((self dy) (values dy dy)))) [function] set-save-for-backward (set-save-for-backward self name tensor) The function set-save-for-backward saves the given tensor to the name slot of self for a future call of backward. This function is dedicated to the macro define-static-node , so it should be placed at the forward/backward definition of the macro, otherwise, the wrong function is binded which returns simple-error. In addition, The place to save the tensor, should be also declared in :save-for-backward-names in the define-static-node macro. Note that this function is ignored in specific conditions: *no-grad* is t or set-save-for-backward in the forward definition in the forward definition. (i.e.: the place which is never called.) See also: read-save-for-backward with-setting-sv4bw with-reading-sv4bw define-static-node [function] read-save-for-backward (read-save-for-backward self name) Reading the slot of name in self , the function read-save-for-backward returns a saved tensor by set-save-for-backward . For the same reason of set-save-for-backward , this function should be placed at right place. [macro] with-reading-save4bw (with-reading-save4bw ((&rest input-forms) &body body)) input-form = (variable-place save-for-backward-name) Reading the save-for-backward of currently working node, the macro binds each variable-place the stored tensor. [macro] with-setting-save4bw (with-setting-save4bw ((&rest input-forms) &body body)) input-form = (save-place tensor) Saves the given tensors to save-place, in the currently working node. Composite [class] Composite Its call bundles several AbstractNode. It is not only used to represent a neural network but also convert nodes into functions or abstractnodes. You can forward composites with (call composite arg1 arg2...) . For the most case, composites are defined by the defmodel macro. [generic] on-print-object (on-print-object model stream) This generic function is used to customize how the model is printed on the display. <Composite: NAME{...}( [...] <- The content of here is depends on on-print-object [PARAMTETERS] ) [macro] defmodel (defmodel ((name (self-name &rest constructor-arguments) &key (slots nil) (initargs) (where nil) (on-call-> nil) (documentation \"\")) &body constructor-body) Defines a composite named name , and constructor function which also named name and receives constructor-arguments as arguments. The main process of its forward process is described in the on-call-> slots. Inputs name[Symbol] the macro defines an class and constructor function named after it. (self-name &rest constructor-arguments) An initializer form of constructor function . slots ((slot-option1) (slot-option2) ...) Parameters of the inherited Composite class. It has the same syntax as defclass slots` initargs (:accessor-name1 accessor-init-form1 :accessor-name2 accessor-init-form2 ...) Unlike structures, CLOS classes are somewhat more cumbersome to initialise parameters. To make this process simple, put here initializer forms in advance likewise we do (make-instance class-name ...) . documentation[String] on-call-> [One of: nil symbol-name function list] The main proces of its forward process, later called with (call model ...) method. This method must be continuous from the given arguments. where[Subscript DSL] (Optional) If you're planning to use defmodel-as macro, this form is needed. Example (defmodel (ExampleLayer (self features) ;; Options/Utils Here, :slots ((param :initarg :param)) :initargs (:param (make-tensor `(,features) :requires-grad t)) :documentation \"ExampleLayer is a ...\") ;; After make-instance is called, the form below is called. ;; make-instance -> make-instance :after -> this form. (print self) ;; <- Initialized ExampleLayer (print features) ;; <- constructor-arguments are also used here. (print \"ExampleLayer is created!\")) ;; The model you created, works like: (let ((layer (ExampleLayer 10))) (call layer ...)) (defmodel (Softmax-Model (self) :where (X[~] -> [~]) :on-call-> ((self x) (declare (ignore self)) (let* ((x1 (!sub x (!mean x :axis 1 :keepdims t))) (z (!sum (!exp x1) :axis 1 :keepdims t))) (!div (!exp x1) z))))) ;; Keep Using Lazily... (proceed (call (Softmax-Model) (randn `(10 10))) {CPUTENSOR[float] :shape (10 10) :named ChainTMP33497 :vec-state [computed] ((0.04800622 0.118814774 0.050377533 ~ 0.053051848 0.050124187 0.25575548) (0.15909052 0.11368358 0.12642372 ~ 0.114795394 0.033397682 0.07605342) ... (0.035624444 0.24828684 0.109363265 ~ 0.020787988 0.027314318 0.04515641) (0.030307569 0.24117047 0.03900468 ~ 0.014522874 0.036584295 0.0971196)) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} (defmodel-as (Softmax-Model) :asif :function :named softmax-static) ;; No compiling overhead (softmax-static (randn `(10 10))) {CPUTENSOR[float] :shape (10 10) :named ChainTMP33788 ((0.16722792 0.018530384 0.014159603 ~ 0.035353966 0.06128503 0.13559735) (0.14498742 0.11881006 0.0692616 ~ 0.03911829 0.10358454 0.02131605) ... (0.055657785 0.44042623 0.030706322 ~ 0.11048273 0.0097645 0.11959953) (0.059088983 0.11067564 0.120767005 ~ 0.15042976 0.06570089 0.20548664)) :facet :input :requires-grad NIL :backward NIL} Dispatching on-call-> method on-call-> is nil In that case, users must define the call definiton manually like (defmethod call ((model YourComposite) arg1 arg2) ...) . on-call-> is symbol In that case, the composite invokes the method named symbol when call is invoked. ;; Set :on-call-> call-example-layer (defmethod call-example-layer ((model ExampleLayer) x y) (print \"call-example-layer is used!\")) (call (ExampleLayer 10) tensor) ;; call-example-layer is used! on-call-> is a list Directly defines a call method. Arguments must be: (self arg1 arg2...) ... :on-call-> ((self x) (!sin x)) [macro] defmodel-as (defmodel-as target-model &key (where nil) (asif :function) (named nil) (differentiable nil)) Redefines a Composite as a new function or AbstractNode specified in the :asif keyword. Further functions or Differentiable AbstractNode can be defined based on existing Composites (also called as model and defined by defmodel macro) which bundles several AbstractNodes , as long as :where form is fulfilled. Example (defmodel-as (SoftmaxNode) :named static-softmax :asif :function :where (A[~] -> A[~])) Inputs target-model[Composite] a form to initialize the composite. ~~This from is executed before running the code, and accordingly static.~~ where[Subscript DSL or null] If the model has no :where declaration, this macro uses this :where form instead. Therefore, as long as defmodel provides :where declaration, this form should be OK if set as nil. named[symbol] this macro will define a new function after named . If set to nil , the macro return a lambda function instead of defining it. If you're trying to define a new AbstractNode , this option should be fulfilled. :asif[keyword] indicates which form the target-model is to be redefined, and could be one of: \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 asif | description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 :function | Defines a function to be executed immediately that does not create a node. \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 :node | Defines a AbstractNode which needs to be compiled later \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Effects If named is not NIL , this macro defines a new function or AbstractNode after named . Notes Depending on the device and dtype used of arguments, several methods are compiled and dispatched. Events for Embedding JIT-Generated Code in runtime If the node is needed to be compiled, compile.NIL","title":"cl-waffe2/vm.nodes"},{"location":"nodes/#formulating-computation-nodes","text":"The package :cl-waffe2/vm.nodes provides a features on AbstractNode and Composite , which is a fundamental data structure to represent computation node. AbstractNode is the smallest unit of the operation in the network, and Composite is a class which bundles several AbstractNodes ( Composite=nn.Module or Model in other frameworks). The role of node and model is completely different. To perform operations with AbstractNode we have to step a two steps: General Definition and Device Specific Implementations . AbstractNode is defined by the macro defnode with its specifications but forward implementation. The macro define-impl or define-impl-op will provide device-specific implementations for each AbstractTensor. The differences between define-impl and define-impl-op is that: The :forward definition is given by a macro or a function respectively. With define-impl macro and the call-with-view function, you can create a operation which optimizes, fuses, and collapses the order of iteration, schedules multi-threading, and computes view offsets in advance with a simple form. On the other hand, since Composite is user to bundle several nodes, it is not only used to juse represent Neural Network Model (e.g.: LinearLayer Conv2D ...) but compiled into function or AbstractNode with the defmodel-as macro by tracing its computation node declared in the :call-> method. However, to do this, you have to declare these information in advance: The rank/shape of tensors, the number of arguments, and which operations are In-place? . This is also true for AbstractNode and cl-waffe2 introduced an small DSL to represent this, Subscript DSL (:where ...) . In short word, Subscript DSL is used to: For AbstractNode, declares the transmission states of the operation (MUST) For Composite, in order to trace the network, it declares the transmission state of operation (Optional) Accordingly, this document is divided to three sections. The specification of Subscript DSL AbstractNode Composite And an overview of APIs is here: [AbstractNode] The fundamental unit of forward/backward propagations. defnode - Declares a general definition of AbstractNode L define-impl Implements a AbstractNode. Its forward definition is given as a macro (to inline/call-with-view), later (compile nil body) is called, and cached when :compile-when-cache=t. L define-impl-op Implements as a lambda function. define-op = defnode + define-impl-op [Composite] Bundles several AbstractNodes, defined by defmodel macro. defmodel - Defines a new Composite L defmodel-as Redefining the existing Composite as a function or AbstractNode to reduce compiling time, to use cl-waffe2 as a define-by-run library. cl-waffe2 VM sorts and compiles the network of AbstractNode into a cl-waffe2 IR (Extended Wengert List) and operations are performed. And, AbstractNode is used to represent an blueprint of lambda functions. Both of AbstractNode and Composite are the CLOS class.","title":"Formulating Computation Nodes"},{"location":"nodes/#representing-shapes-before-and-after-the-operation","text":"When defining an operation in cl-waffe2 with a defnode macro, the shape of the matrix used in the operation must also be defined in the :where keyword. This is a Shaping API, and responsible for shape inspection of all operations, and tracing the network.","title":"Representing shapes before and after the operation."},{"location":"nodes/#introducing-subscript-dsl","text":"I assume you have already seen defnode macro. This macro takes a strange syntax language after :where keyword. (defnode (TransposeNode (myself) :where (A[~ i j] -> A[~ j i]) ...)) (defnode (ScalarAdd (myself) :where (A[~] Scal[scal] -> A[~] where scal = 1) ...)) (defnode (ReshapeNode (myself tensor after &aux (before (shape tensor))) :where (A[before] -> A[after]) ...)) This is a DSL (Domain Specific Language) called Subscript DSL , which is used to notate the pointer and shape to be handled before and after the operation. For example, TransposeNode is said to be: Before and after the operation, we use the same pointer. A is a tensor with more than two dimensions, and after the operation, transposed the last two axes. (i.e.: A=(10 5 2), (10 2 5) is returned) ScalarAdd is said to be: The first argument A can be anything. The second argument Scal is a scalar tensor. The returned tensor shares the pointer with the given A . ReshapeNode is: Before and after the operation, pointers are common. The shape of A will be transformed from before into after","title":"Introducing Subscript DSL"},{"location":"nodes/#basic-grammar","text":"Let's start with learning the grammar. One line code of Subscript DSL follows this format: [Before The Operation] -> [After The Operation] where [symbol = expression (Optional)] ... Note that: the pharse where [symbol = expression (Optional)] ... is Optional One Subscript DSL place can include one line of code. [Before The Operation] and [After The Operation] has the common grammar rule. Let <Arguments> be a grammar rule of [Before The Operation] and [After The Operation], <Arguments> can be defined as: <Arguments> ::= <Arguments> <Argument> <Argument> ::= <PointerName> [ <SubScripts> ] | NIL <PointerName> ::= Symbol // the same as CL's symbol. <SubScripts> ::= <Subscripts> <Subscript> <Subscript> ::= Symbol | NIL To put it bluntly, can be a sequence of: PointerName[SubScripts] // SubScripts can be one of: [A], [A B] [~ i j] etc...","title":"Basic Grammar"},{"location":"nodes/#assigned-task","text":"A[a b] B[a b] -> B[a b] In the DSL above, A and B indicates the name of pointer, they're not needed to be defined in advance. On the other hand a and b inside [ ... ], indicates subscripts of A and B , DSL's assigned work is to inference these undetermined symbols from: determined symbol from where pharse and symbols in arguments of constructor. Shape of the given inputs at runtime. If any, DSL compiles and display a report on Shape-Error before performing the operation. (!add (randn `(3 2)) (randn `(2 4))) ;; will produce... [cl-waffe] Shaping-Error: Couldn't step forward because of shape-error. The operation was : <Node: ADDNODE-CPUTENSOR (A[~] B[~] -> A[~])> Input(s) : ((3 2) (2 4)) Predicted Output(s) : ((3 2)) Here's a list of reports. 1. Couldn't idenfity ~: ~ is determined as 3 butgot: 2. Excepted ~ = (3 2), butgot: (2 4) Also, these reports could be helpful for you (calculated ignoring the first errors.) 2. Couldn't idenfity ~: ~ is determined as 2 butgot: 4. Excepted ~ = (3 2), butgot: (2 4)","title":"Assigned task"},{"location":"nodes/#determine-rules","text":"(defnode (ExampleNode (myself) :where (A[~ i j] B[~ j k] C[~ k i] -> C[~ k i]) ...)) Symbols used in subscripts has a two state: Determined (those that can say i=1, j=2!) Undetermined (those that cannot say i=1, j=2) Before doing (call (ExampleNode) ...) , we create a table which stores determined/undetermined symbols and corresponding values. [TABLE] ~ -> ? // Undetermined before runtime i -> ? // Undetermined before runtime j -> ? // Undetermined before runtime k -> ? // Undetermined before runtime The moment we do (call (ExampleNode) TensorA TensorB TensorC) , we will be able to inference the value of i j k from the shape of given TensorA, TensorB, and TensorC. For Example, Let TensorA be a 2x3x4 Matrix, then the table become: [TABLE] ~ -> 2 i -> 3 j -> 4 k -> ? Then continue to do the same thing for TensorB. Let TensorB be a 2x4x9 Matrix, then the table become: [TABLE] ~ -> 2 i -> 3 j -> 4 k -> 9 Last, applying this operation into TensorC, but what if I gave the wrong shape to TensorC? Let TensorC be a 999x999x999 Matrix. (Obviously this is wrong). [TABLE] ~ -> 2 // \u2260999 i -> 3 // \u2260999 j -> 4 // \u2260999 k -> 9 // \u2260999 All subscripts in the table do not match with 999, resuting in shape-error. In that case, we can try again the operation with giving the correct shape to TensorC. Let TensorC be 2x9x3 Matrix. [TABLE] ~ -> 2 // =2 i -> 3 // = 3 j -> 4 // k -> 9 // = 9 All subscripts passed! (puts error If there's still undetermined symbol.) Using the determined table, we can also inference the shape of output tensor. The returned tensor is the shape of (~ k i) , that is, (2 9 3) . This operation can be done in a chain of lazy-evaluated nodes. Now, moving on to another topic, subscripts can be one of them. [TABLE] a = 1 // Fixnum b = `(1 2) // List consisted of fixnum ~ = `(1 2 3) // ~ is a special symbol which represents batched-input. DSL flattens the list in the subscript. (e.g.: b=(1 2) in A[b] is the equivalent to A[1 2] ) Note that ~ is a reserved word by cl-waffe2 and has a special rule: ~ is used to express dimensions from 0 to N ~ can only be used once for one input of subscript. In tables, ~ is interpreted as one of: NIL or List In addition, ~ has a three behaviour: If ~ never appears in [Before The Operation] and [After The Operation] parts, the length of ~ could be Any. If ~ appears more than once, the length of ~ and content should be common. If ~ appears only in [After The Operation], returns error because we can't determine ~. In conclusion, I believe introducing Subscript DSL produces two benefits: Rigorous Shape Inspection in all operations with small code, and produce better Shape-Error (Initially I'm inspired in: nalgebra ). JIT Compiler can use a shape of given arguments in advance. (If only CL has a const-generics like Rust, Subscript DSL isn't needed anymore!).","title":"Determine Rules"},{"location":"nodes/#initial-value-of-table","text":"In order to give a initial value to tables, you can declare symbols with initial value. Using where pharse in :where form Add this form to your :where form. ;; Syntax is that: Symbol-Name = Expression (defnode (... :where (A[i] B[j] -> C[k] where i = 1 j = 2 k = 3) .... will produce: [TABLE] i = 1 j = 2 k = 3 Using arguments declared in constructor . (defnode (ExampleNode (self i) :where (A[~] -> A[i])) ...) Arguments used in constructor, will automatically interpreted as initial value . (e.g.: i is a initial value.) [TABLE] ~ = ? i = i That is, when ExampleNode is initialized with (ExampleNode 3) , the table become: [TABLE] ~ = ? i = 3 arguments of constructor","title":"Initial value of table"},{"location":"nodes/#api-create-subscript-p","text":"(create-subscript-p subscripts &key macroexpand fixed return-body) Inputs: macroexpand[Boolean] If t, displays the generated program. fixed[Boolean] If t, ~ is ignored. return-body[Boolean] If t, the returned is S-exp. Outputs: (values compiled-function To-Refer-Pointer-Idx Broadcastable_List) Example: (TODO)","title":"API: create-subscript-p"},{"location":"nodes/#abstractnode","text":"","title":"AbstractNode"},{"location":"nodes/#class-abstractnode","text":"AbstractNode is a CLOS class to represent operations. Can be created by a function (AbstractName ...) declared by the defnode macro. In order to step the computation: (forward node arg1 arg2 ...) (using a call instead of forward is ok) And backward: (backward node prev-gradient arg1 arg2 ...)","title":"[class] AbstractNode"},{"location":"nodes/#macro-defnode","text":"(defnode (abstract-name (self &rest constructor-arguments) &key (where t) (out-scalar-p nil) (slots nil) (save-for-backward nil) (backward nil) (extends nil) (documentation \"\")) &body constructor-body) Declares a new AbstractNode .","title":"[macro] defnode"},{"location":"nodes/#effects","text":"defines a class (subclass of AbstractNode ) named abstract-name defines a fucntion which initializes the defined node.","title":"Effects"},{"location":"nodes/#inputs","text":"abstract-name [symbol] indicates the name of class, and constructor. extends[list] set a list of symbols, the class is defined with extending them. (self &rest constructor-arguments) declares the arguments of the constructor function, which cosntructor-body uses. slots[list] Describe the slots which node has as if defclass. Tips: In order to make it shorter to create a constructor, if initargs (i.e.: :initarg :XXX ) is the same as the keyword name of the argument, the initform is replaced with the argument. where[SubscriptDSL] Put here the Subscript DSL (MUST) out-scalar-p [Boolean] Set t if the node returns a ScalarTensor. backward [list] This form is optional. The backward receives arguments like: (dout var1 var2...) and return tensors which is lazy-evaluated. (See examples). You can set this form as nil, but in that case each define-impl and define-impl-op must have a backward slot. documentation [String]","title":"Inputs"},{"location":"nodes/#example","text":";; Tips (defnode (ExampleNode (self arg) :slots ((arg :initarg :arg)))) (slot-value (ExampleNode 10) 'arg) ;; => 10 (defnode (MatMulNode-Revisit (myself dtype &key transpose-a transpose-b) :where (A[~ i j] B[~ j k] C[~ i k] -> C[~ i k]) :slots ((transpose-a :initarg :transpose-a :type boolean :reader trans-a?) (transpose-b :initarg :transpose-b :type boolean :reader trans-b?)) :backward ((self dout da db do) ;; dout=previous gradient, :save-for-backward is set to (t t nil). ;; so da/db is a copy of variable. (declare (ignore do)) ;; Set nil to the direction gradients aren't produced. (values (!matmul dout (!t db)) (!matmul (!t da) dout) nil)) :documentation \" C <- GEMM(1.0 A B 0.0 C) \")) You can invoke the forward/backward by using the method forward/backward. (forward node arg1 arg2...) (backward node dout1 arg1 arg2...) .","title":"Example"},{"location":"nodes/#macro-define-impl","text":"(define-impl (abstract-name &key (device t) (extends nil) (cache-when-compiled t) (reject-p nil)) &key (save-for-backward nil) (forward nil) (backward nil)) Defines an implementation of abstract-name which is already declared by defnode macro, with :forward=macro and later compiled.","title":"[macro] define-impl"},{"location":"nodes/#effects_1","text":"Defines a CLOS class named abstract-name-device extends abstract-name","title":"Effects"},{"location":"nodes/#inputs_1","text":"device [symbol or t] Set the name of AbstractTensor which the impl supports for. Set t to anything. extends [nil or list] In addition to extend abstract-name , the defined implementation will extends the given classses. cache-when-compiled [boolean] Set T to cache the forward definiton depending on dtypes, ranks, devices of arguments. You can set this to NIL but in terms of performance it is not recommended (runtime-compiling overhead is unignorable!) Instead, in that case, using define-impl-op would be nice. save-for-backward [list of boolean] For backward computation, the corresponding position of received variables will be produce a copy. You can check how it works with (disassemble-waffe2-ir toplevel) function and SV4BW(...) is exactly this. In backward, ((self dout x y) ...) will receive the copy. forward [body] Follows this format: ((self arg1 arg2 ...) <<macro-body>>) and the form must return S-expression later compiled by `(compile nil ...) backward [body] Follows this format: ((self prev-gradient arg1 arg2 ...) (values arg1.grad arg2.grad)) Note that the form is given by a function, and computation nodes are continuous. Not a macro. reject-p [nil or function] Set a lambda function returning nil or T. The function is called with arguments: (function constructor-args1 constructor-args2 ...) . In the case the function returned T, the method dispatching is ignored. You can use this method to ignore a certain dtype as a :forward arguments for example.","title":"Inputs"},{"location":"nodes/#macro-define-impl-op","text":"Gives an implementation of abstract-name as a function form. (define-impl-op ((abstract-name &key (device t) (extends nil) (reject-p nil)) &key forward backward))","title":"[macro] define-impl-op"},{"location":"nodes/#macro-define-op","text":"define-op = defnode + define-impl-op Defines a differentiable AbstractNode which its definition is given by a function. (define-op (name (self &rest constructor-args) where slots out-scalar-p save-for-backward-names forward backward documentation extends) &body body)","title":"[macro] define-op"},{"location":"nodes/#effects_2","text":"This macro defines: two AbstractNodes named name and name-backward (if backward is given)","title":"Effects"},{"location":"nodes/#example_1","text":"(define-op (TestAdd-Scalar (self) :where (A[scal] B[scal] -> A[scal] where scal = 1) :out-scalar-p t :forward ((self a b) (make-tensor (+ (tensor-vec a) (tensor-vec b)))) :backward ((self dy) (values dy dy))))","title":"Example"},{"location":"nodes/#function-set-save-for-backward","text":"(set-save-for-backward self name tensor) The function set-save-for-backward saves the given tensor to the name slot of self for a future call of backward. This function is dedicated to the macro define-static-node , so it should be placed at the forward/backward definition of the macro, otherwise, the wrong function is binded which returns simple-error. In addition, The place to save the tensor, should be also declared in :save-for-backward-names in the define-static-node macro. Note that this function is ignored in specific conditions: *no-grad* is t or set-save-for-backward in the forward definition in the forward definition. (i.e.: the place which is never called.) See also: read-save-for-backward with-setting-sv4bw with-reading-sv4bw define-static-node","title":"[function] set-save-for-backward"},{"location":"nodes/#function-read-save-for-backward","text":"(read-save-for-backward self name) Reading the slot of name in self , the function read-save-for-backward returns a saved tensor by set-save-for-backward . For the same reason of set-save-for-backward , this function should be placed at right place.","title":"[function] read-save-for-backward"},{"location":"nodes/#macro-with-reading-save4bw","text":"(with-reading-save4bw ((&rest input-forms) &body body)) input-form = (variable-place save-for-backward-name) Reading the save-for-backward of currently working node, the macro binds each variable-place the stored tensor.","title":"[macro] with-reading-save4bw"},{"location":"nodes/#macro-with-setting-save4bw","text":"(with-setting-save4bw ((&rest input-forms) &body body)) input-form = (save-place tensor) Saves the given tensors to save-place, in the currently working node.","title":"[macro] with-setting-save4bw"},{"location":"nodes/#composite","text":"","title":"Composite"},{"location":"nodes/#class-composite","text":"Its call bundles several AbstractNode. It is not only used to represent a neural network but also convert nodes into functions or abstractnodes. You can forward composites with (call composite arg1 arg2...) . For the most case, composites are defined by the defmodel macro.","title":"[class] Composite"},{"location":"nodes/#generic-on-print-object","text":"(on-print-object model stream) This generic function is used to customize how the model is printed on the display. <Composite: NAME{...}( [...] <- The content of here is depends on on-print-object [PARAMTETERS] )","title":"[generic] on-print-object"},{"location":"nodes/#macro-defmodel","text":"(defmodel ((name (self-name &rest constructor-arguments) &key (slots nil) (initargs) (where nil) (on-call-> nil) (documentation \"\")) &body constructor-body) Defines a composite named name , and constructor function which also named name and receives constructor-arguments as arguments. The main process of its forward process is described in the on-call-> slots.","title":"[macro] defmodel"},{"location":"nodes/#inputs_2","text":"name[Symbol] the macro defines an class and constructor function named after it. (self-name &rest constructor-arguments) An initializer form of constructor function . slots ((slot-option1) (slot-option2) ...) Parameters of the inherited Composite class. It has the same syntax as defclass slots` initargs (:accessor-name1 accessor-init-form1 :accessor-name2 accessor-init-form2 ...) Unlike structures, CLOS classes are somewhat more cumbersome to initialise parameters. To make this process simple, put here initializer forms in advance likewise we do (make-instance class-name ...) . documentation[String] on-call-> [One of: nil symbol-name function list] The main proces of its forward process, later called with (call model ...) method. This method must be continuous from the given arguments. where[Subscript DSL] (Optional) If you're planning to use defmodel-as macro, this form is needed.","title":"Inputs"},{"location":"nodes/#example_2","text":"(defmodel (ExampleLayer (self features) ;; Options/Utils Here, :slots ((param :initarg :param)) :initargs (:param (make-tensor `(,features) :requires-grad t)) :documentation \"ExampleLayer is a ...\") ;; After make-instance is called, the form below is called. ;; make-instance -> make-instance :after -> this form. (print self) ;; <- Initialized ExampleLayer (print features) ;; <- constructor-arguments are also used here. (print \"ExampleLayer is created!\")) ;; The model you created, works like: (let ((layer (ExampleLayer 10))) (call layer ...)) (defmodel (Softmax-Model (self) :where (X[~] -> [~]) :on-call-> ((self x) (declare (ignore self)) (let* ((x1 (!sub x (!mean x :axis 1 :keepdims t))) (z (!sum (!exp x1) :axis 1 :keepdims t))) (!div (!exp x1) z))))) ;; Keep Using Lazily... (proceed (call (Softmax-Model) (randn `(10 10))) {CPUTENSOR[float] :shape (10 10) :named ChainTMP33497 :vec-state [computed] ((0.04800622 0.118814774 0.050377533 ~ 0.053051848 0.050124187 0.25575548) (0.15909052 0.11368358 0.12642372 ~ 0.114795394 0.033397682 0.07605342) ... (0.035624444 0.24828684 0.109363265 ~ 0.020787988 0.027314318 0.04515641) (0.030307569 0.24117047 0.03900468 ~ 0.014522874 0.036584295 0.0971196)) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} (defmodel-as (Softmax-Model) :asif :function :named softmax-static) ;; No compiling overhead (softmax-static (randn `(10 10))) {CPUTENSOR[float] :shape (10 10) :named ChainTMP33788 ((0.16722792 0.018530384 0.014159603 ~ 0.035353966 0.06128503 0.13559735) (0.14498742 0.11881006 0.0692616 ~ 0.03911829 0.10358454 0.02131605) ... (0.055657785 0.44042623 0.030706322 ~ 0.11048273 0.0097645 0.11959953) (0.059088983 0.11067564 0.120767005 ~ 0.15042976 0.06570089 0.20548664)) :facet :input :requires-grad NIL :backward NIL}","title":"Example"},{"location":"nodes/#dispatching-on-call-method","text":"on-call-> is nil In that case, users must define the call definiton manually like (defmethod call ((model YourComposite) arg1 arg2) ...) . on-call-> is symbol In that case, the composite invokes the method named symbol when call is invoked. ;; Set :on-call-> call-example-layer (defmethod call-example-layer ((model ExampleLayer) x y) (print \"call-example-layer is used!\")) (call (ExampleLayer 10) tensor) ;; call-example-layer is used!","title":"Dispatching on-call-&gt; method"},{"location":"nodes/#on-call-is-a-list","text":"Directly defines a call method. Arguments must be: (self arg1 arg2...) ... :on-call-> ((self x) (!sin x))","title":"on-call-&gt; is a list"},{"location":"nodes/#macro-defmodel-as","text":"(defmodel-as target-model &key (where nil) (asif :function) (named nil) (differentiable nil)) Redefines a Composite as a new function or AbstractNode specified in the :asif keyword. Further functions or Differentiable AbstractNode can be defined based on existing Composites (also called as model and defined by defmodel macro) which bundles several AbstractNodes , as long as :where form is fulfilled.","title":"[macro] defmodel-as"},{"location":"nodes/#example_3","text":"(defmodel-as (SoftmaxNode) :named static-softmax :asif :function :where (A[~] -> A[~]))","title":"Example"},{"location":"nodes/#inputs_3","text":"target-model[Composite] a form to initialize the composite. ~~This from is executed before running the code, and accordingly static.~~ where[Subscript DSL or null] If the model has no :where declaration, this macro uses this :where form instead. Therefore, as long as defmodel provides :where declaration, this form should be OK if set as nil. named[symbol] this macro will define a new function after named . If set to nil , the macro return a lambda function instead of defining it. If you're trying to define a new AbstractNode , this option should be fulfilled. :asif[keyword] indicates which form the target-model is to be redefined, and could be one of: \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 asif | description \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 :function | Defines a function to be executed immediately that does not create a node. \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 :node | Defines a AbstractNode which needs to be compiled later \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500","title":"Inputs"},{"location":"nodes/#effects_3","text":"If named is not NIL , this macro defines a new function or AbstractNode after named .","title":"Effects"},{"location":"nodes/#notes","text":"Depending on the device and dtype used of arguments, several methods are compiled and dispatched.","title":"Notes"},{"location":"nodes/#events-for-embedding-jit-generated-code-in-runtime","text":"If the node is needed to be compiled, compile.NIL","title":"Events for Embedding JIT-Generated Code in runtime"},{"location":"optimizer/","text":".katex img { object-fit: fill; padding: unset; display: block; position: absolute; width: 100%; height: inherit; } cl-waffe2/optimizers [class] AbstractOptimizer AbstractTensors with :requires-grad=t can find their gradients with the (backward (build toplevel)) function. AbstractOptimizer is a class which minimizes the value of toplevel subject to (grad tensor) . In cl-waffe2, we initialize one AbstractOptimizer for one AbstractTensor. Specifically, one is able to create a new AbstractOptimizer with the function (name tensor &rest constructor-args) , for example, (adam (parameter (randn (list 3 3))) :lr 1e-3) to create a new Adam Optimizer, and can be tied to the tensor like: (hook-optimizer! tensor abstract-optimizer) . Users can define any optimizer algorithms with the defoptimizer macro. Optimizing tied tensors is performed by calling a (step-optimize optimizer) method. The parameter to be optimized can be accessed by a read-parameter method. Example: Hooks and calls the optimizer tied to the tensor. (let ((a (parameter (randn `(3 3))))) (hook-optimizer! a (Adam a)) (call-optimizer! a)) Tips: Customized Printing At first, AbstractOptimizers are displayed in your terminal like: (Adam (parameter (randn `(3 3)))) ;; <AbstractOptimizer: ADAM( ) -> TID11256> ;; ^ You're allowed to insert something The method cl-waffe2/vm.nodes:on-print-object is also used to customize how AbstractOptimizer is displayed: (defmethod cl-waffe2/vm.nodes:on-print-object ((opt Adam) stream) (format stream \"lr=~a eps=~a beta1=~a beta2=~a N=~a\" (lr-of opt) (eps-of opt) (beta1-of opt) (beta2-of opt) (adam-n opt))) Do not insert Newline here because AbstractOptimizer is also displayed when printing AbstractTensor with hooked optimizers. See also: defoptimizer read-parameter step-optimize . [macro] defoptimizer The macro defoptimizer defines a user-defined optimizer class which is a subclass of AbstractOptimizer . And the class is dispatched one per parameter to be optimized and the method step-optimize is called each time an optimizing is performed. Input param the tensor to be optimized is given as this argument. the tensor is stored in the param slot automatically, being accessed by a read-parameter method. Example We use defmodel and defmodel-as because formulae for optimisation functions can be expressed in Composite and compiled as functions to reduce compilation time. (defoptimizer (SGD (self param &key (lr 1e-3)) :slots ((lr :initarg :lr :reader sgd-lr)))) (defmodel (SGD-Compute-Form (self) :where (Param[~] Grad[~] Lr[scal] -> Param[~] where scal = 1) :documentation \"Param_New <- Param - Param * Grad * Lr\" :on-call-> ((self param grad lr) (declare (ignore self)) (A-=B param (!mul lr grad))))) (defmodel-as (SGD-Compute-Form) :named step-sgd) (defmethod step-optimize ((optimizer SGD)) (let* ((lr (make-tensor (sgd-lr optimizer))) (param (read-parameter optimizer)) (grad (grad param))) (step-sgd param grad lr))) [optimizer] SGD Initializer (SGD param &KEY (LR 0.001)) Description Inputs Implements a simple SGD. P a r a m n e w \u2190 P a r a m \u2212 P a r a m g r a d \u00d7 l r Param_{new}\\gets{Param - Param_{grad}\\times{lr}} P a r a m n e w \u200b \u2190 P a r am \u2212 P a r a m g r a d \u200b \u00d7 l r lr[single-float] learning rate. [optimizer] ADAM Initializer (ADAM param &KEY (LR 0.001) (EPS 1.0e-7) (BETA1 0.9) (BETA2 0.999)) Description Inputs Implements Adam algorithm. See the original paper for detailed algorithms.","title":"cl-waffe2/optimizers"},{"location":"optimizer/#cl-waffe2optimizers","text":"","title":"cl-waffe2/optimizers"},{"location":"optimizer/#class-abstractoptimizer","text":"AbstractTensors with :requires-grad=t can find their gradients with the (backward (build toplevel)) function. AbstractOptimizer is a class which minimizes the value of toplevel subject to (grad tensor) . In cl-waffe2, we initialize one AbstractOptimizer for one AbstractTensor. Specifically, one is able to create a new AbstractOptimizer with the function (name tensor &rest constructor-args) , for example, (adam (parameter (randn (list 3 3))) :lr 1e-3) to create a new Adam Optimizer, and can be tied to the tensor like: (hook-optimizer! tensor abstract-optimizer) . Users can define any optimizer algorithms with the defoptimizer macro. Optimizing tied tensors is performed by calling a (step-optimize optimizer) method. The parameter to be optimized can be accessed by a read-parameter method.","title":"[class] AbstractOptimizer"},{"location":"optimizer/#example-hooks-and-calls-the-optimizer-tied-to-the-tensor","text":"(let ((a (parameter (randn `(3 3))))) (hook-optimizer! a (Adam a)) (call-optimizer! a))","title":"Example: Hooks and calls the optimizer tied to the tensor."},{"location":"optimizer/#tips-customized-printing","text":"At first, AbstractOptimizers are displayed in your terminal like: (Adam (parameter (randn `(3 3)))) ;; <AbstractOptimizer: ADAM( ) -> TID11256> ;; ^ You're allowed to insert something The method cl-waffe2/vm.nodes:on-print-object is also used to customize how AbstractOptimizer is displayed: (defmethod cl-waffe2/vm.nodes:on-print-object ((opt Adam) stream) (format stream \"lr=~a eps=~a beta1=~a beta2=~a N=~a\" (lr-of opt) (eps-of opt) (beta1-of opt) (beta2-of opt) (adam-n opt))) Do not insert Newline here because AbstractOptimizer is also displayed when printing AbstractTensor with hooked optimizers. See also: defoptimizer read-parameter step-optimize .","title":"Tips: Customized Printing"},{"location":"optimizer/#macro-defoptimizer","text":"The macro defoptimizer defines a user-defined optimizer class which is a subclass of AbstractOptimizer . And the class is dispatched one per parameter to be optimized and the method step-optimize is called each time an optimizing is performed.","title":"[macro] defoptimizer"},{"location":"optimizer/#input","text":"param the tensor to be optimized is given as this argument. the tensor is stored in the param slot automatically, being accessed by a read-parameter method.","title":"Input"},{"location":"optimizer/#example","text":"We use defmodel and defmodel-as because formulae for optimisation functions can be expressed in Composite and compiled as functions to reduce compilation time. (defoptimizer (SGD (self param &key (lr 1e-3)) :slots ((lr :initarg :lr :reader sgd-lr)))) (defmodel (SGD-Compute-Form (self) :where (Param[~] Grad[~] Lr[scal] -> Param[~] where scal = 1) :documentation \"Param_New <- Param - Param * Grad * Lr\" :on-call-> ((self param grad lr) (declare (ignore self)) (A-=B param (!mul lr grad))))) (defmodel-as (SGD-Compute-Form) :named step-sgd) (defmethod step-optimize ((optimizer SGD)) (let* ((lr (make-tensor (sgd-lr optimizer))) (param (read-parameter optimizer)) (grad (grad param))) (step-sgd param grad lr)))","title":"Example"},{"location":"optimizer/#optimizer-sgd","text":"","title":"[optimizer] SGD"},{"location":"optimizer/#initializer","text":"(SGD param &KEY (LR 0.001))","title":"Initializer"},{"location":"optimizer/#description","text":"","title":"Description"},{"location":"optimizer/#inputs","text":"Implements a simple SGD. P a r a m n e w \u2190 P a r a m \u2212 P a r a m g r a d \u00d7 l r Param_{new}\\gets{Param - Param_{grad}\\times{lr}} P a r a m n e w \u200b \u2190 P a r am \u2212 P a r a m g r a d \u200b \u00d7 l r lr[single-float] learning rate.","title":"Inputs"},{"location":"optimizer/#optimizer-adam","text":"","title":"[optimizer] ADAM"},{"location":"optimizer/#initializer_1","text":"(ADAM param &KEY (LR 0.001) (EPS 1.0e-7) (BETA1 0.9) (BETA2 0.999))","title":"Initializer"},{"location":"optimizer/#description_1","text":"","title":"Description"},{"location":"optimizer/#inputs_1","text":"Implements Adam algorithm. See the original paper for detailed algorithms.","title":"Inputs"},{"location":"overview/","text":"Programmable Deep Learning Framework In the recent years, the widely accepted libraries and frameworks in the field of data science, such as matrix operations and mathematical optimization, have concentrated on popular and attractive programming languages: Python and Julia. But did you know that a long time ago (about 40~50 years), artificial intelligences are all about Lisp? At that time, research into artificial intelligences using symbolic logics was very popular, and Lisp and Prolog were the languages of choice. As the time went on, however, such research became less and less common, and instead languages that could easily handle large matrices became mainstream, which is roughly the history to date. However, even without the influence of symbolic theory, Common Lisp is still powerful language and indeed I'm one of Lispers who believes the advantages of applying this into data science. In this section, I will explain the concepts and benefits of my project cl-waffe2, an elegant and extensible Deep Learning Framework on Common Lisp. Why not: Keep Using Python? Python is not my cup of tea because of these reasons: Verbose: Python Syntax is very simple and loved by a lot of engineers across their fields, but it's too simple; I'm always annoyed with a ton of decorators and redundant class initialization syntax. Common Lisp is an envolving language under the standardization by ANSI Common Lisp established in 25 years ago; Writing an extension is no longer pain. CL has more; Error Handlings, Dynamic Scopes, MetaProgramming, and more! Ease of Debugging: For Lisp, REPL is their cup of tea! I won't talk too much about this topic here since it has been discussed a lot elsewhere. In fact, however, the equivalent PyTorch code can be rewritten so short and intuitive using cl-waffe2. class NeuralNetwork(nn.Module): def __init__(self): super().__init__() self.flatten = nn.Flatten() self.linear_relu_stack = nn.Sequential( nn.Linear(28*28, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU(), nn.Linear(512, 10), ) def forward(self, x): x = self.flatten(x) logits = self.linear_relu_stack(x) return logits (Reference: BUILD THE NEURAL NETWORK ) (defsequence NeuralNetwork () \"My Neural Network\" (asnode #'!flatten) (LinearLayer (* 28 28) 512) (asnode #'!relu) (LinearLayer 512 512) (asnode #'!relu) (LinearLayer 512 10)) This is not only the case of defining networks but: When writing an extension, defining models, composing several functions, and visualizing... cl-waffe2 is designed to make more codes smaller and obvious! Readers may well feel \"How about Julia?\" - Yes, I think Julia ecosystems are great, and cl-waffe2 is influenced by Julia everywhere. And, this should be introduced into Common Lisp, not just a few languages! Let's Get Started! Note that all sample codes are working under this package: (defpackage :concepts (:use :cl :cl-waffe2 :cl-waffe2/nn :cl-waffe2/distributions :cl-waffe2/base-impl :cl-waffe2/vm.generic-tensor :cl-waffe2/vm.nodes :cl-waffe2/vm :cl-waffe2/optimizers :cl-waffe2/backends.cpu :cl-waffe2/backends.lisp)) (in-pacakge :concepts) Nodes are Abstract, Lazy, and Small. In neural networks, most temporary tensors aren't observed during training time while they actually consist a half of memory usage. Programmers have to take into account which tensors need intermediate values and should be in place - but If only compilers know the start and end points of the computation node, a simple algorithm which just counts up the reference count, can alternative this process. Likewise theano, and other existing libraries (e.g.: Petalisp ...) cl-waffe2 is nothing but Domain Specific Language, which represents the graph being compiled and optimized. I think I don't have the enough background to introduce Theano, but cl-waffe2 is also influenced by this framework; operations are lazy-evaluated, and later compiled. Accepting Lazy-Evaluation, that is, Nodes also have to imply the transformation of shapes, and which tensors should be in-place. These can be expressed in a single line using Subscript DSL macros (compiled in the toplevel). In the simplest case, the AbstractNode MatmulNode-Revisit which is a reimplementation of existing MatmulNode and computes matrix multiplication of given two tensors: A and B, storing the result into C, can be declared like: (defnode (MatMulNode-Revisit (self dtype) :where (A[~ i j] B[~ j k] C[~ i k] -> C[~ i k]) :backward ((self dout a b c) (declare (ignore c)) (values (!matmul dout (!t b)) (!matmul (!t a) dout) nil)) :documentation \"OUT <- GEMM(1.0, A, B, 0.0, C)\")) In term of performance, ranks and shapes are declared anywhere. So you MUST understand this notation and specification if you want to work on cl-waffe2: (Specs: representing-shapes-before-and-after-the-operation ) The macro defnode defines AbstractNode which guarantees the same behaviour across different devices. If you want to implement MatmulNode-Revisit in various devices; CPU, Lisp, CUDA, Metal , that should be defined with its device name, a rank of operations, elsewhere. Here we have a function which calculates GEMM in Lisp Code, let's see how cl-waffe2 can use this. ;; A_ii,B_ik->C_jk ;; The performance would be the worst. Should not be used for practical. (defun gemm! (m n k a-offset a b-offset b c-offset c) \"Computes 1.0 * A[M N] @ B[N K] + 0.0 * C[M K] -> C[M K]\" (declare (type (simple-array single-float (*)) a b c) (type (unsigned-byte 32) m n k a-offset b-offset c-offset) (optimize (speed 3) (safety 0))) (dotimes (mi m) (dotimes (ni n) (dotimes (ki k) (incf (setf c (+ c-offset (* mi K) ni)) (* (aref a (+ a-offset (* mi n) ki)) (aref b (+ b-offset (* ki k) ni)))))))) So first, we need the device to execute the node. cl-waffe2 widely adapts CLOS anywhere, of course, all tensors are subclass of AbstractTensor . cl-waffe2 provides the LispTensor backend in standard which work on CPU. Matrix allocation etc. are already coded, so it is sufficient to create a new class by inheritance: MyTensor when you want to make an extension for CPU. (defclass MyTensor (LispTensor) nil) Matrix Multiplication is the operation performed in every two dimensions. If the given tensor is 3D/4D..., it is batched and applied. If it is sliced, the offsets are added in that part. Regardless of which matmul implementation is used, but at least 2D tensor is needed, the process of adding offsets and batching follows the specification of cl-waffe2. So, we are going to use the call-with-view function returning an S-expression that optimized and collapsed loops through a given tensor up to a defined dimension in order to maximize parallelization of kernel functions. And, describe the steps of calling gemm! as if writing a macro. (Of course, there is another way that writes a function directly though.) One of the implementation of the device, can be given by the define-impl macro. (define-impl (MatmulNode-Revisit :device MyTensor) :save-for-backward (t t nil) :forward ((self a b c) `(,@(call-with-view #'(lambda (a-view b-view c-view) ;; a-view = [a-view[dim=-2] a-view[dim=-1]] ;; view contains: size, offset, stride `(gemm! ,(size-of a-view 0) ;; (size-of view-list dim) ,(size-of b-view 0) ,(size-of c-view 1) ,(offset-of a-view 0) (tensor-vec ,a) ,(offset-of b-view 0) (tensor-vec ,b) ,(offset-of c-view 0) (tensor-vec ,c))) `(,a ,b ,c) :at-least-dim 2) ;; Returning C ,c))) A blueprint of the lambda functions described here is later compiled by (compile nil body) , and depending on the slices(offsets), ranks, dtypes, shapes, and permutations, functions are cached so users don't have to worry about the performance issue due to eval . On the contrary, the loop order is optimised (reordering, collapsing and lparallel) and can be expected to be about 1.1 times faster on average compared to loops without them. So, we've got new implementation of gemm , let's get this going. Your MyTensor is registered as a cl-waffe2 device. CONCEPTS> (show-backends) \u2500\u2500\u2500\u2500\u2500[All Backends Tree]\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 [*]LISPTENSOR: Common Lisp implementation on matrix operations \u2514[*]CPUTENSOR: OpenBLAS=available *simd-extension-p*=available \u2514[*]MYTENSOR: No status. \u2514[-]JITCPUTENSOR: compiler=gcc flags=(-fPIC -O3 -march=native) viz=NIL [-]SCALARTENSOR: is a special tensor for representing scalar values. \u2514[-]JITCPUSCALARTENSOR: Use with JITCPUTensor ([*] : in use, [-] : not in use.) Add a current-backend-state method to display the status. \u2500\u2500\u2500\u2500\u2500[*using-backend*]\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Priority: Higher <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>Lower MYTENSOR CPUTENSOR LISPTENSOR (use with-devices macro or set-devices-toplevel function to change this parameter.) NIL (P.S.: Classes in an inheritance relationship in this tree are regarded as compatible with each other.) There's two ways to declare MyTensor is a valid device that cl-waffe2 can use: locally using the with-devices macro, or using the set-devices-toplevel function. In this case, we use the with-devices macro and locally declares operations are performed under MyTensor backend. ;; Gemm with Lisp (defun test-gemm (&key (bench nil)) (with-devices (MyTensor) (let ((a (randn `(100 100))) (b (randn `(100 100))) (c (make-input `(100 100) nil))) (proceed (call (MatmulNode-Revisit) a b c) :measure-time bench)))) ;; Gemm with OpenBLAS ;; Set bench=t, and measures time ;; As excepted, this one is 20~30times faster. (defun test-gemm-cpu (&key (bench nil)) (with-devices (CPUTensor) (let ((a (randn `(100 100))) (b (randn `(100 100))) (c (make-input `(100 100) nil))) (proceed (call (MatmulNode :float) a b c) :measure-time bench)))) The moment AbstractNode is defined, the constructor function (Node-Name args) is also defined being called its forward propagation with the call or forward method. As with PyTorch and Chainer, call can be used to record the graph, but gemm cannot be executed at that time. You also have to tell a one more thing; \"When will the results be needed?\". The fundamental function is build , and the proceed functions, which makes it differentiable, is easy to use in the REPL and debug. CONCEPTS> (test-gemm) {MYTENSOR[float] :shape (100 100) :named :TENSOR :vec-state [computed] ((-1.1147273 1.2072208 -0.25984392 ~ 0.038998634 -1.2577316 1.049665) (-0.28503713 0.3086878 -0.0664424 ~ 0.009971998 -0.3216035 0.2684006) ... (-0.19921443 0.21574406 -0.04643706 ~ 0.0069694985 -0.22477092 0.18758704) (0.9453751 -1.0238167 0.22036776 ~ -0.03307386 1.0666538 -0.89019716)) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} Btw, the proceed functions provides more; for measuring execution time, and profiling your computation node. CONCEPTS> (proceed-time (!add 1 1)) [proceed-time] build -> Evaluation took: 0.000 seconds of real time 0.000039 seconds of total run time (0.000038 user, 0.000001 system) 100.00% CPU 86,386 processor cycles 0 bytes consed [proceed-time] With allocation time: Evaluation took: 0.000 seconds of real time 0.000030 seconds of total run time (0.000021 user, 0.000009 system) 100.00% CPU 65,918 processor cycles 0 bytes consed [proceed-time] Without allocation time: Evaluation took: 0.000 seconds of real time 0.000009 seconds of total run time (0.000008 user, 0.000001 system) 100.00% CPU 17,032 processor cycles 0 bytes consed CONCEPTS> (proceed-bench (!softmax (randn `(100 100)))) [Sorted by Instructions] Time(s) | Instruction ( * - Beyonds the average execution time) 1.4e-5 | <WfInst[op=MOVETENSORNODE-CPUTENSOR] : TID10689 <= op(TID10689{float, (100 100)} <Input>TID10601{float, (100 100)})> 2.0e-6 | <WfInst[op=VIEWTENSORNODE-T] : TID10683 <= op(TID10641{float, (100 1)} TID10683{float, (100 1)})> 2.0e-6 | <WfInst[op=SCALARMUL-CPUTENSOR] : TID10683 <= op(TID10683{float, (100 1)} <Input>TID10610{float, (1)})> 1.0e-6 | <WfInst[op=VIEWTENSORNODE-T] : TID10683 <= op(TID10683{float, (100 100)} TID10683{float, (100 1)})> 8.4e-5* | <WfInst[op=ADDNODE-CPUTENSOR] : TID10683 <= op(TID10683{float, (100 100)} <Input>TID10601{float, (100 100)})> 1.0e-6 | <WfInst[op=VIEWTENSORNODE-T] : TID10683 <= op(TID10683{float, (100 1)} TID10683{float, (100 100)})> 3.3e-5* | <WfInst[op=SCALARDIV-CPUTENSOR] : TID10683 <= op(TID10683{float, (100 1)} <Input>TID10605{float, (1)})> 2.0e-6 | <WfInst[op=VIEWTENSORNODE-T] : TID10683 <= op(TID10683{float, (100 100)} TID10683{float, (100 1)})> 5.5e-5* | <WfInst[op=SUBNODE-CPUTENSOR] : TID10689 <= op(TID10689{float, (100 100)} TID10683{float, (100 100)})> 1.0e-5 | <WfInst[op=EXPNODE-CPUTENSOR] : TID10689 <= op(TID10689{float, (100 100)} TID10689{float, (100 100)})> 2.0e-6 | <WfInst[op=SCALARMUL-CPUTENSOR] : TID10683 <= op(TID10683{float, (100 1)} <Input>TID10738{float, (1)})> 1.0e-6 | <WfInst[op=VIEWTENSORNODE-T] : TID10683 <= op(TID10683{float, (100 100)} TID10683{float, (100 1)})> 1.12e-4* | <WfInst[op=ADDNODE-CPUTENSOR] : TID10683 <= op(TID10683{float, (100 100)} TID10689{float, (100 100)})> 5.5e-5* | <WfInst[op=DIVNODE-CPUTENSOR] : TID10689 <= op(TID10689{float, (100 100)} TID10683{float, (100 100)})> 14 Instructions | 7 Tensors | Overheads due to SV4BW(...) -> 6.0e-6(s) Total Time: 3.74e-4 sec [Sorted by topK] Instruction | Total time (s) | Time/Total (n-sample=1) <WfInst[op=ADDNODE-CPUTENSOR] | 1.9600001e-4 | 52.406418% <WfInst[op=SUBNODE-CPUTENSOR] | 5.5e-5 | 14.705883% <WfInst[op=DIVNODE-CPUTENSOR] | 5.5e-5 | 14.705883% <WfInst[op=SCALARDIV-CPUTENSOR] | 3.3e-5 | 8.823529% <WfInst[op=MOVETENSORNODE-CPUTENSOR] | 1.4e-5 | 3.7433155% <WfInst[op=EXPNODE-CPUTENSOR] | 1.0e-5 | 2.6737967% <WfInst[op=VIEWTENSORNODE-T] | 7.0e-6 | 1.8716577% <WfInst[op=SCALARMUL-CPUTENSOR] | 4.0e-6 | 1.0695187% {MYTENSOR[float] :shape (100 100) :id TID10689 :vec-state [computed] ((0.01518923 0.049908508 0.017617546 ~ 0.012779288 0.0020210263 0.0018769704) (0.05841501 0.016887225 0.007463644 ~ 0.0027627628 0.0033548716 0.0028829984) ... (0.0016393916 0.0048142807 0.057292804 ~ 0.0036875184 0.015796835 0.0014674882) (0.03651239 0.0038913002 0.009065828 ~ 0.0060259635 0.012457987 0.003022789)) :facet :input :belongs-to :memory-pool :requires-grad NIL :backward <Node: DIVNODE-CPUTENSOR (A[~] B[~] -> A[~])>} Lazy Evaluation Programming is not as hard as you'd think - Functions generated by AbstractNode can be embedded in your CommonLisp code in a natural way. As the simplest and basic example, the build function traces and compiles the network from the endpoints of the computation nodes, returning the Compiled-Composite class which can invoked its forward propagation by the forward method: (let ((a (make-input `(A B) :A)) (b (make-input `(A B) :B))) (let ((model (build (!sum (!mul a b)) :inputs `(:A :B)))) (print model) ;; model is a compiled function: f(a b) (forward model (randn `(3 3)) (randn `(3 3))))) ;;<Compiled-Composite(allocated-p=NIL) ;; forward : forward(model A B) -> CPUTENSOR{FLOAT}(1 1) ;; backward : backward(model) -> t ;; memory-pool : two tensor(s) ;; L {8.0e-6+((A B) x 4.0e-6)}MB ;; inputs: ;; A -> (A B) ;; B -> (A B) ;;> ;;{CPUTENSOR[float] :shape (1 1) -> :view (<(BROADCAST 1)> <(BROADCAST 1)>) -> :visible-shape (1 1) :named ChainTMP646587 ;; ((1.0858848)) ;; :facet :input ;; :requires-grad NIL ;; :backward NIL} And node->defun , node->lambda . If you want to create a dedicated Tensor to store the calculation results, you should use the make-input function with name=nil . InputTensor does not guarantee that the elements are filled with zeros, but the compiler can automatically reconnect them and reduce memory usage. (defun my-matmul (a b) (let* ((m (first (shape a))) (k (second (shape b))) (c (make-input `(,m ,k) nil))) ;; C as output trensor (call (MatmulNode-Revisit) a b c))) (print (proceed (my-matmul (randn `(3 3)) (randn `(3 3))))) ;; Composing (!softmax (matmul a b)) (node->defun %mm-softmax (A[m n] B[n k] -> C[m k]) (!softmax (my-matmul a b))) ;; Here's also (node->lambda (A[m n] B[n k] -> C[m k]) ...) ;; JIT Enabled Matrix Operations (defun local-cached-matmul () ;; Works like Lisp Function (print (time (%mm-softmax (randn `(3 3)) (randn `(3 3))))) (print (time (%mm-softmax (randn `(3 3)) (randn `(3 3)))))) And last, The set-devices-toplevel function make cl-waffe2 use MyTensor anywhere. (set-devices-toplevel 'MyTensor 'CPUTensor 'LispTensor) Advanced Network Constructions We call a set of composed AbstractNode , or a chunk of nodes with trainable parameters, a Composite defined by the defmodel macro. (defmodel (LayerNorm-Revisit (self normalized-shape &key (eps 1.0e-5) (affine T)) :slots ((alpha :initform nil :accessor alpha-of) (beta :initform nil :accessor beta-of) (shape :initform nil :initarg :normalized-shape :accessor dim-of) (eps :initform nil :initarg :eps :accessor eps-of)) ;; Optional :where (X[~ normalized-shape] -> out[~ normalized-shape]) :on-call-> layer-norm) ;; Constructor (when affine (setf (alpha-of self) (parameter (ax+b `(,@normalized-shape) 0 1)) (beta-of self) (parameter (ax+b `(,@normalized-shape) 0 0))))) (defmethod layer-norm ((self LayerNorm-Revisit) x) (with-slots ((alpha alpha) (beta beta)) self (let* ((last-dim (length (dim-of self))) (u (!mean x :axis (- last-dim) :keepdims t)) (s (!mean (!expt (!sub x u) 2) :axis (- last-dim) :keepdims t)) (x (!div (!sub x u) (!sqrt (!add (->contiguous s) (eps-of self)))))) ;; !flexible = (%transform alpha[i] -> [~ i]) ;; both inserts an broadcastable axis (if (and alpha beta) (!add (!mul x (%transform alpha[i] -> [~ i])) (!flexible beta)) x)))) As a chunk of nodes with trainable parameter, Composites can be used merely a subroutine: (proceed (call (LayerNorm-Revisit `(10)) (randn `(10 10 10)))) If you use Composite as a set of composed AbstractNode , they're compiled into another types: (defmodel (Softmax-Model (self) :on-call-> ((self x) (declare (ignore self)) (let* ((x1 (!sub x (!mean x :axis 1 :keepdims t))) (z (!sum (!exp x1) :axis 1 :keepdims t))) (!div (!exp x1) z))))) (defmodel-as (Softmax-Model) :where (A[~] -> B[~]) :asif :function :named %softmax) (%softmax (randn `(10 10))) Composing It is not elegant to use call more than once when composing multiple models. (call (AnyModel1) (call (AnyModel2) (call (AnyModel3) X))) Instead, you can use the the call-> function: (call-> X (AnyModel1) (AnyModel2) (AnyModel3)) If you wanted to insert a function to construct a computation node here, you can also use asnode function to make the function recognised as a Composite. (call-> X (AnyModel1) (asnode #'!softmax) (asnode #'!view 0) ;; Slicing the tensor: (!view x 0 t ...) (asnode #'!add 1.0) ;; X += 1.0 (asnode !matmul Y) ;; X <- Matmul(X, Y) ) If the Composite can be implemented using only call-> , the defsequence can be used for a short description: (defsequence MLP (in-features) \"Docstring (optional)\" (LinearLayer in-features 512) (asnode #'!tanh) (LinearLayer 512 256) (asnode #'!tanh) (LinearLayer 256 10)) ;; Sequences can receive only a single argument. (call (MLP 786) (randn `(10 786))) Make everything user-extensible Customized Autodiff - cl-waffe2 as a graph processing library (TODO) (defclass MyScalarTensor (ScalarTensor) nil) (set-devices-toplevel 'MyTensor 'CPUTensor 'LispTensor 'MyScalarTensor) (define-op (MyMul (self) :where (A[scal] B[scal] -> A[scal] where scal = 1) :out-scalar-p t :save-for-backward-names (a b) :forward ((self a b) (with-setting-save4bw ((a a) (b b)) (setf (tensor-vec a) (* (tensor-vec a) (tensor-vec b))) a)) :backward ((self dy) (with-reading-save4bw ((a a) (b b)) (values (make-tensor (* (tensor-vec dy) (tensor-vec b))) (make-tensor (* (tensor-vec dy) (tensor-vec a)))))))) (define-op (MySin (self) :where (A[scal] out[scal] -> out[scal] where scal = 1) :out-scalar-p t :save-for-backward-names (a) :forward ((self a out) (with-setting-save4bw ((a a)) (setf (tensor-vec out) (sin (tensor-vec a))) out)) :backward ((self dy) (with-reading-save4bw ((a a)) (values (make-tensor (* (tensor-vec dy) (cos (tensor-vec a)))) nil))))) (defun !mymul (a b) (call (MyMul) a b)) (defun !mysin (x) (call (MySin) x (make-clone x))) (defun try-original-autodiff () (let ((a (parameter (make-tensor 1)))) (proceed-backward (!mysin (!mysin a))) (grad a))) Differentiable Programming (TODO) (defoptimizer (MySGD (self param &key (lr 1e-3)) :slots ((lr :initarg :lr :reader sgd-lr)))) (node->defun %step-sgd (Param[~] Grad[~] Lr[scal] -> Param[~] where scal = 1) (A-=B param (A*=scal grad lr))) (defmethod step-optimize ((optimizer MySGD)) (let* ((lr (make-tensor (sgd-lr optimizer))) (param (read-parameter optimizer)) (grad (grad param))) (with-no-grad (%step-sgd param grad lr)))) (defun simple-opt-model () (let* ((loss (!mean (!matmul (parameter (randn `(3 3))) (parameter (randn `(3 3)))))) (model (build loss))) (mapc (hooker x (MySGD x :lr 1e-3)) (model-parameters model)) (forward model) (backward model) (mapc #'call-optimizer! (model-parameters model)))) ({MYTENSOR[float] :shape (3 3) -> :view (<T> <T>) -> :visible-shape (3 3) ((0.25052267 -0.16212857 -1.3183842) (-1.078968 0.27860558 0.40701634) (-0.10987697 -1.2562615 0.6179133)) :facet :exist :requires-grad T :optimizer <AbstractOptimizer: MYSGD() -> TID12604>} {MYTENSOR[float] :shape (3 3) -> :view (<T> <T>) -> :visible-shape (3 3) ((-0.5223165 2.3579814 0.13172081) (0.57671905 0.56324756 1.1230979) (0.10274803 0.008530198 1.7588508)) :facet :exist :requires-grad T :optimizer <AbstractOptimizer: MYSGD() -> TID12610>}) See also: Examples Maximize the benefits of Graph-Level Optimization (TODO) ~~ [Steps] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 1. Network Construction : Create a network of AbstractNode with multiple backends. 2. Sorting/Pruning : Sort the network and prune unused nodes. 3. In-place mutation : Optimize the list by deleting unused MoveTensorNode. 4. More Localize : Reconnecting InputTensors, the comiler optimizes the locality of memory. 5. Reschedule : Create an allocation planning considering 4. and in-place ops: !view !permute !reshape etc. 6. Backward(Optional) : Construct backward propagation 7. Adjoint Optimization : Minimizes the number of copies arising at adjoints 8. Compile/Inline : If any, compiles lisp blueprints generated by call-with-view (If cached, ignored) 9. Rewriting : If any, replaces the list of declared patterns by the defpath macro 10. Completed -> Compiled-Composite is cached/inlined everywhere ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Interop: Common Lisp Array and cl-waffe2 AbstractTensor (TODO) See also: Converting AbstractTensor and other arrays Debugging (TODO) cl-waffe2 is enough clever to detect Shape-Error and suggest an alternative arising from wrong inputs. In this case, both ranks are invaild because broadcasing rank-up rule is not applied in cl-waffe2: (!add (randn `(3 3)) (randn `(3))) If you do this, you will get the following error before running the operation [cl-waffe] Shaping-Error: [Shaping Error]: The AbstractNode ADDNODE-CPUTENSOR was called with invaild arguments. The constraint: ADDNODE-CPUTENSOR: (A[~] B[~] -> A[~]) Received: (forward (ADDNODE-CPUTENSOR ...) CPUTENSOR{FLOAT}(3) CPUTENSOR{FLOAT}(3) \u2500 B: The length of ~~ do not match. The Rank is too low ) B: \u2500 the 1th shape is (3) but it violates ~ = (3 3) \u2500 The given rank 2 do not match declared: (3) Excepted: (forward (ADDNODE-CPUTENSOR ...) B(3) \u2500> B: ) B: \u2500 Use (!flexible tensor) to explict a rank-up rule of broadcasting. When adding or repeating ranks by broadcasting rule, it is necessary to declare in advance in which position they are to be added: ;; X[a b c] -> X[~ a b c] (!flexible x :at 0) ;; X[a] -> X[~ a] (%transform (randn `(3))[i] -> [~ i]) Following the suggestion, fix the code: (defparmeter out (!add (randn `(3 3)) (!flexible (randn `(3))))) And passed: (proceed out) This is a case of before execution, speaking of runtime error (e.g.: floating-point overflow), it gets a bit complicated; you have to face the disassembled code to find out the details. When doing (!sqrt x) where x is a negative number: (proceed (!sin (!sqrt (!mul -1.0 (!sin (ax+b `(3 3) 0 1)))))) As excepted it produces FLOATING-POINT-INVAILD-OPERATION: cl-waffe2 VM: Encountered Runtime Error at 3th instruction. disassemble: 0 : <WfInst[op=MOVETENSORNODE-CPUTENSOR] : TID14200 <= op(TID14200{float, (3 3)} <Input>TID14197{float, (3 3)})> 1 : <WfInst[op=SINNODE-CPUTENSOR] : TID14200 <= op(<Input>TID14197{float, (3 3)} TID14200{float, (3 3)})> 2 : <WfInst[op=SCALARMUL-CPUTENSOR] : TID14200 <= op(TID14200{float, (3 3)} <Input>TID14218{float, (1)})> 3*: <WfInst[op=SQRTNODE-CPUTENSOR] : TID14200 <= op(TID14200{float, (3 3)} TID14200{float, (3 3)})> 4 : <WfInst[op=SINNODE-CPUTENSOR] : TID14200 <= op(TID14200{float, (3 3)} TID14200{float, (3 3)})> condition: arithmetic error FLOATING-POINT-INVALID-OPERATION signalled Use the disassemble-waffe2-ir function to check the full disassembled code instead of proceed: (disassemble-waffe2-ir (!sin (!sqrt (!mul -1.0 (!sin (parameter (ax+b `(3 3) 0 1))))))) disassemble-waffe2-ir: [Forward]: <WfInst[op=MOVETENSORNODE-CPUTENSOR] : TID14654 <= op(TID14654{float, (3 3)} <Param>TID14649{float, (3 3)})> <WfInst[op=SINNODE-CPUTENSOR] : TID14654 <= op(<Param>SV4BW(TID14649{float, (3 3)}) TID14654{float, (3 3)})> <WfInst[op=SCALARMUL-CPUTENSOR] : TID14654 <= op(SV4BW(TID14654{float, (3 3)}) <Input>TID14675{float, (1)})> <WfInst[op=SQRTNODE-CPUTENSOR] : TID14654 <= op(SV4BW(TID14654{float, (3 3)}) TID14654{float, (3 3)})> <WfInst[op=SINNODE-CPUTENSOR] : TID14654 <= op(SV4BW(TID14654{float, (3 3)}) TID14654{float, (3 3)})> 5 Instructions | 2 Tensors | 1 Scalars [Pullback]: <WfInst[op=MOVETENSORNODE-CPUTENSOR] : TID14764 <= op(TID14764{float, (3 3)} <Input>TID14742{float, (3 3)})> <WfInst[op=COSNODE-CPUTENSOR] : TID14732 <= op(TID14732{float, (3 3)} TID14732{float, (3 3)})> <WfInst[op=MULNODE-CPUTENSOR] : TID14764 <= op(TID14764{float, (3 3)} TID14732{float, (3 3)})> <WfInst[op=INVERSETENSORNODE-CPUTENSOR] : TID14710 <= op(TID14710{float, (3 3)})> <WfInst[op=SCALARMUL-CPUTENSOR] : TID14710 <= op(TID14710{float, (3 3)} <Input>TID14782{float, (1)})> <WfInst[op=MULNODE-CPUTENSOR] : TID14764 <= op(TID14764{float, (3 3)} TID14710{float, (3 3)})> <WfInst[op=SCALARMUL-CPUTENSOR] : TID14764 <= op(TID14764{float, (3 3)} <Input>TID14675{float, (1)})> <WfInst[op=COSNODE-CPUTENSOR] : TID14665 <= op(TID14665{float, (3 3)} TID14665{float, (3 3)})> <WfInst[op=MULNODE-CPUTENSOR] : TID14764 <= op(TID14764{float, (3 3)} TID14665{float, (3 3)})> <WfInst[op=MOVETENSORNODE-CPUTENSOR] : <Input>TID14651 <= op(<Input>TID14651{float, (3 3)} TID14764{float, (3 3)})> 10 Instructions | 6 Tensors | 2 Scalars Graph Rewriting (Still Experimental, but coming soon...) We gonna talk about defpath which enables theano-like symbolic differentiation and device-specific optimizations.","title":"Tutorials"},{"location":"overview/#programmable-deep-learning-framework","text":"In the recent years, the widely accepted libraries and frameworks in the field of data science, such as matrix operations and mathematical optimization, have concentrated on popular and attractive programming languages: Python and Julia. But did you know that a long time ago (about 40~50 years), artificial intelligences are all about Lisp? At that time, research into artificial intelligences using symbolic logics was very popular, and Lisp and Prolog were the languages of choice. As the time went on, however, such research became less and less common, and instead languages that could easily handle large matrices became mainstream, which is roughly the history to date. However, even without the influence of symbolic theory, Common Lisp is still powerful language and indeed I'm one of Lispers who believes the advantages of applying this into data science. In this section, I will explain the concepts and benefits of my project cl-waffe2, an elegant and extensible Deep Learning Framework on Common Lisp.","title":"Programmable Deep Learning Framework"},{"location":"overview/#why-not-keep-using-python","text":"Python is not my cup of tea because of these reasons: Verbose: Python Syntax is very simple and loved by a lot of engineers across their fields, but it's too simple; I'm always annoyed with a ton of decorators and redundant class initialization syntax. Common Lisp is an envolving language under the standardization by ANSI Common Lisp established in 25 years ago; Writing an extension is no longer pain. CL has more; Error Handlings, Dynamic Scopes, MetaProgramming, and more! Ease of Debugging: For Lisp, REPL is their cup of tea! I won't talk too much about this topic here since it has been discussed a lot elsewhere. In fact, however, the equivalent PyTorch code can be rewritten so short and intuitive using cl-waffe2. class NeuralNetwork(nn.Module): def __init__(self): super().__init__() self.flatten = nn.Flatten() self.linear_relu_stack = nn.Sequential( nn.Linear(28*28, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU(), nn.Linear(512, 10), ) def forward(self, x): x = self.flatten(x) logits = self.linear_relu_stack(x) return logits (Reference: BUILD THE NEURAL NETWORK ) (defsequence NeuralNetwork () \"My Neural Network\" (asnode #'!flatten) (LinearLayer (* 28 28) 512) (asnode #'!relu) (LinearLayer 512 512) (asnode #'!relu) (LinearLayer 512 10)) This is not only the case of defining networks but: When writing an extension, defining models, composing several functions, and visualizing... cl-waffe2 is designed to make more codes smaller and obvious! Readers may well feel \"How about Julia?\" - Yes, I think Julia ecosystems are great, and cl-waffe2 is influenced by Julia everywhere. And, this should be introduced into Common Lisp, not just a few languages!","title":"Why not: Keep Using Python?"},{"location":"overview/#lets-get-started","text":"Note that all sample codes are working under this package: (defpackage :concepts (:use :cl :cl-waffe2 :cl-waffe2/nn :cl-waffe2/distributions :cl-waffe2/base-impl :cl-waffe2/vm.generic-tensor :cl-waffe2/vm.nodes :cl-waffe2/vm :cl-waffe2/optimizers :cl-waffe2/backends.cpu :cl-waffe2/backends.lisp)) (in-pacakge :concepts)","title":"Let's Get Started!"},{"location":"overview/#nodes-are-abstract-lazy-and-small","text":"In neural networks, most temporary tensors aren't observed during training time while they actually consist a half of memory usage. Programmers have to take into account which tensors need intermediate values and should be in place - but If only compilers know the start and end points of the computation node, a simple algorithm which just counts up the reference count, can alternative this process. Likewise theano, and other existing libraries (e.g.: Petalisp ...) cl-waffe2 is nothing but Domain Specific Language, which represents the graph being compiled and optimized. I think I don't have the enough background to introduce Theano, but cl-waffe2 is also influenced by this framework; operations are lazy-evaluated, and later compiled. Accepting Lazy-Evaluation, that is, Nodes also have to imply the transformation of shapes, and which tensors should be in-place. These can be expressed in a single line using Subscript DSL macros (compiled in the toplevel). In the simplest case, the AbstractNode MatmulNode-Revisit which is a reimplementation of existing MatmulNode and computes matrix multiplication of given two tensors: A and B, storing the result into C, can be declared like: (defnode (MatMulNode-Revisit (self dtype) :where (A[~ i j] B[~ j k] C[~ i k] -> C[~ i k]) :backward ((self dout a b c) (declare (ignore c)) (values (!matmul dout (!t b)) (!matmul (!t a) dout) nil)) :documentation \"OUT <- GEMM(1.0, A, B, 0.0, C)\")) In term of performance, ranks and shapes are declared anywhere. So you MUST understand this notation and specification if you want to work on cl-waffe2: (Specs: representing-shapes-before-and-after-the-operation ) The macro defnode defines AbstractNode which guarantees the same behaviour across different devices. If you want to implement MatmulNode-Revisit in various devices; CPU, Lisp, CUDA, Metal , that should be defined with its device name, a rank of operations, elsewhere. Here we have a function which calculates GEMM in Lisp Code, let's see how cl-waffe2 can use this. ;; A_ii,B_ik->C_jk ;; The performance would be the worst. Should not be used for practical. (defun gemm! (m n k a-offset a b-offset b c-offset c) \"Computes 1.0 * A[M N] @ B[N K] + 0.0 * C[M K] -> C[M K]\" (declare (type (simple-array single-float (*)) a b c) (type (unsigned-byte 32) m n k a-offset b-offset c-offset) (optimize (speed 3) (safety 0))) (dotimes (mi m) (dotimes (ni n) (dotimes (ki k) (incf (setf c (+ c-offset (* mi K) ni)) (* (aref a (+ a-offset (* mi n) ki)) (aref b (+ b-offset (* ki k) ni)))))))) So first, we need the device to execute the node. cl-waffe2 widely adapts CLOS anywhere, of course, all tensors are subclass of AbstractTensor . cl-waffe2 provides the LispTensor backend in standard which work on CPU. Matrix allocation etc. are already coded, so it is sufficient to create a new class by inheritance: MyTensor when you want to make an extension for CPU. (defclass MyTensor (LispTensor) nil) Matrix Multiplication is the operation performed in every two dimensions. If the given tensor is 3D/4D..., it is batched and applied. If it is sliced, the offsets are added in that part. Regardless of which matmul implementation is used, but at least 2D tensor is needed, the process of adding offsets and batching follows the specification of cl-waffe2. So, we are going to use the call-with-view function returning an S-expression that optimized and collapsed loops through a given tensor up to a defined dimension in order to maximize parallelization of kernel functions. And, describe the steps of calling gemm! as if writing a macro. (Of course, there is another way that writes a function directly though.) One of the implementation of the device, can be given by the define-impl macro. (define-impl (MatmulNode-Revisit :device MyTensor) :save-for-backward (t t nil) :forward ((self a b c) `(,@(call-with-view #'(lambda (a-view b-view c-view) ;; a-view = [a-view[dim=-2] a-view[dim=-1]] ;; view contains: size, offset, stride `(gemm! ,(size-of a-view 0) ;; (size-of view-list dim) ,(size-of b-view 0) ,(size-of c-view 1) ,(offset-of a-view 0) (tensor-vec ,a) ,(offset-of b-view 0) (tensor-vec ,b) ,(offset-of c-view 0) (tensor-vec ,c))) `(,a ,b ,c) :at-least-dim 2) ;; Returning C ,c))) A blueprint of the lambda functions described here is later compiled by (compile nil body) , and depending on the slices(offsets), ranks, dtypes, shapes, and permutations, functions are cached so users don't have to worry about the performance issue due to eval . On the contrary, the loop order is optimised (reordering, collapsing and lparallel) and can be expected to be about 1.1 times faster on average compared to loops without them. So, we've got new implementation of gemm , let's get this going. Your MyTensor is registered as a cl-waffe2 device. CONCEPTS> (show-backends) \u2500\u2500\u2500\u2500\u2500[All Backends Tree]\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 [*]LISPTENSOR: Common Lisp implementation on matrix operations \u2514[*]CPUTENSOR: OpenBLAS=available *simd-extension-p*=available \u2514[*]MYTENSOR: No status. \u2514[-]JITCPUTENSOR: compiler=gcc flags=(-fPIC -O3 -march=native) viz=NIL [-]SCALARTENSOR: is a special tensor for representing scalar values. \u2514[-]JITCPUSCALARTENSOR: Use with JITCPUTensor ([*] : in use, [-] : not in use.) Add a current-backend-state method to display the status. \u2500\u2500\u2500\u2500\u2500[*using-backend*]\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Priority: Higher <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>Lower MYTENSOR CPUTENSOR LISPTENSOR (use with-devices macro or set-devices-toplevel function to change this parameter.) NIL (P.S.: Classes in an inheritance relationship in this tree are regarded as compatible with each other.) There's two ways to declare MyTensor is a valid device that cl-waffe2 can use: locally using the with-devices macro, or using the set-devices-toplevel function. In this case, we use the with-devices macro and locally declares operations are performed under MyTensor backend. ;; Gemm with Lisp (defun test-gemm (&key (bench nil)) (with-devices (MyTensor) (let ((a (randn `(100 100))) (b (randn `(100 100))) (c (make-input `(100 100) nil))) (proceed (call (MatmulNode-Revisit) a b c) :measure-time bench)))) ;; Gemm with OpenBLAS ;; Set bench=t, and measures time ;; As excepted, this one is 20~30times faster. (defun test-gemm-cpu (&key (bench nil)) (with-devices (CPUTensor) (let ((a (randn `(100 100))) (b (randn `(100 100))) (c (make-input `(100 100) nil))) (proceed (call (MatmulNode :float) a b c) :measure-time bench)))) The moment AbstractNode is defined, the constructor function (Node-Name args) is also defined being called its forward propagation with the call or forward method. As with PyTorch and Chainer, call can be used to record the graph, but gemm cannot be executed at that time. You also have to tell a one more thing; \"When will the results be needed?\". The fundamental function is build , and the proceed functions, which makes it differentiable, is easy to use in the REPL and debug. CONCEPTS> (test-gemm) {MYTENSOR[float] :shape (100 100) :named :TENSOR :vec-state [computed] ((-1.1147273 1.2072208 -0.25984392 ~ 0.038998634 -1.2577316 1.049665) (-0.28503713 0.3086878 -0.0664424 ~ 0.009971998 -0.3216035 0.2684006) ... (-0.19921443 0.21574406 -0.04643706 ~ 0.0069694985 -0.22477092 0.18758704) (0.9453751 -1.0238167 0.22036776 ~ -0.03307386 1.0666538 -0.89019716)) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} Btw, the proceed functions provides more; for measuring execution time, and profiling your computation node. CONCEPTS> (proceed-time (!add 1 1)) [proceed-time] build -> Evaluation took: 0.000 seconds of real time 0.000039 seconds of total run time (0.000038 user, 0.000001 system) 100.00% CPU 86,386 processor cycles 0 bytes consed [proceed-time] With allocation time: Evaluation took: 0.000 seconds of real time 0.000030 seconds of total run time (0.000021 user, 0.000009 system) 100.00% CPU 65,918 processor cycles 0 bytes consed [proceed-time] Without allocation time: Evaluation took: 0.000 seconds of real time 0.000009 seconds of total run time (0.000008 user, 0.000001 system) 100.00% CPU 17,032 processor cycles 0 bytes consed CONCEPTS> (proceed-bench (!softmax (randn `(100 100)))) [Sorted by Instructions] Time(s) | Instruction ( * - Beyonds the average execution time) 1.4e-5 | <WfInst[op=MOVETENSORNODE-CPUTENSOR] : TID10689 <= op(TID10689{float, (100 100)} <Input>TID10601{float, (100 100)})> 2.0e-6 | <WfInst[op=VIEWTENSORNODE-T] : TID10683 <= op(TID10641{float, (100 1)} TID10683{float, (100 1)})> 2.0e-6 | <WfInst[op=SCALARMUL-CPUTENSOR] : TID10683 <= op(TID10683{float, (100 1)} <Input>TID10610{float, (1)})> 1.0e-6 | <WfInst[op=VIEWTENSORNODE-T] : TID10683 <= op(TID10683{float, (100 100)} TID10683{float, (100 1)})> 8.4e-5* | <WfInst[op=ADDNODE-CPUTENSOR] : TID10683 <= op(TID10683{float, (100 100)} <Input>TID10601{float, (100 100)})> 1.0e-6 | <WfInst[op=VIEWTENSORNODE-T] : TID10683 <= op(TID10683{float, (100 1)} TID10683{float, (100 100)})> 3.3e-5* | <WfInst[op=SCALARDIV-CPUTENSOR] : TID10683 <= op(TID10683{float, (100 1)} <Input>TID10605{float, (1)})> 2.0e-6 | <WfInst[op=VIEWTENSORNODE-T] : TID10683 <= op(TID10683{float, (100 100)} TID10683{float, (100 1)})> 5.5e-5* | <WfInst[op=SUBNODE-CPUTENSOR] : TID10689 <= op(TID10689{float, (100 100)} TID10683{float, (100 100)})> 1.0e-5 | <WfInst[op=EXPNODE-CPUTENSOR] : TID10689 <= op(TID10689{float, (100 100)} TID10689{float, (100 100)})> 2.0e-6 | <WfInst[op=SCALARMUL-CPUTENSOR] : TID10683 <= op(TID10683{float, (100 1)} <Input>TID10738{float, (1)})> 1.0e-6 | <WfInst[op=VIEWTENSORNODE-T] : TID10683 <= op(TID10683{float, (100 100)} TID10683{float, (100 1)})> 1.12e-4* | <WfInst[op=ADDNODE-CPUTENSOR] : TID10683 <= op(TID10683{float, (100 100)} TID10689{float, (100 100)})> 5.5e-5* | <WfInst[op=DIVNODE-CPUTENSOR] : TID10689 <= op(TID10689{float, (100 100)} TID10683{float, (100 100)})> 14 Instructions | 7 Tensors | Overheads due to SV4BW(...) -> 6.0e-6(s) Total Time: 3.74e-4 sec [Sorted by topK] Instruction | Total time (s) | Time/Total (n-sample=1) <WfInst[op=ADDNODE-CPUTENSOR] | 1.9600001e-4 | 52.406418% <WfInst[op=SUBNODE-CPUTENSOR] | 5.5e-5 | 14.705883% <WfInst[op=DIVNODE-CPUTENSOR] | 5.5e-5 | 14.705883% <WfInst[op=SCALARDIV-CPUTENSOR] | 3.3e-5 | 8.823529% <WfInst[op=MOVETENSORNODE-CPUTENSOR] | 1.4e-5 | 3.7433155% <WfInst[op=EXPNODE-CPUTENSOR] | 1.0e-5 | 2.6737967% <WfInst[op=VIEWTENSORNODE-T] | 7.0e-6 | 1.8716577% <WfInst[op=SCALARMUL-CPUTENSOR] | 4.0e-6 | 1.0695187% {MYTENSOR[float] :shape (100 100) :id TID10689 :vec-state [computed] ((0.01518923 0.049908508 0.017617546 ~ 0.012779288 0.0020210263 0.0018769704) (0.05841501 0.016887225 0.007463644 ~ 0.0027627628 0.0033548716 0.0028829984) ... (0.0016393916 0.0048142807 0.057292804 ~ 0.0036875184 0.015796835 0.0014674882) (0.03651239 0.0038913002 0.009065828 ~ 0.0060259635 0.012457987 0.003022789)) :facet :input :belongs-to :memory-pool :requires-grad NIL :backward <Node: DIVNODE-CPUTENSOR (A[~] B[~] -> A[~])>} Lazy Evaluation Programming is not as hard as you'd think - Functions generated by AbstractNode can be embedded in your CommonLisp code in a natural way. As the simplest and basic example, the build function traces and compiles the network from the endpoints of the computation nodes, returning the Compiled-Composite class which can invoked its forward propagation by the forward method: (let ((a (make-input `(A B) :A)) (b (make-input `(A B) :B))) (let ((model (build (!sum (!mul a b)) :inputs `(:A :B)))) (print model) ;; model is a compiled function: f(a b) (forward model (randn `(3 3)) (randn `(3 3))))) ;;<Compiled-Composite(allocated-p=NIL) ;; forward : forward(model A B) -> CPUTENSOR{FLOAT}(1 1) ;; backward : backward(model) -> t ;; memory-pool : two tensor(s) ;; L {8.0e-6+((A B) x 4.0e-6)}MB ;; inputs: ;; A -> (A B) ;; B -> (A B) ;;> ;;{CPUTENSOR[float] :shape (1 1) -> :view (<(BROADCAST 1)> <(BROADCAST 1)>) -> :visible-shape (1 1) :named ChainTMP646587 ;; ((1.0858848)) ;; :facet :input ;; :requires-grad NIL ;; :backward NIL} And node->defun , node->lambda . If you want to create a dedicated Tensor to store the calculation results, you should use the make-input function with name=nil . InputTensor does not guarantee that the elements are filled with zeros, but the compiler can automatically reconnect them and reduce memory usage. (defun my-matmul (a b) (let* ((m (first (shape a))) (k (second (shape b))) (c (make-input `(,m ,k) nil))) ;; C as output trensor (call (MatmulNode-Revisit) a b c))) (print (proceed (my-matmul (randn `(3 3)) (randn `(3 3))))) ;; Composing (!softmax (matmul a b)) (node->defun %mm-softmax (A[m n] B[n k] -> C[m k]) (!softmax (my-matmul a b))) ;; Here's also (node->lambda (A[m n] B[n k] -> C[m k]) ...) ;; JIT Enabled Matrix Operations (defun local-cached-matmul () ;; Works like Lisp Function (print (time (%mm-softmax (randn `(3 3)) (randn `(3 3))))) (print (time (%mm-softmax (randn `(3 3)) (randn `(3 3)))))) And last, The set-devices-toplevel function make cl-waffe2 use MyTensor anywhere. (set-devices-toplevel 'MyTensor 'CPUTensor 'LispTensor)","title":"Nodes are Abstract, Lazy, and Small."},{"location":"overview/#advanced-network-constructions","text":"We call a set of composed AbstractNode , or a chunk of nodes with trainable parameters, a Composite defined by the defmodel macro. (defmodel (LayerNorm-Revisit (self normalized-shape &key (eps 1.0e-5) (affine T)) :slots ((alpha :initform nil :accessor alpha-of) (beta :initform nil :accessor beta-of) (shape :initform nil :initarg :normalized-shape :accessor dim-of) (eps :initform nil :initarg :eps :accessor eps-of)) ;; Optional :where (X[~ normalized-shape] -> out[~ normalized-shape]) :on-call-> layer-norm) ;; Constructor (when affine (setf (alpha-of self) (parameter (ax+b `(,@normalized-shape) 0 1)) (beta-of self) (parameter (ax+b `(,@normalized-shape) 0 0))))) (defmethod layer-norm ((self LayerNorm-Revisit) x) (with-slots ((alpha alpha) (beta beta)) self (let* ((last-dim (length (dim-of self))) (u (!mean x :axis (- last-dim) :keepdims t)) (s (!mean (!expt (!sub x u) 2) :axis (- last-dim) :keepdims t)) (x (!div (!sub x u) (!sqrt (!add (->contiguous s) (eps-of self)))))) ;; !flexible = (%transform alpha[i] -> [~ i]) ;; both inserts an broadcastable axis (if (and alpha beta) (!add (!mul x (%transform alpha[i] -> [~ i])) (!flexible beta)) x)))) As a chunk of nodes with trainable parameter, Composites can be used merely a subroutine: (proceed (call (LayerNorm-Revisit `(10)) (randn `(10 10 10)))) If you use Composite as a set of composed AbstractNode , they're compiled into another types: (defmodel (Softmax-Model (self) :on-call-> ((self x) (declare (ignore self)) (let* ((x1 (!sub x (!mean x :axis 1 :keepdims t))) (z (!sum (!exp x1) :axis 1 :keepdims t))) (!div (!exp x1) z))))) (defmodel-as (Softmax-Model) :where (A[~] -> B[~]) :asif :function :named %softmax) (%softmax (randn `(10 10)))","title":"Advanced Network Constructions"},{"location":"overview/#composing","text":"It is not elegant to use call more than once when composing multiple models. (call (AnyModel1) (call (AnyModel2) (call (AnyModel3) X))) Instead, you can use the the call-> function: (call-> X (AnyModel1) (AnyModel2) (AnyModel3)) If you wanted to insert a function to construct a computation node here, you can also use asnode function to make the function recognised as a Composite. (call-> X (AnyModel1) (asnode #'!softmax) (asnode #'!view 0) ;; Slicing the tensor: (!view x 0 t ...) (asnode #'!add 1.0) ;; X += 1.0 (asnode !matmul Y) ;; X <- Matmul(X, Y) ) If the Composite can be implemented using only call-> , the defsequence can be used for a short description: (defsequence MLP (in-features) \"Docstring (optional)\" (LinearLayer in-features 512) (asnode #'!tanh) (LinearLayer 512 256) (asnode #'!tanh) (LinearLayer 256 10)) ;; Sequences can receive only a single argument. (call (MLP 786) (randn `(10 786)))","title":"Composing"},{"location":"overview/#make-everything-user-extensible","text":"","title":"Make everything user-extensible"},{"location":"overview/#customized-autodiff-cl-waffe2-as-a-graph-processing-library","text":"(TODO) (defclass MyScalarTensor (ScalarTensor) nil) (set-devices-toplevel 'MyTensor 'CPUTensor 'LispTensor 'MyScalarTensor) (define-op (MyMul (self) :where (A[scal] B[scal] -> A[scal] where scal = 1) :out-scalar-p t :save-for-backward-names (a b) :forward ((self a b) (with-setting-save4bw ((a a) (b b)) (setf (tensor-vec a) (* (tensor-vec a) (tensor-vec b))) a)) :backward ((self dy) (with-reading-save4bw ((a a) (b b)) (values (make-tensor (* (tensor-vec dy) (tensor-vec b))) (make-tensor (* (tensor-vec dy) (tensor-vec a)))))))) (define-op (MySin (self) :where (A[scal] out[scal] -> out[scal] where scal = 1) :out-scalar-p t :save-for-backward-names (a) :forward ((self a out) (with-setting-save4bw ((a a)) (setf (tensor-vec out) (sin (tensor-vec a))) out)) :backward ((self dy) (with-reading-save4bw ((a a)) (values (make-tensor (* (tensor-vec dy) (cos (tensor-vec a)))) nil))))) (defun !mymul (a b) (call (MyMul) a b)) (defun !mysin (x) (call (MySin) x (make-clone x))) (defun try-original-autodiff () (let ((a (parameter (make-tensor 1)))) (proceed-backward (!mysin (!mysin a))) (grad a)))","title":"Customized Autodiff - cl-waffe2 as a graph processing library"},{"location":"overview/#differentiable-programming","text":"(TODO) (defoptimizer (MySGD (self param &key (lr 1e-3)) :slots ((lr :initarg :lr :reader sgd-lr)))) (node->defun %step-sgd (Param[~] Grad[~] Lr[scal] -> Param[~] where scal = 1) (A-=B param (A*=scal grad lr))) (defmethod step-optimize ((optimizer MySGD)) (let* ((lr (make-tensor (sgd-lr optimizer))) (param (read-parameter optimizer)) (grad (grad param))) (with-no-grad (%step-sgd param grad lr)))) (defun simple-opt-model () (let* ((loss (!mean (!matmul (parameter (randn `(3 3))) (parameter (randn `(3 3)))))) (model (build loss))) (mapc (hooker x (MySGD x :lr 1e-3)) (model-parameters model)) (forward model) (backward model) (mapc #'call-optimizer! (model-parameters model)))) ({MYTENSOR[float] :shape (3 3) -> :view (<T> <T>) -> :visible-shape (3 3) ((0.25052267 -0.16212857 -1.3183842) (-1.078968 0.27860558 0.40701634) (-0.10987697 -1.2562615 0.6179133)) :facet :exist :requires-grad T :optimizer <AbstractOptimizer: MYSGD() -> TID12604>} {MYTENSOR[float] :shape (3 3) -> :view (<T> <T>) -> :visible-shape (3 3) ((-0.5223165 2.3579814 0.13172081) (0.57671905 0.56324756 1.1230979) (0.10274803 0.008530198 1.7588508)) :facet :exist :requires-grad T :optimizer <AbstractOptimizer: MYSGD() -> TID12610>}) See also: Examples","title":"Differentiable Programming"},{"location":"overview/#maximize-the-benefits-of-graph-level-optimization","text":"(TODO) ~~ [Steps] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 1. Network Construction : Create a network of AbstractNode with multiple backends. 2. Sorting/Pruning : Sort the network and prune unused nodes. 3. In-place mutation : Optimize the list by deleting unused MoveTensorNode. 4. More Localize : Reconnecting InputTensors, the comiler optimizes the locality of memory. 5. Reschedule : Create an allocation planning considering 4. and in-place ops: !view !permute !reshape etc. 6. Backward(Optional) : Construct backward propagation 7. Adjoint Optimization : Minimizes the number of copies arising at adjoints 8. Compile/Inline : If any, compiles lisp blueprints generated by call-with-view (If cached, ignored) 9. Rewriting : If any, replaces the list of declared patterns by the defpath macro 10. Completed -> Compiled-Composite is cached/inlined everywhere ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~","title":"Maximize the benefits of Graph-Level Optimization"},{"location":"overview/#interop-common-lisp-array-and-cl-waffe2-abstracttensor","text":"(TODO) See also: Converting AbstractTensor and other arrays","title":"Interop: Common Lisp Array and cl-waffe2 AbstractTensor"},{"location":"overview/#debugging","text":"(TODO) cl-waffe2 is enough clever to detect Shape-Error and suggest an alternative arising from wrong inputs. In this case, both ranks are invaild because broadcasing rank-up rule is not applied in cl-waffe2: (!add (randn `(3 3)) (randn `(3))) If you do this, you will get the following error before running the operation [cl-waffe] Shaping-Error: [Shaping Error]: The AbstractNode ADDNODE-CPUTENSOR was called with invaild arguments. The constraint: ADDNODE-CPUTENSOR: (A[~] B[~] -> A[~]) Received: (forward (ADDNODE-CPUTENSOR ...) CPUTENSOR{FLOAT}(3) CPUTENSOR{FLOAT}(3) \u2500 B: The length of ~~ do not match. The Rank is too low ) B: \u2500 the 1th shape is (3) but it violates ~ = (3 3) \u2500 The given rank 2 do not match declared: (3) Excepted: (forward (ADDNODE-CPUTENSOR ...) B(3) \u2500> B: ) B: \u2500 Use (!flexible tensor) to explict a rank-up rule of broadcasting. When adding or repeating ranks by broadcasting rule, it is necessary to declare in advance in which position they are to be added: ;; X[a b c] -> X[~ a b c] (!flexible x :at 0) ;; X[a] -> X[~ a] (%transform (randn `(3))[i] -> [~ i]) Following the suggestion, fix the code: (defparmeter out (!add (randn `(3 3)) (!flexible (randn `(3))))) And passed: (proceed out) This is a case of before execution, speaking of runtime error (e.g.: floating-point overflow), it gets a bit complicated; you have to face the disassembled code to find out the details. When doing (!sqrt x) where x is a negative number: (proceed (!sin (!sqrt (!mul -1.0 (!sin (ax+b `(3 3) 0 1)))))) As excepted it produces FLOATING-POINT-INVAILD-OPERATION: cl-waffe2 VM: Encountered Runtime Error at 3th instruction. disassemble: 0 : <WfInst[op=MOVETENSORNODE-CPUTENSOR] : TID14200 <= op(TID14200{float, (3 3)} <Input>TID14197{float, (3 3)})> 1 : <WfInst[op=SINNODE-CPUTENSOR] : TID14200 <= op(<Input>TID14197{float, (3 3)} TID14200{float, (3 3)})> 2 : <WfInst[op=SCALARMUL-CPUTENSOR] : TID14200 <= op(TID14200{float, (3 3)} <Input>TID14218{float, (1)})> 3*: <WfInst[op=SQRTNODE-CPUTENSOR] : TID14200 <= op(TID14200{float, (3 3)} TID14200{float, (3 3)})> 4 : <WfInst[op=SINNODE-CPUTENSOR] : TID14200 <= op(TID14200{float, (3 3)} TID14200{float, (3 3)})> condition: arithmetic error FLOATING-POINT-INVALID-OPERATION signalled Use the disassemble-waffe2-ir function to check the full disassembled code instead of proceed: (disassemble-waffe2-ir (!sin (!sqrt (!mul -1.0 (!sin (parameter (ax+b `(3 3) 0 1))))))) disassemble-waffe2-ir: [Forward]: <WfInst[op=MOVETENSORNODE-CPUTENSOR] : TID14654 <= op(TID14654{float, (3 3)} <Param>TID14649{float, (3 3)})> <WfInst[op=SINNODE-CPUTENSOR] : TID14654 <= op(<Param>SV4BW(TID14649{float, (3 3)}) TID14654{float, (3 3)})> <WfInst[op=SCALARMUL-CPUTENSOR] : TID14654 <= op(SV4BW(TID14654{float, (3 3)}) <Input>TID14675{float, (1)})> <WfInst[op=SQRTNODE-CPUTENSOR] : TID14654 <= op(SV4BW(TID14654{float, (3 3)}) TID14654{float, (3 3)})> <WfInst[op=SINNODE-CPUTENSOR] : TID14654 <= op(SV4BW(TID14654{float, (3 3)}) TID14654{float, (3 3)})> 5 Instructions | 2 Tensors | 1 Scalars [Pullback]: <WfInst[op=MOVETENSORNODE-CPUTENSOR] : TID14764 <= op(TID14764{float, (3 3)} <Input>TID14742{float, (3 3)})> <WfInst[op=COSNODE-CPUTENSOR] : TID14732 <= op(TID14732{float, (3 3)} TID14732{float, (3 3)})> <WfInst[op=MULNODE-CPUTENSOR] : TID14764 <= op(TID14764{float, (3 3)} TID14732{float, (3 3)})> <WfInst[op=INVERSETENSORNODE-CPUTENSOR] : TID14710 <= op(TID14710{float, (3 3)})> <WfInst[op=SCALARMUL-CPUTENSOR] : TID14710 <= op(TID14710{float, (3 3)} <Input>TID14782{float, (1)})> <WfInst[op=MULNODE-CPUTENSOR] : TID14764 <= op(TID14764{float, (3 3)} TID14710{float, (3 3)})> <WfInst[op=SCALARMUL-CPUTENSOR] : TID14764 <= op(TID14764{float, (3 3)} <Input>TID14675{float, (1)})> <WfInst[op=COSNODE-CPUTENSOR] : TID14665 <= op(TID14665{float, (3 3)} TID14665{float, (3 3)})> <WfInst[op=MULNODE-CPUTENSOR] : TID14764 <= op(TID14764{float, (3 3)} TID14665{float, (3 3)})> <WfInst[op=MOVETENSORNODE-CPUTENSOR] : <Input>TID14651 <= op(<Input>TID14651{float, (3 3)} TID14764{float, (3 3)})> 10 Instructions | 6 Tensors | 2 Scalars","title":"Debugging"},{"location":"overview/#graph-rewriting","text":"(Still Experimental, but coming soon...) We gonna talk about defpath which enables theano-like symbolic differentiation and device-specific optimizations.","title":"Graph Rewriting"},{"location":"utils/","text":"[package] cl-waffe2 The cl-waffe2 package provides utilities for a wide range needs: Object Convertion, Advance Network Construction, Trainer, and so on. [Tensor Facet] Converting AbstractTensor <-> Anything If you're looking for the way to create an AbstractTensor from a Common Lisp array or manipulate an AbstractTensor as a Common Lisp array, this section is perfect for you. Here we provide a common APIs for the conversion between AbstractTensor and other matrix types. The most basic method is a convert-tensor-facet method and we're welcome to add a new method by users. Other functions are macros work by assigning a method according to the type of object and the direction. Plus, conversions are performed while sharing pointers as much as possible. If turned out to be they can't be shared, the with-facet macro forces a copy to be performed and pseudo-synchronises them. [generic] convert-tensor-facet (convert-tensor-facet from to) Converts the given object (anything is ok; from= AbstractTensor simple-array etc as long as declared) into the direction indicated in to . Inputs From[Anything] The object to be converted To[Symbol] Indicates to where the object is converted Adding an extension Welcome to define the addition of method by users. For example, Fixnum -> AbstractTensor convertion can be written like: (defmethod convert-tensor-facet ((from fixnum) (to (eql 'AbstractTensor))) (make-tensor from)) (print (change-facet 1 :direction 'AbstractTensor)) ;;{SCALARTENSOR[float] ;; 1.0 ;; :facet :exist ;; :requires-grad NIL ;; :backward NIL} If any object to AbstractTensor conversion is implemented, it is strongly recommended to add it to this method. Example (convert-tensor-facet (randn `(3 3)) 'simple-array) See also: convert-facet (more convenient API) [function] change-facet (change-facet (array-from &key (direction 'array))) By calling the conver-tensor-facet method, this function can change the facet of given array-form into the direction . (Just an alias of (convert-tensor-facet array-from direction) ) See also: convert-tensor-facet Standard Directions We provide these symbols as a direction in standard. array : Any Object -> Common Lisp Standard ND Array simple-array : Any Object -> Common Lisp Simple-Array AbstractTensor : Any Object -> AbstractTensor. If couldn't determine the dtype, dtype of the first element of array-from is used instead. [function] ->tensor Using the convert-tensor-facet method, converts the given object into AbstractTensor. Example (->tensor #2A((1 2 3) (4 5 6))) [macro] with-facet (with-facet (var (object-from &key (direction 'simple-array)) &body body)) By using the convert-tensor-facet method, this macro changes the facet of object-from into the direction . If you want to apply any operations to object-from and ensure that modifications are applied to the object-from , set sync =t and moves element forcibly (only available when direction= 'abstracttensor`). This is useful when editing AbstractTensor or calling other libraries without making copies. For a more explict workflow, see below: [macro with-facet] \u2193 [Binding var = (convert-tensor-facet object-from direction)] \u2193 [Processing body] \u2193 [If sync=t, (setf (tensor-vec object-from) (tensor-vec (convert-tensor-facet var 'AbstractTensor)))] Example (let ((a (randn `(3 3)))) (with-facet (a* (a :direction 'simple-array)) (print a*) (setf (aref a* 0) 10.0)) a) ;; Operations called with simple-array a*, also effects on a. #(0.92887694 -0.710253 1.2339028 -0.78008 1.6763965 0.93389416 -0.5691122 1.6552123 -0.108502984) {CPUTENSOR[float] :shape (3 3) ((10.0 -0.710253 1.2339028) (-0.78008 1.6763965 0.93389416) (-0.5691122 1.6552123 -0.108502984)) :facet :exist :requires-grad NIL :backward NIL} See also: with-facets [macro] with-facets Bundles several with-facet macro. (with-facets ((a ((randn `(3 3)) :direction 'array)) (b ((randn `(3 3)) :direction 'array))) (print a) (print b)) #2A((-0.020553567 -0.016298171 -2.0616999) (0.68268335 0.33567926 -0.79862773) (1.7132819 0.8081283 0.47327513)) #2A((-0.9344233 0.3149136 -0.8516832) (0.17137305 -0.026806794 -0.8192844) (0.19916026 -0.5102597 1.1834184)) Advanced Network Construction Powerful macros in Common Lisp enabled me to provide an advanced APIs for make the construction of nodes more systematic, and elegant. Computational nodes that are lazy evaluated can be treated as pseudo-models, for example, even if they are created via functions. And, APIs in this section will make it easy to compose/compile several nodes. [function] asnode (asnode function &rest arguments) Wraps the given function which excepted to create computation nodes with the Encapsulated-Node composite. That is, functions are regarded as a Composite and be able to use a variety of APIs (e.g.: call , call-> , defmodel-as ...). In principle, a function takes one argument and returns one value, but by adding more arguments the macro automatically wraps the function to satisfy it. For example, (asnode #'!add 1.0) is transformed into: #'(lambda (x) (!add x 1.0)) . So the first arguments should receive AbstractTensor. Usage: call-> It is not elegant to use call more than once when composing multiple models. (call (AnyModel1) (call (AnyModel2) (call (AnyModel3) X))) Instead, you can use the call-> function: (call-> X (AnyModel1) (AnyModel2) (AnyModel3)) However, due to constrains of call , it is not possible to place functions here. asnode is exactly for this! (call-> X (AnyModel1) (asnode #'!softmax) (asnode #'!view 0) ;; Slicing the tensor: (!view x 0 t ...) (asnode #'!add 1.0) ;; X += 1.0 (asnode !matmul Y) ;; X <- Matmul(X, Y) ) Usage2: defmodel-as The macro cl-waffe2/vm.nodes:defmodel-as is able to define new functions/nodes from existing Composite . However, this macro only needs the traced computation nodes information to do this. As the simplest case, compiling the AbstractNode SinNode (which is callable as !sin ) into static function, matrix-sin . ;; The number of arguments is anything: (defmodel-as (asnode #'(lambda (x y z) ... is also ok (defmodel-as (asnode #'!sin) :where (A[~] -> B[~]) :asif :function :named matrix-sin) (matrix-sin (ax+b `(10 10) 0 1)) ;; <- No compiling overhead. Just works like Numpy On a side note: Encapsulated-Node itself doesn't provide for :where declaration, but you can it with the keyword :where . [function] call-> (call-> input &rest nodes) Starting from input , this macro applies a composed function. (call-> (randn `(3 3)) ;; To the given input: (asnode #'!add 1.0) ;; | (asnode #'!relu) ;; | Applies operations in this order. (asnode #'!sum)))) ;; \u2193 nodes could be anything as long as the call method can handle, but I except node= Composite , AbstractNode , and (asnode function ...) . [macro] defsequence (defsequence (name (&rest args) &optional docstring &rest nodes)) Defines a Composite that can be defined only by the call-> method. Inputs name[symbol] defines the new Composite after name args[list] a list of arguments that used to initialize nodes. Not for call . docstring[string] docstring Example (defsequence MLP (in-features) \"Docstring (optional)\" (LinearLayer in-features 512) (asnode #'!tanh) (LinearLayer 512 256) (asnode #'!tanh) (LinearLayer 256 10)) ;; Sequence can receive a single argument. (call (MLP 786) (randn `(10 786))) Tips: Use (sequencelist-nth n sequence-model) to read the nth layer of sequence. [macro] hooker (hooker bind optimizer) A convenient macro to hook AbstractOptimizers to each AbstractTensor. As the most straightforward explanation: this macro is expanded into this form. `(lambda (,bind) (hook-optimizer! ,bind ,optimizer)) where bind is excepted to be AbstractTensor, optimizer is a creation form of AbstractOptimizer , and the function hook-optimizer! hooks the given optimizer into bind. In cl-waffe2, one independent Optimizer must be initialised per parameter. This macro can be used to concisely describe the process of initialising the same Optimiser for many parameters. Example ;; (model-parameters compiled-composite) to read the list of all parameters in the network (let ((model (build (!matmul (parameter (randn `(3 3))) (parameter (randn `(3 3))))))) (mapc (hooker x (Adam X :lr 1e-3)) (model-parameters model)) (forward model) (backward model) (mapc #'call-optimizer! (model-parameters model))) [macro] node->lambda (node->lambda (&rest where) &body body) Creates a lambda function obtained by tracing and compiling the computation node described in body. Inputs where declares the shape transforms. the tensor names used here are the same as those used in body. (i.e.: everything is AbstractTensor) body Describe the construction of the computation node here. Example (node->lambda (A[~] -> B[~]) (!sin (!cos a))) (funcall * (randn `(3 3))) Note \u26a0\ufe0f Caches of functions are created for each location where this macro is located. Never place it inside a loop! [macro] node->defun (node->defun (name (&rest where) &body body)) Defines a function obtained by tracing and compiling the computation node described in the body. Inputs name[symbol] the function is defined after it where declares the shape transforms. the tensor names used here are the same as those used in body. body Describe the construction of the computation node here. Example (node->defun log-softmax (A[~] -> OUT[~]) (!softmax (!loge a) :axis 1)) (log-softmax (ax+b `(3 3) 0 1)) {CPUTENSOR[float] :shape (3 3) :id TID261835 ((0.33333334 0.33333334 0.33333334) (0.33333334 0.33333334 0.33333334) (0.33333334 0.33333334 0.33333334)) :facet :input :belongs-to :memory-pool :requires-grad NIL :backward NIL} [function] show-backends (show-backends &key (stream t)) collects and displays the current state of devices to the given stream Example (show-backends) \u2500\u2500\u2500\u2500\u2500[All Backends Tree]\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 [*]CPUTENSOR: OpenBLAS=available *simd-extension-p*=available \u2514[-]JITCPUTENSOR: compiler=gcc flags=(-fPIC -O3 -march=native) viz=NIL [*]LISPTENSOR: Common Lisp implementation on matrix operations \u2514[-]JITLISPTENSOR: To be deleted in the future release. do not use this. [-]SCALARTENSOR: is a special tensor for representing scalar values. \u2514[-]JITCPUSCALARTENSOR: Use with JITCPUTensor ([*] : in use, [-] : not in use.) Add a current-backend-state method to display the status. \u2500\u2500\u2500\u2500\u2500[*using-backend*]\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Priority: Higher <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>Lower CPUTENSOR LISPTENSOR (use with-devices macro or set-devices-toplevel function to change this parameter.) [function] set-devices-toplevel (set-devices-toplevel &rest devices) Declares devices to use.","title":"cl-waffe2"},{"location":"utils/#package-cl-waffe2","text":"The cl-waffe2 package provides utilities for a wide range needs: Object Convertion, Advance Network Construction, Trainer, and so on.","title":"[package] cl-waffe2"},{"location":"utils/#tensor-facet-converting-abstracttensor-anything","text":"If you're looking for the way to create an AbstractTensor from a Common Lisp array or manipulate an AbstractTensor as a Common Lisp array, this section is perfect for you. Here we provide a common APIs for the conversion between AbstractTensor and other matrix types. The most basic method is a convert-tensor-facet method and we're welcome to add a new method by users. Other functions are macros work by assigning a method according to the type of object and the direction. Plus, conversions are performed while sharing pointers as much as possible. If turned out to be they can't be shared, the with-facet macro forces a copy to be performed and pseudo-synchronises them.","title":"[Tensor Facet] Converting AbstractTensor &lt;-&gt; Anything"},{"location":"utils/#generic-convert-tensor-facet","text":"(convert-tensor-facet from to) Converts the given object (anything is ok; from= AbstractTensor simple-array etc as long as declared) into the direction indicated in to .","title":"[generic] convert-tensor-facet"},{"location":"utils/#inputs","text":"From[Anything] The object to be converted To[Symbol] Indicates to where the object is converted","title":"Inputs"},{"location":"utils/#adding-an-extension","text":"Welcome to define the addition of method by users. For example, Fixnum -> AbstractTensor convertion can be written like: (defmethod convert-tensor-facet ((from fixnum) (to (eql 'AbstractTensor))) (make-tensor from)) (print (change-facet 1 :direction 'AbstractTensor)) ;;{SCALARTENSOR[float] ;; 1.0 ;; :facet :exist ;; :requires-grad NIL ;; :backward NIL} If any object to AbstractTensor conversion is implemented, it is strongly recommended to add it to this method.","title":"Adding an extension"},{"location":"utils/#example","text":"(convert-tensor-facet (randn `(3 3)) 'simple-array) See also: convert-facet (more convenient API)","title":"Example"},{"location":"utils/#function-change-facet","text":"(change-facet (array-from &key (direction 'array))) By calling the conver-tensor-facet method, this function can change the facet of given array-form into the direction . (Just an alias of (convert-tensor-facet array-from direction) ) See also: convert-tensor-facet","title":"[function] change-facet"},{"location":"utils/#standard-directions","text":"We provide these symbols as a direction in standard. array : Any Object -> Common Lisp Standard ND Array simple-array : Any Object -> Common Lisp Simple-Array AbstractTensor : Any Object -> AbstractTensor. If couldn't determine the dtype, dtype of the first element of array-from is used instead.","title":"Standard Directions"},{"location":"utils/#function-tensor","text":"Using the convert-tensor-facet method, converts the given object into AbstractTensor.","title":"[function] -&gt;tensor"},{"location":"utils/#example_1","text":"(->tensor #2A((1 2 3) (4 5 6)))","title":"Example"},{"location":"utils/#macro-with-facet","text":"(with-facet (var (object-from &key (direction 'simple-array)) &body body)) By using the convert-tensor-facet method, this macro changes the facet of object-from into the direction . If you want to apply any operations to object-from and ensure that modifications are applied to the object-from , set sync =t and moves element forcibly (only available when direction= 'abstracttensor`). This is useful when editing AbstractTensor or calling other libraries without making copies. For a more explict workflow, see below: [macro with-facet] \u2193 [Binding var = (convert-tensor-facet object-from direction)] \u2193 [Processing body] \u2193 [If sync=t, (setf (tensor-vec object-from) (tensor-vec (convert-tensor-facet var 'AbstractTensor)))]","title":"[macro] with-facet"},{"location":"utils/#example_2","text":"(let ((a (randn `(3 3)))) (with-facet (a* (a :direction 'simple-array)) (print a*) (setf (aref a* 0) 10.0)) a) ;; Operations called with simple-array a*, also effects on a. #(0.92887694 -0.710253 1.2339028 -0.78008 1.6763965 0.93389416 -0.5691122 1.6552123 -0.108502984) {CPUTENSOR[float] :shape (3 3) ((10.0 -0.710253 1.2339028) (-0.78008 1.6763965 0.93389416) (-0.5691122 1.6552123 -0.108502984)) :facet :exist :requires-grad NIL :backward NIL} See also: with-facets","title":"Example"},{"location":"utils/#macro-with-facets","text":"Bundles several with-facet macro. (with-facets ((a ((randn `(3 3)) :direction 'array)) (b ((randn `(3 3)) :direction 'array))) (print a) (print b)) #2A((-0.020553567 -0.016298171 -2.0616999) (0.68268335 0.33567926 -0.79862773) (1.7132819 0.8081283 0.47327513)) #2A((-0.9344233 0.3149136 -0.8516832) (0.17137305 -0.026806794 -0.8192844) (0.19916026 -0.5102597 1.1834184))","title":"[macro] with-facets"},{"location":"utils/#advanced-network-construction","text":"Powerful macros in Common Lisp enabled me to provide an advanced APIs for make the construction of nodes more systematic, and elegant. Computational nodes that are lazy evaluated can be treated as pseudo-models, for example, even if they are created via functions. And, APIs in this section will make it easy to compose/compile several nodes.","title":"Advanced Network Construction"},{"location":"utils/#function-asnode","text":"(asnode function &rest arguments) Wraps the given function which excepted to create computation nodes with the Encapsulated-Node composite. That is, functions are regarded as a Composite and be able to use a variety of APIs (e.g.: call , call-> , defmodel-as ...). In principle, a function takes one argument and returns one value, but by adding more arguments the macro automatically wraps the function to satisfy it. For example, (asnode #'!add 1.0) is transformed into: #'(lambda (x) (!add x 1.0)) . So the first arguments should receive AbstractTensor.","title":"[function] asnode"},{"location":"utils/#usage-call-","text":"It is not elegant to use call more than once when composing multiple models. (call (AnyModel1) (call (AnyModel2) (call (AnyModel3) X))) Instead, you can use the call-> function: (call-> X (AnyModel1) (AnyModel2) (AnyModel3)) However, due to constrains of call , it is not possible to place functions here. asnode is exactly for this! (call-> X (AnyModel1) (asnode #'!softmax) (asnode #'!view 0) ;; Slicing the tensor: (!view x 0 t ...) (asnode #'!add 1.0) ;; X += 1.0 (asnode !matmul Y) ;; X <- Matmul(X, Y) )","title":"Usage: call-&gt;"},{"location":"utils/#usage2-defmodel-as","text":"The macro cl-waffe2/vm.nodes:defmodel-as is able to define new functions/nodes from existing Composite . However, this macro only needs the traced computation nodes information to do this. As the simplest case, compiling the AbstractNode SinNode (which is callable as !sin ) into static function, matrix-sin . ;; The number of arguments is anything: (defmodel-as (asnode #'(lambda (x y z) ... is also ok (defmodel-as (asnode #'!sin) :where (A[~] -> B[~]) :asif :function :named matrix-sin) (matrix-sin (ax+b `(10 10) 0 1)) ;; <- No compiling overhead. Just works like Numpy On a side note: Encapsulated-Node itself doesn't provide for :where declaration, but you can it with the keyword :where .","title":"Usage2: defmodel-as"},{"location":"utils/#function-call-","text":"(call-> input &rest nodes) Starting from input , this macro applies a composed function. (call-> (randn `(3 3)) ;; To the given input: (asnode #'!add 1.0) ;; | (asnode #'!relu) ;; | Applies operations in this order. (asnode #'!sum)))) ;; \u2193 nodes could be anything as long as the call method can handle, but I except node= Composite , AbstractNode , and (asnode function ...) .","title":"[function] call-&gt;"},{"location":"utils/#macro-defsequence","text":"(defsequence (name (&rest args) &optional docstring &rest nodes)) Defines a Composite that can be defined only by the call-> method.","title":"[macro] defsequence"},{"location":"utils/#inputs_1","text":"name[symbol] defines the new Composite after name args[list] a list of arguments that used to initialize nodes. Not for call . docstring[string] docstring","title":"Inputs"},{"location":"utils/#example_3","text":"(defsequence MLP (in-features) \"Docstring (optional)\" (LinearLayer in-features 512) (asnode #'!tanh) (LinearLayer 512 256) (asnode #'!tanh) (LinearLayer 256 10)) ;; Sequence can receive a single argument. (call (MLP 786) (randn `(10 786))) Tips: Use (sequencelist-nth n sequence-model) to read the nth layer of sequence.","title":"Example"},{"location":"utils/#macro-hooker","text":"(hooker bind optimizer) A convenient macro to hook AbstractOptimizers to each AbstractTensor. As the most straightforward explanation: this macro is expanded into this form. `(lambda (,bind) (hook-optimizer! ,bind ,optimizer)) where bind is excepted to be AbstractTensor, optimizer is a creation form of AbstractOptimizer , and the function hook-optimizer! hooks the given optimizer into bind. In cl-waffe2, one independent Optimizer must be initialised per parameter. This macro can be used to concisely describe the process of initialising the same Optimiser for many parameters.","title":"[macro] hooker"},{"location":"utils/#example_4","text":";; (model-parameters compiled-composite) to read the list of all parameters in the network (let ((model (build (!matmul (parameter (randn `(3 3))) (parameter (randn `(3 3))))))) (mapc (hooker x (Adam X :lr 1e-3)) (model-parameters model)) (forward model) (backward model) (mapc #'call-optimizer! (model-parameters model)))","title":"Example"},{"location":"utils/#macro-node-lambda","text":"(node->lambda (&rest where) &body body) Creates a lambda function obtained by tracing and compiling the computation node described in body.","title":"[macro] node-&gt;lambda"},{"location":"utils/#inputs_2","text":"where declares the shape transforms. the tensor names used here are the same as those used in body. (i.e.: everything is AbstractTensor) body Describe the construction of the computation node here.","title":"Inputs"},{"location":"utils/#example_5","text":"(node->lambda (A[~] -> B[~]) (!sin (!cos a))) (funcall * (randn `(3 3)))","title":"Example"},{"location":"utils/#note","text":"\u26a0\ufe0f Caches of functions are created for each location where this macro is located. Never place it inside a loop!","title":"Note"},{"location":"utils/#macro-node-defun","text":"(node->defun (name (&rest where) &body body)) Defines a function obtained by tracing and compiling the computation node described in the body.","title":"[macro] node-&gt;defun"},{"location":"utils/#inputs_3","text":"name[symbol] the function is defined after it where declares the shape transforms. the tensor names used here are the same as those used in body. body Describe the construction of the computation node here.","title":"Inputs"},{"location":"utils/#example_6","text":"(node->defun log-softmax (A[~] -> OUT[~]) (!softmax (!loge a) :axis 1)) (log-softmax (ax+b `(3 3) 0 1)) {CPUTENSOR[float] :shape (3 3) :id TID261835 ((0.33333334 0.33333334 0.33333334) (0.33333334 0.33333334 0.33333334) (0.33333334 0.33333334 0.33333334)) :facet :input :belongs-to :memory-pool :requires-grad NIL :backward NIL}","title":"Example"},{"location":"utils/#function-show-backends","text":"(show-backends &key (stream t)) collects and displays the current state of devices to the given stream","title":"[function] show-backends"},{"location":"utils/#example_7","text":"(show-backends) \u2500\u2500\u2500\u2500\u2500[All Backends Tree]\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 [*]CPUTENSOR: OpenBLAS=available *simd-extension-p*=available \u2514[-]JITCPUTENSOR: compiler=gcc flags=(-fPIC -O3 -march=native) viz=NIL [*]LISPTENSOR: Common Lisp implementation on matrix operations \u2514[-]JITLISPTENSOR: To be deleted in the future release. do not use this. [-]SCALARTENSOR: is a special tensor for representing scalar values. \u2514[-]JITCPUSCALARTENSOR: Use with JITCPUTensor ([*] : in use, [-] : not in use.) Add a current-backend-state method to display the status. \u2500\u2500\u2500\u2500\u2500[*using-backend*]\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Priority: Higher <\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>Lower CPUTENSOR LISPTENSOR (use with-devices macro or set-devices-toplevel function to change this parameter.)","title":"Example"},{"location":"utils/#function-set-devices-toplevel","text":"(set-devices-toplevel &rest devices) Declares devices to use.","title":"[function] set-devices-toplevel"},{"location":"vm/","text":"cl-waffe2 VM The package cl-waffe2/vm is the central of system, and features are focused on low-level stuffs: compiling/optimizing/rewriting cl-waffe2 IRs and how they're executed. So, most of APIs are accesible by convinient API wrappers of other packages. Global Variables optimization level logging IR and Compiler WfInstruction compiler acceptor Analyzing compiled codes disassemble profiling Adding Symbolic Diff and Device-Specific Optimization FusionPathQuery defpath [parameter] *opt-level* This parameter indicates the degree of runtime error detection. Whichever you choose, cl-waffe2 never apply something unsafe code transformation. It takes the fixnum from 1 to 3, and the larger, the faster. Set 1 to use safety-mode, in every instructions, runtime error is checked. Set 2 to use middle-mode, runtime error is checked only when first execution. Set 3 to use fastest-mode, no runtime error checking is done. Again, whichever levels you choose, the graph cl-waffe2 executes is the same. So the effects on the performance is very small (within < 1e-4~1e-5 sec). In default, set to 2. [parameter] *logging-vm-execution* This parameter is useful for printing how all instructions are performed. If set to T, all results and arguments produced by executing cl-waffe2 IR is displayed into the terminal. In default, set to nil. [struct] WfInstruction WfInstruction is IR for cl-waffe2 VM. Its form is represented by an extended Wengert list which is able to return multiple outputs. In this document, we call this cl-waffe2 IR , and compiled cl-waffe2 code, that is, the list of WfInstruction is called InstructionSeq or iseq . Unlike other frameworks, this IR is not only used to represent backpropagation but also forward propagation. In cl-waffe2, WfInstruction is created by compiling AbstractNode, and its operation can be obtained by compiling lisp code or passing lambda functions. A single WfInstruction represents: out_to[0], out_to[1], ... <- \u03bb(Args[0], Args[1], Args[2], ...) ^wfop-out-to ^wfop-op ^wfop-args where \u03bb represents the operation. And, if any, ArgsN is wrapped with SV4BW . out_to[0], out_to[1], ... <- \u03bb(SV4BW(Args[0]), Args[1], Args[2], ...) ^ wfop-sv4bw SV4BW (i.e: save-for-backward) is a temporary tensor to compute backwards and cl-waffe2 reads the :save-for-backward slots in the define-impl macro, and the corresponding tensors are copied. Slots wfop-op[function] corresponds with compiled \u03bb function. wfop-node[AbstractNode or string or function] The node which generates \u03bb function. For the most case, this slot is set to AbstractNode , but the node is something special, (e.g.: CodeBlock , IfNode etc...), set to function . wfop-out-to[list of AbstractTensor] indicates list of tensors that results are to be stored. wfop-self[AbstractTensor] corresponds with out_target , that is, the tensor to store the results wfop-args[list of AbstractTensor] corresponds with (tensor-variable wfop-self) . tensors to be called with: arg1 arg2 arg3... . wfop-sv4bw[list of AbstractTensor] indicates list of tensors storing save-for-backward tensors. if the corresponding position is save-for-backward=nil , the corresponding position also become nil. [function] compile-forward-and-backward (compile-forward-and-backward toplevel &key (need-backward t) (fuse-p t) (compile-mode :default) (optimize-locality t)) Compiles into cl-waffe2 IR (so-called iseq) from the given toplevel to each leaf points (where detach-p=t or backward=null variables). toplevel is AbstractTensor with backwards. Tips: disassemble-waffe2-ir to display compiled Instruction Sequence. Return (values forward-iseq backward-iseq leaves[an list of AbstractTensor that appeared in the node] dout alloc-state) [function] accept-instructions (accept-instructions iseq) Evaluates generated cl-waffe2 IR sequence. iseq[list] an list of WFInstruction [function] disassemble-waffe2-ir (disassemble-waffe2-ir toplevel &key (backward t) (stream t) (fuse-p t)) Prints out the compiled cl-waffe2 IR from toplevel to each leaf points to stream . If backward was set to t, backward is also displayed. Example (with-output-to-string (out) (disassemble-waffe2-ir (!softmax (parameter (randn `(3 3))) :avoid-overflow nil) :stream out)) disassemble-waffe2-ir: [Forward]: <WfInst[op=MOVETENSORNODE-CPUTENSOR] : TID323 <= op(TID323{float, (3 3)} <Param>TID318{float, (3 3)})> <WfInst[op=EXPNODE-CPUTENSOR] : TID323 <= op(<Param>SV4BW(TID318{float, (3 3)}) TID323{float, (3 3)})> <WfInst[op=SCALARMUL-CPUTENSOR] : TID356 <= op(TID356{float, (3 1)} <Input>TID347{float, (1)})> <WfInst[op=VIEWTENSORNODE-T] : TID356 <= op(TID356{float, (3 3)} TID356{float, (3 1)})> <WfInst[op=ADDNODE-CPUTENSOR] : TID356 <= op(TID356{float, (3 3)} TID323{float, (3 3)})> <WfInst[op=DIVNODE-CPUTENSOR] : TID323 <= op(SV4BW(TID323{float, (3 3)}) SV4BW(TID356{float, (3 3)}))> 6 Instructions | 3 Tensors | 1 Scalars [Pullback]: <WfInst[op=MOVETENSORNODE-CPUTENSOR] : TID418 <= op(TID418{float, (3 3)} <Input>TID415{float, (3 3)})> <WfInst[op=DIVNODE-CPUTENSOR] : TID418 <= op(TID418{float, (3 3)} TID395{float, (3 3)})> <WfInst[op=MOVETENSORNODE-CPUTENSOR] : TID448 <= op(TID448{float, (3 3)} <Input>TID415{float, (3 3)})> <WfInst[op=SCALARMUL-CPUTENSOR] : TID448 <= op(TID448{float, (3 3)} <Input>TID445{float, (1)})> <WfInst[op=MULNODE-CPUTENSOR] : TID390 <= op(TID390{float, (3 3)} TID448{float, (3 3)})> <WfInst[op=VIEWTENSORNODE-T] : TID395 <= op(TID395{float, (3 3)} TID395{float, (3 1)})> <WfInst[op=MULNODE-CPUTENSOR] : TID395 <= op(TID395{float, (3 3)} TID395{float, (3 3)})> <WfInst[op=DIVNODE-CPUTENSOR] : TID390 <= op(TID390{float, (3 3)} TID395{float, (3 3)})> <WfInst[op=SYSTEM-LAZY-CONS-T] : TID418 TID390 <= op(TID418{float, (3 3)} TID390{float, (3 3)})> <WfInst[op=EXPNODE-CPUTENSOR] : TID334 <= op(TID334{float, (3 3)} TID334{float, (3 3)})> <WfInst[op=MULNODE-CPUTENSOR] : TID418 <= op(TID418{float, (3 3)} TID334{float, (3 3)})> <WfInst[op=MOVETENSORNODE-CPUTENSOR] : <Input>TID320 <= op(<Input>TID320{float, (3 3)} TID418{float, (3 3)})> 12 Instructions | 7 Tensors | 1 Scalars [function] benchmark-accept-instructions (benchmark-accept-instructions iseq &key (n-sample 1) (ignore-first-call nil) (stream t) (top-k 10)) Basically, the function benchmark-accept-instruction executes the given list of instructions with profiling execution time, but at the end of proess, displays the report into stream . Inputs n-sample[fixnum] repeats the iseq execution for n-sample times ignore-first-call[boolean] If t, ignores the first call to avoid including allocating time. stream[stream] the place to display the result top-k[fixnum] top-k slowest nodes are displayed at the end of report. Return result[AbstractTensor] See also: proceed-bench Example (with-output-to-string (out) (proceed-bench (!softmax (randn `(100 100))) :n-sample 100 :stream out)) [Sorted by Instructions] Time(s) | Instruction ( * - Beyonds the average execution time) 3.32e-4 | <WfInst[op=MOVETENSORNODE-CPUTENSOR] : TID700 <= op(TID700{float, (100 100)} <Input>TID612{float, (100 100)})> 9.9e-5 | <WfInst[op=VIEWTENSORNODE-T] : TID694 <= op(TID652{float, (100 1)} TID694{float, (100 1)})> 1.53e-4 | <WfInst[op=SCALARMUL-CPUTENSOR] : TID694 <= op(TID694{float, (100 1)} <Input>TID621{float, (1)})> 9.8e-5 | <WfInst[op=VIEWTENSORNODE-T] : TID694 <= op(TID694{float, (100 100)} TID694{float, (100 1)})> 0.006497* | <WfInst[op=ADDNODE-CPUTENSOR] : TID694 <= op(TID694{float, (100 100)} <Input>TID612{float, (100 100)})> 9.6e-5 | <WfInst[op=VIEWTENSORNODE-T] : TID694 <= op(TID694{float, (100 1)} TID694{float, (100 100)})> 0.002096* | <WfInst[op=SCALARDIV-CPUTENSOR] : TID694 <= op(TID694{float, (100 1)} <Input>TID616{float, (1)})> 9.1e-5 | <WfInst[op=VIEWTENSORNODE-T] : TID694 <= op(TID694{float, (100 100)} TID694{float, (100 1)})> 0.004272* | <WfInst[op=SUBNODE-CPUTENSOR] : TID700 <= op(TID700{float, (100 100)} TID694{float, (100 100)})> 0.001026 | <WfInst[op=EXPNODE-CPUTENSOR] : TID700 <= op(TID700{float, (100 100)} TID700{float, (100 100)})> 1.27e-4 | <WfInst[op=SCALARMUL-CPUTENSOR] : TID694 <= op(TID694{float, (100 1)} <Input>TID749{float, (1)})> 9.3e-5 | <WfInst[op=VIEWTENSORNODE-T] : TID694 <= op(TID694{float, (100 100)} TID694{float, (100 1)})> 0.006432* | <WfInst[op=ADDNODE-CPUTENSOR] : TID694 <= op(TID694{float, (100 100)} TID700{float, (100 100)})> 0.004236* | <WfInst[op=DIVNODE-CPUTENSOR] : TID700 <= op(TID700{float, (100 100)} TID694{float, (100 100)})> 14 Instructions | 7 Tensors | Overheads due to SV4BW(...) -> 4.82e-6(s) Total Time: 0.025648 sec [Sorted by topK] Instruction | Total time (s) | Time/Total (n-sample=100) <WfInst[op=ADDNODE-CPUTENSOR] | 0.012929 | 50.40939% <WfInst[op=SUBNODE-CPUTENSOR] | 0.004272 | 16.65627% <WfInst[op=DIVNODE-CPUTENSOR] | 0.004236 | 16.515907% <WfInst[op=SCALARDIV-CPUTENSOR] | 0.002096 | 8.172176% <WfInst[op=EXPNODE-CPUTENSOR] | 0.001026 | 4.000312% <WfInst[op=VIEWTENSORNODE-T] | 4.77e-4 | 1.8597941% <WfInst[op=MOVETENSORNODE-CPUTENSOR] | 3.32e-4 | 1.2944479% <WfInst[op=SCALARMUL-CPUTENSOR] | 2.8e-4 | 1.0917032% [struct] FusionPathQuery (make-query abstract-node &key (device t) (dtype t) (pred #'(lambda (node) t))) (make-query ...) and create a new query. A single FusionPathQuery becomes t only when satisfies all of following conditions: abstract-node[symbol] become t when the node is a subtype of abstract-node device[t or symbol] become t when the node is working under the device or subtype of it. dtype[t or list] become t when the dtype is set to t, or the list of dtype in arguments are corresponds with the list. (e.g.: (list :float :float) ) pred[function] specifies an additional predicator, the function receives (node) as arguments and return t to accept it. ( arguments-tensor is an list of tensors, which forward or call used.) This structure is excepted to be combined with defpath . [macro] defpath (defpath (fusion-name &rest query-list) &key (reject-p #'(lambda ())) (replaced-with nil)) \u26a0\ufe0f This API is still in the conceptial stage, tests are not enough. DO NOT USE THIS. The macro defpath introduces to cl-waffe2 Symbolic Differentiation . Users can define a FusionQueryPath to relocate compiled instructions with reference to the search. Composing the sequence of generated IRs to suit the device or model is the easiest way to speed up your model, cl-waffe2 searches for compiled nodes and replaces those matching the conditions specified in query-list with the computed nodes specified in replaced-with , if :fuse-p is set to t (default: t ). In the simplest case, defpath can detect [AddNode-CPUTensor] [MulNode-CPUTensor] sequence and replace it with [AddMulNode-CPUTensor] node to reduce the number of instructions. [When adding a new device to cl-waffe2...] 1. Declare the new device (e.g.: CPUTensor, CUDATensor ...) 2. Prepare allocator and accessors (e.g.: initialize-instance method, vref and (setf vref)) 3. Implement existing operations with define-impl macro 4. Blush up the generated IR with defpath macro to fuse more operations in a small cycle. <- defpath, here! The created and registered path, will be reset with the (reset-all-path!) function. All registered paths are stored in *user-defined-path-list* parameter. Rules cl-waffe2 replaces the existing operations with following the rules: The search is performed ignoring SaveForBackwardNode. If it is contained in the area to be replaced, it is moved to the last sequence of replaced one. ;; Example Rule: [A] [B] -> [C] Before Fusion: [A] [SAVE_FOR_BACKWARD] [B] [M] [N] Searching will be done ignoring [SAVE_FOR_BACKWARD] ^ [A] | [B] | [M] | [N] reading in this direction. After fusion: [C] ;; [A] [B] -> [C] [SAVE_FOR_BACKWARD] ;; placed after the operation [M] [N] defpath priority is given to those registered first. Repeat the search until no more targets are found to replace it. query-list Not replaced until the query-list matches everything, including the order. Example (TODO: For the case of ReLU) make-query","title":"cl-waffe2/vm"},{"location":"vm/#cl-waffe2-vm","text":"The package cl-waffe2/vm is the central of system, and features are focused on low-level stuffs: compiling/optimizing/rewriting cl-waffe2 IRs and how they're executed. So, most of APIs are accesible by convinient API wrappers of other packages. Global Variables optimization level logging IR and Compiler WfInstruction compiler acceptor Analyzing compiled codes disassemble profiling Adding Symbolic Diff and Device-Specific Optimization FusionPathQuery defpath","title":"cl-waffe2 VM"},{"location":"vm/#parameter-opt-level","text":"This parameter indicates the degree of runtime error detection. Whichever you choose, cl-waffe2 never apply something unsafe code transformation. It takes the fixnum from 1 to 3, and the larger, the faster. Set 1 to use safety-mode, in every instructions, runtime error is checked. Set 2 to use middle-mode, runtime error is checked only when first execution. Set 3 to use fastest-mode, no runtime error checking is done. Again, whichever levels you choose, the graph cl-waffe2 executes is the same. So the effects on the performance is very small (within < 1e-4~1e-5 sec). In default, set to 2.","title":"[parameter] *opt-level*"},{"location":"vm/#parameter-logging-vm-execution","text":"This parameter is useful for printing how all instructions are performed. If set to T, all results and arguments produced by executing cl-waffe2 IR is displayed into the terminal. In default, set to nil.","title":"[parameter] *logging-vm-execution*"},{"location":"vm/#struct-wfinstruction","text":"WfInstruction is IR for cl-waffe2 VM. Its form is represented by an extended Wengert list which is able to return multiple outputs. In this document, we call this cl-waffe2 IR , and compiled cl-waffe2 code, that is, the list of WfInstruction is called InstructionSeq or iseq . Unlike other frameworks, this IR is not only used to represent backpropagation but also forward propagation. In cl-waffe2, WfInstruction is created by compiling AbstractNode, and its operation can be obtained by compiling lisp code or passing lambda functions. A single WfInstruction represents: out_to[0], out_to[1], ... <- \u03bb(Args[0], Args[1], Args[2], ...) ^wfop-out-to ^wfop-op ^wfop-args where \u03bb represents the operation. And, if any, ArgsN is wrapped with SV4BW . out_to[0], out_to[1], ... <- \u03bb(SV4BW(Args[0]), Args[1], Args[2], ...) ^ wfop-sv4bw SV4BW (i.e: save-for-backward) is a temporary tensor to compute backwards and cl-waffe2 reads the :save-for-backward slots in the define-impl macro, and the corresponding tensors are copied.","title":"[struct] WfInstruction"},{"location":"vm/#slots","text":"wfop-op[function] corresponds with compiled \u03bb function. wfop-node[AbstractNode or string or function] The node which generates \u03bb function. For the most case, this slot is set to AbstractNode , but the node is something special, (e.g.: CodeBlock , IfNode etc...), set to function . wfop-out-to[list of AbstractTensor] indicates list of tensors that results are to be stored. wfop-self[AbstractTensor] corresponds with out_target , that is, the tensor to store the results wfop-args[list of AbstractTensor] corresponds with (tensor-variable wfop-self) . tensors to be called with: arg1 arg2 arg3... . wfop-sv4bw[list of AbstractTensor] indicates list of tensors storing save-for-backward tensors. if the corresponding position is save-for-backward=nil , the corresponding position also become nil.","title":"Slots"},{"location":"vm/#function-compile-forward-and-backward","text":"(compile-forward-and-backward toplevel &key (need-backward t) (fuse-p t) (compile-mode :default) (optimize-locality t)) Compiles into cl-waffe2 IR (so-called iseq) from the given toplevel to each leaf points (where detach-p=t or backward=null variables). toplevel is AbstractTensor with backwards. Tips: disassemble-waffe2-ir to display compiled Instruction Sequence.","title":"[function] compile-forward-and-backward"},{"location":"vm/#return","text":"(values forward-iseq backward-iseq leaves[an list of AbstractTensor that appeared in the node] dout alloc-state)","title":"Return"},{"location":"vm/#function-accept-instructions","text":"(accept-instructions iseq) Evaluates generated cl-waffe2 IR sequence. iseq[list] an list of WFInstruction","title":"[function] accept-instructions"},{"location":"vm/#function-disassemble-waffe2-ir","text":"(disassemble-waffe2-ir toplevel &key (backward t) (stream t) (fuse-p t)) Prints out the compiled cl-waffe2 IR from toplevel to each leaf points to stream . If backward was set to t, backward is also displayed.","title":"[function] disassemble-waffe2-ir"},{"location":"vm/#example","text":"(with-output-to-string (out) (disassemble-waffe2-ir (!softmax (parameter (randn `(3 3))) :avoid-overflow nil) :stream out)) disassemble-waffe2-ir: [Forward]: <WfInst[op=MOVETENSORNODE-CPUTENSOR] : TID323 <= op(TID323{float, (3 3)} <Param>TID318{float, (3 3)})> <WfInst[op=EXPNODE-CPUTENSOR] : TID323 <= op(<Param>SV4BW(TID318{float, (3 3)}) TID323{float, (3 3)})> <WfInst[op=SCALARMUL-CPUTENSOR] : TID356 <= op(TID356{float, (3 1)} <Input>TID347{float, (1)})> <WfInst[op=VIEWTENSORNODE-T] : TID356 <= op(TID356{float, (3 3)} TID356{float, (3 1)})> <WfInst[op=ADDNODE-CPUTENSOR] : TID356 <= op(TID356{float, (3 3)} TID323{float, (3 3)})> <WfInst[op=DIVNODE-CPUTENSOR] : TID323 <= op(SV4BW(TID323{float, (3 3)}) SV4BW(TID356{float, (3 3)}))> 6 Instructions | 3 Tensors | 1 Scalars [Pullback]: <WfInst[op=MOVETENSORNODE-CPUTENSOR] : TID418 <= op(TID418{float, (3 3)} <Input>TID415{float, (3 3)})> <WfInst[op=DIVNODE-CPUTENSOR] : TID418 <= op(TID418{float, (3 3)} TID395{float, (3 3)})> <WfInst[op=MOVETENSORNODE-CPUTENSOR] : TID448 <= op(TID448{float, (3 3)} <Input>TID415{float, (3 3)})> <WfInst[op=SCALARMUL-CPUTENSOR] : TID448 <= op(TID448{float, (3 3)} <Input>TID445{float, (1)})> <WfInst[op=MULNODE-CPUTENSOR] : TID390 <= op(TID390{float, (3 3)} TID448{float, (3 3)})> <WfInst[op=VIEWTENSORNODE-T] : TID395 <= op(TID395{float, (3 3)} TID395{float, (3 1)})> <WfInst[op=MULNODE-CPUTENSOR] : TID395 <= op(TID395{float, (3 3)} TID395{float, (3 3)})> <WfInst[op=DIVNODE-CPUTENSOR] : TID390 <= op(TID390{float, (3 3)} TID395{float, (3 3)})> <WfInst[op=SYSTEM-LAZY-CONS-T] : TID418 TID390 <= op(TID418{float, (3 3)} TID390{float, (3 3)})> <WfInst[op=EXPNODE-CPUTENSOR] : TID334 <= op(TID334{float, (3 3)} TID334{float, (3 3)})> <WfInst[op=MULNODE-CPUTENSOR] : TID418 <= op(TID418{float, (3 3)} TID334{float, (3 3)})> <WfInst[op=MOVETENSORNODE-CPUTENSOR] : <Input>TID320 <= op(<Input>TID320{float, (3 3)} TID418{float, (3 3)})> 12 Instructions | 7 Tensors | 1 Scalars","title":"Example"},{"location":"vm/#function-benchmark-accept-instructions","text":"(benchmark-accept-instructions iseq &key (n-sample 1) (ignore-first-call nil) (stream t) (top-k 10)) Basically, the function benchmark-accept-instruction executes the given list of instructions with profiling execution time, but at the end of proess, displays the report into stream .","title":"[function] benchmark-accept-instructions"},{"location":"vm/#inputs","text":"n-sample[fixnum] repeats the iseq execution for n-sample times ignore-first-call[boolean] If t, ignores the first call to avoid including allocating time. stream[stream] the place to display the result top-k[fixnum] top-k slowest nodes are displayed at the end of report.","title":"Inputs"},{"location":"vm/#return_1","text":"result[AbstractTensor] See also: proceed-bench","title":"Return"},{"location":"vm/#example_1","text":"(with-output-to-string (out) (proceed-bench (!softmax (randn `(100 100))) :n-sample 100 :stream out)) [Sorted by Instructions] Time(s) | Instruction ( * - Beyonds the average execution time) 3.32e-4 | <WfInst[op=MOVETENSORNODE-CPUTENSOR] : TID700 <= op(TID700{float, (100 100)} <Input>TID612{float, (100 100)})> 9.9e-5 | <WfInst[op=VIEWTENSORNODE-T] : TID694 <= op(TID652{float, (100 1)} TID694{float, (100 1)})> 1.53e-4 | <WfInst[op=SCALARMUL-CPUTENSOR] : TID694 <= op(TID694{float, (100 1)} <Input>TID621{float, (1)})> 9.8e-5 | <WfInst[op=VIEWTENSORNODE-T] : TID694 <= op(TID694{float, (100 100)} TID694{float, (100 1)})> 0.006497* | <WfInst[op=ADDNODE-CPUTENSOR] : TID694 <= op(TID694{float, (100 100)} <Input>TID612{float, (100 100)})> 9.6e-5 | <WfInst[op=VIEWTENSORNODE-T] : TID694 <= op(TID694{float, (100 1)} TID694{float, (100 100)})> 0.002096* | <WfInst[op=SCALARDIV-CPUTENSOR] : TID694 <= op(TID694{float, (100 1)} <Input>TID616{float, (1)})> 9.1e-5 | <WfInst[op=VIEWTENSORNODE-T] : TID694 <= op(TID694{float, (100 100)} TID694{float, (100 1)})> 0.004272* | <WfInst[op=SUBNODE-CPUTENSOR] : TID700 <= op(TID700{float, (100 100)} TID694{float, (100 100)})> 0.001026 | <WfInst[op=EXPNODE-CPUTENSOR] : TID700 <= op(TID700{float, (100 100)} TID700{float, (100 100)})> 1.27e-4 | <WfInst[op=SCALARMUL-CPUTENSOR] : TID694 <= op(TID694{float, (100 1)} <Input>TID749{float, (1)})> 9.3e-5 | <WfInst[op=VIEWTENSORNODE-T] : TID694 <= op(TID694{float, (100 100)} TID694{float, (100 1)})> 0.006432* | <WfInst[op=ADDNODE-CPUTENSOR] : TID694 <= op(TID694{float, (100 100)} TID700{float, (100 100)})> 0.004236* | <WfInst[op=DIVNODE-CPUTENSOR] : TID700 <= op(TID700{float, (100 100)} TID694{float, (100 100)})> 14 Instructions | 7 Tensors | Overheads due to SV4BW(...) -> 4.82e-6(s) Total Time: 0.025648 sec [Sorted by topK] Instruction | Total time (s) | Time/Total (n-sample=100) <WfInst[op=ADDNODE-CPUTENSOR] | 0.012929 | 50.40939% <WfInst[op=SUBNODE-CPUTENSOR] | 0.004272 | 16.65627% <WfInst[op=DIVNODE-CPUTENSOR] | 0.004236 | 16.515907% <WfInst[op=SCALARDIV-CPUTENSOR] | 0.002096 | 8.172176% <WfInst[op=EXPNODE-CPUTENSOR] | 0.001026 | 4.000312% <WfInst[op=VIEWTENSORNODE-T] | 4.77e-4 | 1.8597941% <WfInst[op=MOVETENSORNODE-CPUTENSOR] | 3.32e-4 | 1.2944479% <WfInst[op=SCALARMUL-CPUTENSOR] | 2.8e-4 | 1.0917032%","title":"Example"},{"location":"vm/#struct-fusionpathquery","text":"(make-query abstract-node &key (device t) (dtype t) (pred #'(lambda (node) t))) (make-query ...) and create a new query. A single FusionPathQuery becomes t only when satisfies all of following conditions: abstract-node[symbol] become t when the node is a subtype of abstract-node device[t or symbol] become t when the node is working under the device or subtype of it. dtype[t or list] become t when the dtype is set to t, or the list of dtype in arguments are corresponds with the list. (e.g.: (list :float :float) ) pred[function] specifies an additional predicator, the function receives (node) as arguments and return t to accept it. ( arguments-tensor is an list of tensors, which forward or call used.) This structure is excepted to be combined with defpath .","title":"[struct] FusionPathQuery"},{"location":"vm/#macro-defpath","text":"(defpath (fusion-name &rest query-list) &key (reject-p #'(lambda ())) (replaced-with nil)) \u26a0\ufe0f This API is still in the conceptial stage, tests are not enough. DO NOT USE THIS. The macro defpath introduces to cl-waffe2 Symbolic Differentiation . Users can define a FusionQueryPath to relocate compiled instructions with reference to the search. Composing the sequence of generated IRs to suit the device or model is the easiest way to speed up your model, cl-waffe2 searches for compiled nodes and replaces those matching the conditions specified in query-list with the computed nodes specified in replaced-with , if :fuse-p is set to t (default: t ). In the simplest case, defpath can detect [AddNode-CPUTensor] [MulNode-CPUTensor] sequence and replace it with [AddMulNode-CPUTensor] node to reduce the number of instructions. [When adding a new device to cl-waffe2...] 1. Declare the new device (e.g.: CPUTensor, CUDATensor ...) 2. Prepare allocator and accessors (e.g.: initialize-instance method, vref and (setf vref)) 3. Implement existing operations with define-impl macro 4. Blush up the generated IR with defpath macro to fuse more operations in a small cycle. <- defpath, here! The created and registered path, will be reset with the (reset-all-path!) function. All registered paths are stored in *user-defined-path-list* parameter.","title":"[macro] defpath"},{"location":"vm/#rules","text":"cl-waffe2 replaces the existing operations with following the rules: The search is performed ignoring SaveForBackwardNode. If it is contained in the area to be replaced, it is moved to the last sequence of replaced one. ;; Example Rule: [A] [B] -> [C] Before Fusion: [A] [SAVE_FOR_BACKWARD] [B] [M] [N] Searching will be done ignoring [SAVE_FOR_BACKWARD] ^ [A] | [B] | [M] | [N] reading in this direction. After fusion: [C] ;; [A] [B] -> [C] [SAVE_FOR_BACKWARD] ;; placed after the operation [M] [N] defpath priority is given to those registered first. Repeat the search until no more targets are found to replace it. query-list Not replaced until the query-list matches everything, including the order.","title":"Rules"},{"location":"vm/#example_2","text":"(TODO: For the case of ReLU)","title":"Example"},{"location":"vm/#make-query","text":"","title":"make-query"}]}