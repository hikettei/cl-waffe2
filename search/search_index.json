{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Programmable Deep Learning Framework Repository \u00bb Issues \u00b7 Benchmarks \u00b7 Tutorials \u26a0\ufe0f cl-waffe2 is still in the concept stage and has a limited feature. Also, things are subject to change. cl-waffe2 provides a set of differentiable matrix operations which is aimed to apply to build neural network models on Common Lisp. The primary concepts and goals of this project is following One cl-waffe2 code, consisted of multiple and user-extensible backends, which consisted of small pieces of backends. Everything is lazy-evaluated, and compiled. defined-and-run, closer to defined-by-run APIs. Everything in this documentation is incomplete! I guess should be much more polished!","title":"Home"},{"location":"Tips/","text":"Tips Backend/Kernel Abstraction Adding a new backend Adding a new operation Make a Input","title":"Tips"},{"location":"Tips/#tips","text":"","title":"Tips"},{"location":"Tips/#backendkernel-abstraction","text":"","title":"Backend/Kernel Abstraction"},{"location":"Tips/#adding-a-new-backend","text":"","title":"Adding a new backend"},{"location":"Tips/#adding-a-new-operation","text":"","title":"Adding a new operation"},{"location":"Tips/#make-a-input","text":"","title":"Make a Input"},{"location":"base-impl-nodes/","text":".katex img { object-fit: fill; padding: unset; display: block; position: absolute; width: 100%; height: inherit; } Standard Nodes [node] ADDNODE (A[~] B[~] -> A[~]) Description AddNode is a node which computes following operation element-wise. Let X and Y be a given arguments and both are matrix. X \u2190 X + Y X\\gets{X + Y} X \u2190 X + Y Constructor (AddNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx dy) (values (!move dx dout) (!move dy dout))) No need to implement backwards at define-impl . (they'd be ignored.) [node] SUBNODE (A[~] B[~] -> A[~]) Description SubNode is a node which computes following operation element-wise. Let X and Y be a given arguments and both are matrix. X \u2190 X \u2212 Y X\\gets{X - Y} X \u2190 X \u2212 Y Constructor (SubNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx dy) (values (!move dx dout) (!move dy (!mul -1 dout)))) No need to implement backwards at define-impl . (they'd be ignored.) [node] MULNODE (A[~] B[~] -> A[~]) Description MulNode is a node which computes following operation element-wise. Let X and Y be a given arguments and both are matrix. X \u2190 X \u2217 Y X\\gets{X * Y} X \u2190 X \u2217 Y Constructor (MulNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx dy) (values (!mul dout dy) (!mul dout dx))) No need to implement backwards at define-impl . (they'd be ignored.) [node] DIVNODE (A[~] B[~] -> A[~]) Description DivNode is a node which computes following operation element-wise. Let X and Y be a given arguments and both are matrix. X \u2190 X / Y X\\gets{X / Y} X \u2190 X / Y Constructor (DivNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx dy) (values (!div dout dy) (!div (!mul dx (!mul -1 dout)) (!square dy)))) No need to implement backwards at define-impl . (they'd be ignored.) [node] INVERSETENSORNODE (A[~] -> A[~]) Description InverseTensorNode is a node which computes following operation element-wise A \u2190 1 / A A\\gets{1 / A} A \u2190 1/ A Constructor (InverseTensorNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx) (values (!div (!mul -1 dout) (!square dx)))) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALARADD (A[SCAL] B[~] -> B[~] WHERE SCAL = 1) Description ScalarAdd is a node which computes following operation element-wise. Let X be a given matrix and S be a given scalar. X \u2190 X + s c a l a r X\\gets{X + scalar} X \u2190 X + sc a l a r Constructor (ScalarAdd dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dx dy)) (values (->scal (!mean dout)) dout)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALARSUB (A[SCAL] B[~] -> B[~] WHERE SCAL = 1) Description ScalarSub is a node which computes following operation element-wise. Let X be a given matrix and S be a given scalar. X \u2190 X \u2212 s c a l a r X\\gets{X - scalar} X \u2190 X \u2212 sc a l a r Constructor (ScalarSub dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dx dy)) (values (->scal (!mul -1.0 (!mean dout))) dout)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALARMUL (A[SCAL] B[~] -> B[~] WHERE SCAL = 1) Description ScalarMul is a node which computes following operation element-wise. Let X be a given matrix and S be a given scalar. X \u2190 X \u2217 s c a l a r X\\gets{X * scalar} X \u2190 X \u2217 sc a l a r Constructor (ScalarMul dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx dy) (values (->scal (!mean (!mul dy dout))) (!mul dout dx))) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALARDIV (A[SCAL] B[~] -> B[~] WHERE SCAL = 1) Description ScalarDiv is a node which computes following operation element-wise. Let X be a given matrix and S be a given scalar. X \u2190 X / s c a l a r X\\gets{X / scalar} X \u2190 X / sc a l a r Constructor (ScalarDiv dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 ) Backward \u2705 Already defined. ((self dout dx dy) (values (->scal (!mean (!div (!mul dy (!mul -1 dout)) (!square dx)))) (!div dout dx))) No need to implement backwards at define-impl . (they'd be ignored.) [node] MOVETENSORNODE (A[~] B[~] -> A[~]) Description Move all the visible elements of B into visible areas of A . A \u2190 B A\\gets{B} A \u2190 B Constraints In order to implement the behaviour for compilers of eliminating unused copies, all the implementations must satisfy as follows: On forward: If (movetensor-ignore-me self) is t, return B without doing anything. Otherwise, Move all the visible elements of B into A , and return A . (Note that: until (tensor-vec A) is called, A is never allocated.) Constructor (MoveTensorNode dtype) dtype dtype to use. Backward \u2705 Already defined. ((self dout dx dy) (let ((dy-out (if (and (eql (tensor-attribute dy) chain) (movetensor-ignore-me self)) dout (!copy dout)))) (values (if (eql (tensor-attribute dx) chain) (!move dx dout) dout) (if (eql (tensor-attribute dy) chain) (!move dy dy-out) dy-out)))) No need to implement backwards at define-impl . (they'd be ignored.) [node] ABSNODE (X[~] OUT[~] -> OUT[~]) Description The node ABSNODE takes X as an argument, applying a abs function into each element and writes the result into out. O U T \u2190 a b s ( X ) OUT\\gets{abs(X)} O U T \u2190 ab s ( X ) save-for-backward: (T NIL) See also: SCALAR-ABSNODE !abs Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dy)) (values (!mul dout (!sign dx)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-ABSNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-ABSNODE takes scalar X as an argument, applying a abs function into each element and writes the result into out. o u t \u2190 a b s ( x ) out\\gets{abs(x)} o u t \u2190 ab s ( x ) save-for-backward: (T NIL) See also: ABSNODE !abs Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dy)) (values (!mul dout (!sign dx)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SIGNNODE (X[~] OUT[~] -> OUT[~]) Description The node SIGNNODE takes X as an argument, applying a sign function into each element and writes the result into out. O U T \u2190 s i g n ( X ) OUT\\gets{sign(X)} O U T \u2190 s i g n ( X ) save-for-backward: (T NIL) See also: SCALAR-SIGNNODE !sign Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dout dy)) (values (!mul dx 0) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-SIGNNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-SIGNNODE takes scalar X as an argument, applying a sign function into each element and writes the result into out. o u t \u2190 s i g n ( x ) out\\gets{sign(x)} o u t \u2190 s i g n ( x ) save-for-backward: (T NIL) See also: SIGNNODE !sign Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dout dy)) (values (!mul dx 0) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SQRTNODE (X[~] OUT[~] -> OUT[~]) Description The node SQRTNODE takes X as an argument, applying a sqrt function into each element and writes the result into out. O U T \u2190 s q r t ( X ) OUT\\gets{sqrt(X)} O U T \u2190 s q r t ( X ) save-for-backward: (T NIL) See also: SCALAR-SQRTNODE !sqrt Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dy)) (values (!mul dout (!div 1 dx)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-SQRTNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-SQRTNODE takes scalar X as an argument, applying a sqrt function into each element and writes the result into out. o u t \u2190 s q r t ( x ) out\\gets{sqrt(x)} o u t \u2190 s q r t ( x ) save-for-backward: (T NIL) See also: SQRTNODE !sqrt Backward \u2705 Already defined. ((self dout dx dy) (declare (ignore dy)) (values (!mul dout (!div 1 dx)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SQUARENODE (X[~] OUT[~] -> OUT[~]) Description The node SQUARENODE takes X as an argument, applying a square function into each element and writes the result into out. O U T \u2190 s q u a r e ( X ) OUT\\gets{square(X)} O U T \u2190 s q u a re ( X ) save-for-backward: (T NIL) See also: SCALAR-SQUARENODE !square Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout x) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-SQUARENODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-SQUARENODE takes scalar X as an argument, applying a square function into each element and writes the result into out. o u t \u2190 s q u a r e ( x ) out\\gets{square(x)} o u t \u2190 s q u a re ( x ) save-for-backward: (T NIL) See also: SQUARENODE !square Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout x) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SINNODE (X[~] OUT[~] -> OUT[~]) Description The node SINNODE takes X as an argument, applying a sin function into each element and writes the result into out. O U T \u2190 s i n ( X ) OUT\\gets{sin(X)} O U T \u2190 s in ( X ) save-for-backward: (T NIL) See also: SCALAR-SINNODE !sin Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!cos x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-SINNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-SINNODE takes scalar X as an argument, applying a sin function into each element and writes the result into out. o u t \u2190 s i n ( x ) out\\gets{sin(x)} o u t \u2190 s in ( x ) save-for-backward: (T NIL) See also: SINNODE !sin Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!cos x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] COSNODE (X[~] OUT[~] -> OUT[~]) Description The node COSNODE takes X as an argument, applying a cos function into each element and writes the result into out. O U T \u2190 c o s ( X ) OUT\\gets{cos(X)} O U T \u2190 cos ( X ) save-for-backward: (T NIL) See also: SCALAR-COSNODE !cos Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!mul -1 (!sin x))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-COSNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-COSNODE takes scalar X as an argument, applying a cos function into each element and writes the result into out. o u t \u2190 c o s ( x ) out\\gets{cos(x)} o u t \u2190 cos ( x ) save-for-backward: (T NIL) See also: COSNODE !cos Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!mul -1 (!sin x))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] TANNODE (X[~] OUT[~] -> OUT[~]) Description The node TANNODE takes X as an argument, applying a tan function into each element and writes the result into out. O U T \u2190 t a n ( X ) OUT\\gets{tan(X)} O U T \u2190 t an ( X ) save-for-backward: (T NIL) See also: SCALAR-TANNODE !tan Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul (!cos x) (!cos x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-TANNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-TANNODE takes scalar X as an argument, applying a tan function into each element and writes the result into out. o u t \u2190 t a n ( x ) out\\gets{tan(x)} o u t \u2190 t an ( x ) save-for-backward: (T NIL) See also: TANNODE !tan Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul (!cos x) (!cos x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] ASINNODE (X[~] OUT[~] -> OUT[~]) Description The node ASINNODE takes X as an argument, applying a asin function into each element and writes the result into out. O U T \u2190 a s i n ( X ) OUT\\gets{asin(X)} O U T \u2190 a s in ( X ) save-for-backward: (T NIL) See also: SCALAR-ASINNODE !asin Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!sqrt (!sub 1 (!square x))))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-ASINNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-ASINNODE takes scalar X as an argument, applying a asin function into each element and writes the result into out. o u t \u2190 a s i n ( x ) out\\gets{asin(x)} o u t \u2190 a s in ( x ) save-for-backward: (T NIL) See also: ASINNODE !asin Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!sqrt (!sub 1 (!square x))))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] ACOSNODE (X[~] OUT[~] -> OUT[~]) Description The node ACOSNODE takes X as an argument, applying a acos function into each element and writes the result into out. O U T \u2190 a c o s ( X ) OUT\\gets{acos(X)} O U T \u2190 a cos ( X ) save-for-backward: (T NIL) See also: SCALAR-ACOSNODE !acos Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div -1 (!sqrt (!sub 1 (!square x))))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-ACOSNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-ACOSNODE takes scalar X as an argument, applying a acos function into each element and writes the result into out. o u t \u2190 a c o s ( x ) out\\gets{acos(x)} o u t \u2190 a cos ( x ) save-for-backward: (T NIL) See also: ACOSNODE !acos Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div -1 (!sqrt (!sub 1 (!square x))))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] ATANNODE (X[~] OUT[~] -> OUT[~]) Description The node ATANNODE takes X as an argument, applying a atan function into each element and writes the result into out. O U T \u2190 a t a n ( X ) OUT\\gets{atan(X)} O U T \u2190 a t an ( X ) save-for-backward: (T NIL) See also: SCALAR-ATANNODE !atan Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!add 1 (!square x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-ATANNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-ATANNODE takes scalar X as an argument, applying a atan function into each element and writes the result into out. o u t \u2190 a t a n ( x ) out\\gets{atan(x)} o u t \u2190 a t an ( x ) save-for-backward: (T NIL) See also: ATANNODE !atan Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!add 1 (!square x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SINHNODE (X[~] OUT[~] -> OUT[~]) Description The node SINHNODE takes X as an argument, applying a sinh function into each element and writes the result into out. O U T \u2190 s i n h ( X ) OUT\\gets{sinh(X)} O U T \u2190 s inh ( X ) save-for-backward: (T NIL) See also: SCALAR-SINHNODE !sinh Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!cosh x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-SINHNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-SINHNODE takes scalar X as an argument, applying a sinh function into each element and writes the result into out. o u t \u2190 s i n h ( x ) out\\gets{sinh(x)} o u t \u2190 s inh ( x ) save-for-backward: (T NIL) See also: SINHNODE !sinh Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!cosh x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] COSHNODE (X[~] OUT[~] -> OUT[~]) Description The node COSHNODE takes X as an argument, applying a cosh function into each element and writes the result into out. O U T \u2190 c o s h ( X ) OUT\\gets{cosh(X)} O U T \u2190 cos h ( X ) save-for-backward: (T NIL) See also: SCALAR-COSHNODE !cosh Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!mul -1 (!sinh x))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-COSHNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-COSHNODE takes scalar X as an argument, applying a cosh function into each element and writes the result into out. o u t \u2190 c o s h ( x ) out\\gets{cosh(x)} o u t \u2190 cos h ( x ) save-for-backward: (T NIL) See also: COSHNODE !cosh Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!mul -1 (!sinh x))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] TANHNODE (X[~] OUT[~] -> OUT[~]) Description The node TANHNODE takes X as an argument, applying a tanh function into each element and writes the result into out. O U T \u2190 t a n h ( X ) OUT\\gets{tanh(X)} O U T \u2190 t anh ( X ) save-for-backward: (T NIL) See also: SCALAR-TANHNODE !tanh Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul (!cosh x) (!cosh x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-TANHNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-TANHNODE takes scalar X as an argument, applying a tanh function into each element and writes the result into out. o u t \u2190 t a n h ( x ) out\\gets{tanh(x)} o u t \u2190 t anh ( x ) save-for-backward: (T NIL) See also: TANHNODE !tanh Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul (!cosh x) (!cosh x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] ASINHNODE (X[~] OUT[~] -> OUT[~]) Description The node ASINHNODE takes X as an argument, applying a asinh function into each element and writes the result into out. O U T \u2190 a s i n h ( X ) OUT\\gets{asinh(X)} O U T \u2190 a s inh ( X ) save-for-backward: NIL See also: SCALAR-ASINHNODE !asinh Backward \u274c Undefined. (To make it differentiable, must be defined with define-impl macro.) [node] SCALAR-ASINHNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-ASINHNODE takes scalar X as an argument, applying a asinh function into each element and writes the result into out. o u t \u2190 a s i n h ( x ) out\\gets{asinh(x)} o u t \u2190 a s inh ( x ) save-for-backward: NIL See also: ASINHNODE !asinh Backward \u274c Undefined. (To make it differentiable, must be defined with define-impl macro.) [node] ACOSHNODE (X[~] OUT[~] -> OUT[~]) Description The node ACOSHNODE takes X as an argument, applying a acosh function into each element and writes the result into out. O U T \u2190 a c o s h ( X ) OUT\\gets{acosh(X)} O U T \u2190 a cos h ( X ) save-for-backward: NIL See also: SCALAR-ACOSHNODE !acosh Backward \u274c Undefined. (To make it differentiable, must be defined with define-impl macro.) [node] SCALAR-ACOSHNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-ACOSHNODE takes scalar X as an argument, applying a acosh function into each element and writes the result into out. o u t \u2190 a c o s h ( x ) out\\gets{acosh(x)} o u t \u2190 a cos h ( x ) save-for-backward: NIL See also: ACOSHNODE !acosh Backward \u274c Undefined. (To make it differentiable, must be defined with define-impl macro.) [node] ATANHNODE (X[~] OUT[~] -> OUT[~]) Description The node ATANHNODE takes X as an argument, applying a atanh function into each element and writes the result into out. O U T \u2190 a t a n h ( X ) OUT\\gets{atanh(X)} O U T \u2190 a t anh ( X ) save-for-backward: NIL See also: SCALAR-ATANHNODE !atanh Backward \u274c Undefined. (To make it differentiable, must be defined with define-impl macro.) [node] SCALAR-ATANHNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-ATANHNODE takes scalar X as an argument, applying a atanh function into each element and writes the result into out. o u t \u2190 a t a n h ( x ) out\\gets{atanh(x)} o u t \u2190 a t anh ( x ) save-for-backward: NIL See also: ATANHNODE !atanh Backward \u274c Undefined. (To make it differentiable, must be defined with define-impl macro.) [node] EXPNODE (X[~] OUT[~] -> OUT[~]) Description The node EXPNODE takes X as an argument, applying a exp function into each element and writes the result into out. O U T \u2190 e x p ( X ) OUT\\gets{exp(X)} O U T \u2190 e x p ( X ) save-for-backward: (T NIL) See also: SCALAR-EXPNODE !exp Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!exp x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-EXPNODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-EXPNODE takes scalar X as an argument, applying a exp function into each element and writes the result into out. o u t \u2190 e x p ( x ) out\\gets{exp(x)} o u t \u2190 e x p ( x ) save-for-backward: (T NIL) See also: EXPNODE !exp Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!exp x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] LOG2NODE (X[~] OUT[~] -> OUT[~]) Description The node LOG2NODE takes X as an argument, applying a log2 function into each element and writes the result into out. O U T \u2190 l o g 2 ( X ) OUT\\gets{log2(X)} O U T \u2190 l o g 2 ( X ) save-for-backward: (T NIL) See also: SCALAR-LOG2NODE !log2 Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul x (log 2)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-LOG2NODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-LOG2NODE takes scalar X as an argument, applying a log2 function into each element and writes the result into out. o u t \u2190 l o g 2 ( x ) out\\gets{log2(x)} o u t \u2190 l o g 2 ( x ) save-for-backward: (T NIL) See also: LOG2NODE !log2 Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul x (log 2)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] LOG10NODE (X[~] OUT[~] -> OUT[~]) Description The node LOG10NODE takes X as an argument, applying a log10 function into each element and writes the result into out. O U T \u2190 l o g 10 ( X ) OUT\\gets{log10(X)} O U T \u2190 l o g 10 ( X ) save-for-backward: (T NIL) See also: SCALAR-LOG10NODE !log10 Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul x (log 10)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-LOG10NODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-LOG10NODE takes scalar X as an argument, applying a log10 function into each element and writes the result into out. o u t \u2190 l o g 10 ( x ) out\\gets{log10(x)} o u t \u2190 l o g 10 ( x ) save-for-backward: (T NIL) See also: LOG10NODE !log10 Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul x (log 10)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] LOGENODE (X[~] OUT[~] -> OUT[~]) Description The node LOGENODE takes X as an argument, applying a loge function into each element and writes the result into out. O U T \u2190 l o g e ( X ) OUT\\gets{loge(X)} O U T \u2190 l o g e ( X ) save-for-backward: (T NIL) See also: SCALAR-LOGENODE !loge Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] SCALAR-LOGENODE (X[~] OUT[~] -> OUT[~]) Description The node SCALAR-LOGENODE takes scalar X as an argument, applying a loge function into each element and writes the result into out. o u t \u2190 l o g e ( x ) out\\gets{loge(x)} o u t \u2190 l o g e ( x ) save-for-backward: (T NIL) See also: LOGENODE !loge Backward \u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] LAZYTRANSPOSENODE (A[~ I J] -> A[~ J I]) Description LazyTransposeNode is the matmul-dedicated node which supplies the lazy-transpose feature. Internally, This Node Returns The Given A itself but taking transpose of A's shape. If the computation node is like: [LazyTransposeNode] -> [MatmulNode], then transpose will be done with NO overhead. Backward \u2705 Already defined. ((self dout dx) (declare (ignore dx)) (values (!t dout))) No need to implement backwards at define-impl . (they'd be ignored.) [node] ARGMAX-NODE (A[~] OUT[OUT-SIZE] -> OUT[OUT-SIZE]) Description ArgMax-Node finds a maximum value of all elements in A. OUT is overwritten with the result. A is a target to find a maximum value, and OUT is a place to set the index. Constructor (ArgMax-Node out-size) out-size the reducted shape of out . Backward \u2705 Already defined. ((self dout da do) (declare (ignore dout da do)) (values nil nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] ARGMIN-NODE (A[~] OUT[OUT-SIZE] -> OUT[OUT-SIZE]) Description ArgMin-Node finds a minimum value of all elements in A. OUT is overwritten with the result. A is a target to find a minimum value, and OUT is a place to set the index. Constructor (ArgMin-Node out-size) out-size the reducted shape of out . Backward \u2705 Already defined. ((self dout da do) (declare (ignore dout da do)) (values nil nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] MATMULNODE (A[~ I J] B[~ J K] C[~ I K] -> C[~ I K]) Description MatmulNode Computes a matrix multiplication of given A and B, set the result to C. C \u2190 g e m m ( 1.0 , A , B , 0.0 , C ) C\\gets{gemm(1.0, A, B, 0.0, C)} C \u2190 g e mm ( 1.0 , A , B , 0.0 , C ) Constructor (MatMulNode dtype &key transpose-a transpose-b) dtype dtype to use. transpose-a transpose-b set t to call with transposing (reversing the last two axes the matrix). Backward \u2705 Already defined. ((self dout da db do) (declare (ignore do)) (values (!matmul dout (!t db)) (!matmul (!t da) dout) nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] WHERE-OPERATION-NODE (A[~] OUT[~] -> OUT[~]) Description Where-Operation-Node is a node which set true-then , if the result of calling condition with each element of A, is t and if it is NIL, set false-then at corresponding position. Constructor (Where-Operation-Node condition true-then false-then) true-then and false-then is a number. condition a single argument function, each element of A is argument. (e.g.: this could be #'evenp #'oddp etc...) Backward \u2705 Already defined. ((self dout da do) (declare (ignore dout da do)) ;; todo: :no-grad t (values nil nil)) No need to implement backwards at define-impl . (they'd be ignored.) [node] COMPARE-OPERATION-NODE (A[~] B[~] OUT[~] -> OUT[~]) Description Compare-Operation-Node is a node which set true-then , if the result of calling condition with each element of A and B, if it is NIl set false-then at corresponding position. Constructor (Compare-Operation-Node condition true-then false-then) true-then and false-then is a number. condition a two arguments function, each element of A and B is argument. (e.g.: this could be #'> or #'< etc...) Backward \u2705 Already defined. ((self dout da db do) (declare (ignore dout da db do)) ;; todo: :no-grad t (values nil nil nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"[Nodes] cl-waffe2/base-impl"},{"location":"base-impl-nodes/#standard-nodes","text":"","title":"Standard Nodes"},{"location":"base-impl-nodes/#node-addnode","text":"(A[~] B[~] -> A[~])","title":"[node] ADDNODE"},{"location":"base-impl-nodes/#description","text":"AddNode is a node which computes following operation element-wise. Let X and Y be a given arguments and both are matrix. X \u2190 X + Y X\\gets{X + Y} X \u2190 X + Y","title":"Description"},{"location":"base-impl-nodes/#constructor","text":"(AddNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward","text":"\u2705 Already defined. ((self dout dx dy) (values (!move dx dout) (!move dy dout))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-subnode","text":"(A[~] B[~] -> A[~])","title":"[node] SUBNODE"},{"location":"base-impl-nodes/#description_1","text":"SubNode is a node which computes following operation element-wise. Let X and Y be a given arguments and both are matrix. X \u2190 X \u2212 Y X\\gets{X - Y} X \u2190 X \u2212 Y","title":"Description"},{"location":"base-impl-nodes/#constructor_1","text":"(SubNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward_1","text":"\u2705 Already defined. ((self dout dx dy) (values (!move dx dout) (!move dy (!mul -1 dout)))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-mulnode","text":"(A[~] B[~] -> A[~])","title":"[node] MULNODE"},{"location":"base-impl-nodes/#description_2","text":"MulNode is a node which computes following operation element-wise. Let X and Y be a given arguments and both are matrix. X \u2190 X \u2217 Y X\\gets{X * Y} X \u2190 X \u2217 Y","title":"Description"},{"location":"base-impl-nodes/#constructor_2","text":"(MulNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward_2","text":"\u2705 Already defined. ((self dout dx dy) (values (!mul dout dy) (!mul dout dx))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-divnode","text":"(A[~] B[~] -> A[~])","title":"[node] DIVNODE"},{"location":"base-impl-nodes/#description_3","text":"DivNode is a node which computes following operation element-wise. Let X and Y be a given arguments and both are matrix. X \u2190 X / Y X\\gets{X / Y} X \u2190 X / Y","title":"Description"},{"location":"base-impl-nodes/#constructor_3","text":"(DivNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward_3","text":"\u2705 Already defined. ((self dout dx dy) (values (!div dout dy) (!div (!mul dx (!mul -1 dout)) (!square dy)))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-inversetensornode","text":"(A[~] -> A[~])","title":"[node] INVERSETENSORNODE"},{"location":"base-impl-nodes/#description_4","text":"InverseTensorNode is a node which computes following operation element-wise A \u2190 1 / A A\\gets{1 / A} A \u2190 1/ A","title":"Description"},{"location":"base-impl-nodes/#constructor_4","text":"(InverseTensorNode dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward_4","text":"\u2705 Already defined. ((self dout dx) (values (!div (!mul -1 dout) (!square dx)))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalaradd","text":"(A[SCAL] B[~] -> B[~] WHERE SCAL = 1)","title":"[node] SCALARADD"},{"location":"base-impl-nodes/#description_5","text":"ScalarAdd is a node which computes following operation element-wise. Let X be a given matrix and S be a given scalar. X \u2190 X + s c a l a r X\\gets{X + scalar} X \u2190 X + sc a l a r","title":"Description"},{"location":"base-impl-nodes/#constructor_5","text":"(ScalarAdd dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward_5","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dx dy)) (values (->scal (!mean dout)) dout)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalarsub","text":"(A[SCAL] B[~] -> B[~] WHERE SCAL = 1)","title":"[node] SCALARSUB"},{"location":"base-impl-nodes/#description_6","text":"ScalarSub is a node which computes following operation element-wise. Let X be a given matrix and S be a given scalar. X \u2190 X \u2212 s c a l a r X\\gets{X - scalar} X \u2190 X \u2212 sc a l a r","title":"Description"},{"location":"base-impl-nodes/#constructor_6","text":"(ScalarSub dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward_6","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dx dy)) (values (->scal (!mul -1.0 (!mean dout))) dout)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalarmul","text":"(A[SCAL] B[~] -> B[~] WHERE SCAL = 1)","title":"[node] SCALARMUL"},{"location":"base-impl-nodes/#description_7","text":"ScalarMul is a node which computes following operation element-wise. Let X be a given matrix and S be a given scalar. X \u2190 X \u2217 s c a l a r X\\gets{X * scalar} X \u2190 X \u2217 sc a l a r","title":"Description"},{"location":"base-impl-nodes/#constructor_7","text":"(ScalarMul dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward_7","text":"\u2705 Already defined. ((self dout dx dy) (values (->scal (!mean (!mul dy dout))) (!mul dout dx))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalardiv","text":"(A[SCAL] B[~] -> B[~] WHERE SCAL = 1)","title":"[node] SCALARDIV"},{"location":"base-impl-nodes/#description_8","text":"ScalarDiv is a node which computes following operation element-wise. Let X be a given matrix and S be a given scalar. X \u2190 X / s c a l a r X\\gets{X / scalar} X \u2190 X / sc a l a r","title":"Description"},{"location":"base-impl-nodes/#constructor_8","text":"(ScalarDiv dtype) dtype dtype to use, being used to dispatch backends. (e.g.: :float :uint8 )","title":"Constructor"},{"location":"base-impl-nodes/#backward_8","text":"\u2705 Already defined. ((self dout dx dy) (values (->scal (!mean (!div (!mul dy (!mul -1 dout)) (!square dx)))) (!div dout dx))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-movetensornode","text":"(A[~] B[~] -> A[~])","title":"[node] MOVETENSORNODE"},{"location":"base-impl-nodes/#description_9","text":"Move all the visible elements of B into visible areas of A . A \u2190 B A\\gets{B} A \u2190 B","title":"Description"},{"location":"base-impl-nodes/#constraints","text":"In order to implement the behaviour for compilers of eliminating unused copies, all the implementations must satisfy as follows: On forward: If (movetensor-ignore-me self) is t, return B without doing anything. Otherwise, Move all the visible elements of B into A , and return A . (Note that: until (tensor-vec A) is called, A is never allocated.)","title":"Constraints"},{"location":"base-impl-nodes/#constructor_9","text":"(MoveTensorNode dtype) dtype dtype to use.","title":"Constructor"},{"location":"base-impl-nodes/#backward_9","text":"\u2705 Already defined. ((self dout dx dy) (let ((dy-out (if (and (eql (tensor-attribute dy) chain) (movetensor-ignore-me self)) dout (!copy dout)))) (values (if (eql (tensor-attribute dx) chain) (!move dx dout) dout) (if (eql (tensor-attribute dy) chain) (!move dy dy-out) dy-out)))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-absnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] ABSNODE"},{"location":"base-impl-nodes/#description_10","text":"The node ABSNODE takes X as an argument, applying a abs function into each element and writes the result into out. O U T \u2190 a b s ( X ) OUT\\gets{abs(X)} O U T \u2190 ab s ( X ) save-for-backward: (T NIL) See also: SCALAR-ABSNODE !abs","title":"Description"},{"location":"base-impl-nodes/#backward_10","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dy)) (values (!mul dout (!sign dx)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-absnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-ABSNODE"},{"location":"base-impl-nodes/#description_11","text":"The node SCALAR-ABSNODE takes scalar X as an argument, applying a abs function into each element and writes the result into out. o u t \u2190 a b s ( x ) out\\gets{abs(x)} o u t \u2190 ab s ( x ) save-for-backward: (T NIL) See also: ABSNODE !abs","title":"Description"},{"location":"base-impl-nodes/#backward_11","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dy)) (values (!mul dout (!sign dx)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-signnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SIGNNODE"},{"location":"base-impl-nodes/#description_12","text":"The node SIGNNODE takes X as an argument, applying a sign function into each element and writes the result into out. O U T \u2190 s i g n ( X ) OUT\\gets{sign(X)} O U T \u2190 s i g n ( X ) save-for-backward: (T NIL) See also: SCALAR-SIGNNODE !sign","title":"Description"},{"location":"base-impl-nodes/#backward_12","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dout dy)) (values (!mul dx 0) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-signnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-SIGNNODE"},{"location":"base-impl-nodes/#description_13","text":"The node SCALAR-SIGNNODE takes scalar X as an argument, applying a sign function into each element and writes the result into out. o u t \u2190 s i g n ( x ) out\\gets{sign(x)} o u t \u2190 s i g n ( x ) save-for-backward: (T NIL) See also: SIGNNODE !sign","title":"Description"},{"location":"base-impl-nodes/#backward_13","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dout dy)) (values (!mul dx 0) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-sqrtnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SQRTNODE"},{"location":"base-impl-nodes/#description_14","text":"The node SQRTNODE takes X as an argument, applying a sqrt function into each element and writes the result into out. O U T \u2190 s q r t ( X ) OUT\\gets{sqrt(X)} O U T \u2190 s q r t ( X ) save-for-backward: (T NIL) See also: SCALAR-SQRTNODE !sqrt","title":"Description"},{"location":"base-impl-nodes/#backward_14","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dy)) (values (!mul dout (!div 1 dx)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-sqrtnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-SQRTNODE"},{"location":"base-impl-nodes/#description_15","text":"The node SCALAR-SQRTNODE takes scalar X as an argument, applying a sqrt function into each element and writes the result into out. o u t \u2190 s q r t ( x ) out\\gets{sqrt(x)} o u t \u2190 s q r t ( x ) save-for-backward: (T NIL) See also: SQRTNODE !sqrt","title":"Description"},{"location":"base-impl-nodes/#backward_15","text":"\u2705 Already defined. ((self dout dx dy) (declare (ignore dy)) (values (!mul dout (!div 1 dx)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-squarenode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SQUARENODE"},{"location":"base-impl-nodes/#description_16","text":"The node SQUARENODE takes X as an argument, applying a square function into each element and writes the result into out. O U T \u2190 s q u a r e ( X ) OUT\\gets{square(X)} O U T \u2190 s q u a re ( X ) save-for-backward: (T NIL) See also: SCALAR-SQUARENODE !square","title":"Description"},{"location":"base-impl-nodes/#backward_16","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout x) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-squarenode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-SQUARENODE"},{"location":"base-impl-nodes/#description_17","text":"The node SCALAR-SQUARENODE takes scalar X as an argument, applying a square function into each element and writes the result into out. o u t \u2190 s q u a r e ( x ) out\\gets{square(x)} o u t \u2190 s q u a re ( x ) save-for-backward: (T NIL) See also: SQUARENODE !square","title":"Description"},{"location":"base-impl-nodes/#backward_17","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout x) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-sinnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SINNODE"},{"location":"base-impl-nodes/#description_18","text":"The node SINNODE takes X as an argument, applying a sin function into each element and writes the result into out. O U T \u2190 s i n ( X ) OUT\\gets{sin(X)} O U T \u2190 s in ( X ) save-for-backward: (T NIL) See also: SCALAR-SINNODE !sin","title":"Description"},{"location":"base-impl-nodes/#backward_18","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!cos x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-sinnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-SINNODE"},{"location":"base-impl-nodes/#description_19","text":"The node SCALAR-SINNODE takes scalar X as an argument, applying a sin function into each element and writes the result into out. o u t \u2190 s i n ( x ) out\\gets{sin(x)} o u t \u2190 s in ( x ) save-for-backward: (T NIL) See also: SINNODE !sin","title":"Description"},{"location":"base-impl-nodes/#backward_19","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!cos x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-cosnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] COSNODE"},{"location":"base-impl-nodes/#description_20","text":"The node COSNODE takes X as an argument, applying a cos function into each element and writes the result into out. O U T \u2190 c o s ( X ) OUT\\gets{cos(X)} O U T \u2190 cos ( X ) save-for-backward: (T NIL) See also: SCALAR-COSNODE !cos","title":"Description"},{"location":"base-impl-nodes/#backward_20","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!mul -1 (!sin x))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-cosnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-COSNODE"},{"location":"base-impl-nodes/#description_21","text":"The node SCALAR-COSNODE takes scalar X as an argument, applying a cos function into each element and writes the result into out. o u t \u2190 c o s ( x ) out\\gets{cos(x)} o u t \u2190 cos ( x ) save-for-backward: (T NIL) See also: COSNODE !cos","title":"Description"},{"location":"base-impl-nodes/#backward_21","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!mul -1 (!sin x))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-tannode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] TANNODE"},{"location":"base-impl-nodes/#description_22","text":"The node TANNODE takes X as an argument, applying a tan function into each element and writes the result into out. O U T \u2190 t a n ( X ) OUT\\gets{tan(X)} O U T \u2190 t an ( X ) save-for-backward: (T NIL) See also: SCALAR-TANNODE !tan","title":"Description"},{"location":"base-impl-nodes/#backward_22","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul (!cos x) (!cos x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-tannode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-TANNODE"},{"location":"base-impl-nodes/#description_23","text":"The node SCALAR-TANNODE takes scalar X as an argument, applying a tan function into each element and writes the result into out. o u t \u2190 t a n ( x ) out\\gets{tan(x)} o u t \u2190 t an ( x ) save-for-backward: (T NIL) See also: TANNODE !tan","title":"Description"},{"location":"base-impl-nodes/#backward_23","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul (!cos x) (!cos x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-asinnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] ASINNODE"},{"location":"base-impl-nodes/#description_24","text":"The node ASINNODE takes X as an argument, applying a asin function into each element and writes the result into out. O U T \u2190 a s i n ( X ) OUT\\gets{asin(X)} O U T \u2190 a s in ( X ) save-for-backward: (T NIL) See also: SCALAR-ASINNODE !asin","title":"Description"},{"location":"base-impl-nodes/#backward_24","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!sqrt (!sub 1 (!square x))))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-asinnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-ASINNODE"},{"location":"base-impl-nodes/#description_25","text":"The node SCALAR-ASINNODE takes scalar X as an argument, applying a asin function into each element and writes the result into out. o u t \u2190 a s i n ( x ) out\\gets{asin(x)} o u t \u2190 a s in ( x ) save-for-backward: (T NIL) See also: ASINNODE !asin","title":"Description"},{"location":"base-impl-nodes/#backward_25","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!sqrt (!sub 1 (!square x))))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-acosnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] ACOSNODE"},{"location":"base-impl-nodes/#description_26","text":"The node ACOSNODE takes X as an argument, applying a acos function into each element and writes the result into out. O U T \u2190 a c o s ( X ) OUT\\gets{acos(X)} O U T \u2190 a cos ( X ) save-for-backward: (T NIL) See also: SCALAR-ACOSNODE !acos","title":"Description"},{"location":"base-impl-nodes/#backward_26","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div -1 (!sqrt (!sub 1 (!square x))))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-acosnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-ACOSNODE"},{"location":"base-impl-nodes/#description_27","text":"The node SCALAR-ACOSNODE takes scalar X as an argument, applying a acos function into each element and writes the result into out. o u t \u2190 a c o s ( x ) out\\gets{acos(x)} o u t \u2190 a cos ( x ) save-for-backward: (T NIL) See also: ACOSNODE !acos","title":"Description"},{"location":"base-impl-nodes/#backward_27","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div -1 (!sqrt (!sub 1 (!square x))))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-atannode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] ATANNODE"},{"location":"base-impl-nodes/#description_28","text":"The node ATANNODE takes X as an argument, applying a atan function into each element and writes the result into out. O U T \u2190 a t a n ( X ) OUT\\gets{atan(X)} O U T \u2190 a t an ( X ) save-for-backward: (T NIL) See also: SCALAR-ATANNODE !atan","title":"Description"},{"location":"base-impl-nodes/#backward_28","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!add 1 (!square x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-atannode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-ATANNODE"},{"location":"base-impl-nodes/#description_29","text":"The node SCALAR-ATANNODE takes scalar X as an argument, applying a atan function into each element and writes the result into out. o u t \u2190 a t a n ( x ) out\\gets{atan(x)} o u t \u2190 a t an ( x ) save-for-backward: (T NIL) See also: ATANNODE !atan","title":"Description"},{"location":"base-impl-nodes/#backward_29","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!add 1 (!square x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-sinhnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SINHNODE"},{"location":"base-impl-nodes/#description_30","text":"The node SINHNODE takes X as an argument, applying a sinh function into each element and writes the result into out. O U T \u2190 s i n h ( X ) OUT\\gets{sinh(X)} O U T \u2190 s inh ( X ) save-for-backward: (T NIL) See also: SCALAR-SINHNODE !sinh","title":"Description"},{"location":"base-impl-nodes/#backward_30","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!cosh x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-sinhnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-SINHNODE"},{"location":"base-impl-nodes/#description_31","text":"The node SCALAR-SINHNODE takes scalar X as an argument, applying a sinh function into each element and writes the result into out. o u t \u2190 s i n h ( x ) out\\gets{sinh(x)} o u t \u2190 s inh ( x ) save-for-backward: (T NIL) See also: SINHNODE !sinh","title":"Description"},{"location":"base-impl-nodes/#backward_31","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!cosh x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-coshnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] COSHNODE"},{"location":"base-impl-nodes/#description_32","text":"The node COSHNODE takes X as an argument, applying a cosh function into each element and writes the result into out. O U T \u2190 c o s h ( X ) OUT\\gets{cosh(X)} O U T \u2190 cos h ( X ) save-for-backward: (T NIL) See also: SCALAR-COSHNODE !cosh","title":"Description"},{"location":"base-impl-nodes/#backward_32","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!mul -1 (!sinh x))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-coshnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-COSHNODE"},{"location":"base-impl-nodes/#description_33","text":"The node SCALAR-COSHNODE takes scalar X as an argument, applying a cosh function into each element and writes the result into out. o u t \u2190 c o s h ( x ) out\\gets{cosh(x)} o u t \u2190 cos h ( x ) save-for-backward: (T NIL) See also: COSHNODE !cosh","title":"Description"},{"location":"base-impl-nodes/#backward_33","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!mul -1 (!sinh x))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-tanhnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] TANHNODE"},{"location":"base-impl-nodes/#description_34","text":"The node TANHNODE takes X as an argument, applying a tanh function into each element and writes the result into out. O U T \u2190 t a n h ( X ) OUT\\gets{tanh(X)} O U T \u2190 t anh ( X ) save-for-backward: (T NIL) See also: SCALAR-TANHNODE !tanh","title":"Description"},{"location":"base-impl-nodes/#backward_34","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul (!cosh x) (!cosh x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-tanhnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-TANHNODE"},{"location":"base-impl-nodes/#description_35","text":"The node SCALAR-TANHNODE takes scalar X as an argument, applying a tanh function into each element and writes the result into out. o u t \u2190 t a n h ( x ) out\\gets{tanh(x)} o u t \u2190 t anh ( x ) save-for-backward: (T NIL) See also: TANHNODE !tanh","title":"Description"},{"location":"base-impl-nodes/#backward_35","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul (!cosh x) (!cosh x)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-asinhnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] ASINHNODE"},{"location":"base-impl-nodes/#description_36","text":"The node ASINHNODE takes X as an argument, applying a asinh function into each element and writes the result into out. O U T \u2190 a s i n h ( X ) OUT\\gets{asinh(X)} O U T \u2190 a s inh ( X ) save-for-backward: NIL See also: SCALAR-ASINHNODE !asinh","title":"Description"},{"location":"base-impl-nodes/#backward_36","text":"\u274c Undefined. (To make it differentiable, must be defined with define-impl macro.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-asinhnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-ASINHNODE"},{"location":"base-impl-nodes/#description_37","text":"The node SCALAR-ASINHNODE takes scalar X as an argument, applying a asinh function into each element and writes the result into out. o u t \u2190 a s i n h ( x ) out\\gets{asinh(x)} o u t \u2190 a s inh ( x ) save-for-backward: NIL See also: ASINHNODE !asinh","title":"Description"},{"location":"base-impl-nodes/#backward_37","text":"\u274c Undefined. (To make it differentiable, must be defined with define-impl macro.)","title":"Backward"},{"location":"base-impl-nodes/#node-acoshnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] ACOSHNODE"},{"location":"base-impl-nodes/#description_38","text":"The node ACOSHNODE takes X as an argument, applying a acosh function into each element and writes the result into out. O U T \u2190 a c o s h ( X ) OUT\\gets{acosh(X)} O U T \u2190 a cos h ( X ) save-for-backward: NIL See also: SCALAR-ACOSHNODE !acosh","title":"Description"},{"location":"base-impl-nodes/#backward_38","text":"\u274c Undefined. (To make it differentiable, must be defined with define-impl macro.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-acoshnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-ACOSHNODE"},{"location":"base-impl-nodes/#description_39","text":"The node SCALAR-ACOSHNODE takes scalar X as an argument, applying a acosh function into each element and writes the result into out. o u t \u2190 a c o s h ( x ) out\\gets{acosh(x)} o u t \u2190 a cos h ( x ) save-for-backward: NIL See also: ACOSHNODE !acosh","title":"Description"},{"location":"base-impl-nodes/#backward_39","text":"\u274c Undefined. (To make it differentiable, must be defined with define-impl macro.)","title":"Backward"},{"location":"base-impl-nodes/#node-atanhnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] ATANHNODE"},{"location":"base-impl-nodes/#description_40","text":"The node ATANHNODE takes X as an argument, applying a atanh function into each element and writes the result into out. O U T \u2190 a t a n h ( X ) OUT\\gets{atanh(X)} O U T \u2190 a t anh ( X ) save-for-backward: NIL See also: SCALAR-ATANHNODE !atanh","title":"Description"},{"location":"base-impl-nodes/#backward_40","text":"\u274c Undefined. (To make it differentiable, must be defined with define-impl macro.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-atanhnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-ATANHNODE"},{"location":"base-impl-nodes/#description_41","text":"The node SCALAR-ATANHNODE takes scalar X as an argument, applying a atanh function into each element and writes the result into out. o u t \u2190 a t a n h ( x ) out\\gets{atanh(x)} o u t \u2190 a t anh ( x ) save-for-backward: NIL See also: ATANHNODE !atanh","title":"Description"},{"location":"base-impl-nodes/#backward_41","text":"\u274c Undefined. (To make it differentiable, must be defined with define-impl macro.)","title":"Backward"},{"location":"base-impl-nodes/#node-expnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] EXPNODE"},{"location":"base-impl-nodes/#description_42","text":"The node EXPNODE takes X as an argument, applying a exp function into each element and writes the result into out. O U T \u2190 e x p ( X ) OUT\\gets{exp(X)} O U T \u2190 e x p ( X ) save-for-backward: (T NIL) See also: SCALAR-EXPNODE !exp","title":"Description"},{"location":"base-impl-nodes/#backward_42","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!exp x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-expnode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-EXPNODE"},{"location":"base-impl-nodes/#description_43","text":"The node SCALAR-EXPNODE takes scalar X as an argument, applying a exp function into each element and writes the result into out. o u t \u2190 e x p ( x ) out\\gets{exp(x)} o u t \u2190 e x p ( x ) save-for-backward: (T NIL) See also: EXPNODE !exp","title":"Description"},{"location":"base-impl-nodes/#backward_43","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!exp x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-log2node","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] LOG2NODE"},{"location":"base-impl-nodes/#description_44","text":"The node LOG2NODE takes X as an argument, applying a log2 function into each element and writes the result into out. O U T \u2190 l o g 2 ( X ) OUT\\gets{log2(X)} O U T \u2190 l o g 2 ( X ) save-for-backward: (T NIL) See also: SCALAR-LOG2NODE !log2","title":"Description"},{"location":"base-impl-nodes/#backward_44","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul x (log 2)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-log2node","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-LOG2NODE"},{"location":"base-impl-nodes/#description_45","text":"The node SCALAR-LOG2NODE takes scalar X as an argument, applying a log2 function into each element and writes the result into out. o u t \u2190 l o g 2 ( x ) out\\gets{log2(x)} o u t \u2190 l o g 2 ( x ) save-for-backward: (T NIL) See also: LOG2NODE !log2","title":"Description"},{"location":"base-impl-nodes/#backward_45","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul x (log 2)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-log10node","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] LOG10NODE"},{"location":"base-impl-nodes/#description_46","text":"The node LOG10NODE takes X as an argument, applying a log10 function into each element and writes the result into out. O U T \u2190 l o g 10 ( X ) OUT\\gets{log10(X)} O U T \u2190 l o g 10 ( X ) save-for-backward: (T NIL) See also: SCALAR-LOG10NODE !log10","title":"Description"},{"location":"base-impl-nodes/#backward_46","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul x (log 10)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-log10node","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-LOG10NODE"},{"location":"base-impl-nodes/#description_47","text":"The node SCALAR-LOG10NODE takes scalar X as an argument, applying a log10 function into each element and writes the result into out. o u t \u2190 l o g 10 ( x ) out\\gets{log10(x)} o u t \u2190 l o g 10 ( x ) save-for-backward: (T NIL) See also: LOG10NODE !log10","title":"Description"},{"location":"base-impl-nodes/#backward_47","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 (!mul x (log 10)))) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-logenode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] LOGENODE"},{"location":"base-impl-nodes/#description_48","text":"The node LOGENODE takes X as an argument, applying a loge function into each element and writes the result into out. O U T \u2190 l o g e ( X ) OUT\\gets{loge(X)} O U T \u2190 l o g e ( X ) save-for-backward: (T NIL) See also: SCALAR-LOGENODE !loge","title":"Description"},{"location":"base-impl-nodes/#backward_48","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-scalar-logenode","text":"(X[~] OUT[~] -> OUT[~])","title":"[node] SCALAR-LOGENODE"},{"location":"base-impl-nodes/#description_49","text":"The node SCALAR-LOGENODE takes scalar X as an argument, applying a loge function into each element and writes the result into out. o u t \u2190 l o g e ( x ) out\\gets{loge(x)} o u t \u2190 l o g e ( x ) save-for-backward: (T NIL) See also: LOGENODE !loge","title":"Description"},{"location":"base-impl-nodes/#backward_49","text":"\u2705 Already defined. ((self dout x out) (declare (ignore out)) (values (!mul dout (!div 1 x)) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-lazytransposenode","text":"(A[~ I J] -> A[~ J I])","title":"[node] LAZYTRANSPOSENODE"},{"location":"base-impl-nodes/#description_50","text":"LazyTransposeNode is the matmul-dedicated node which supplies the lazy-transpose feature. Internally, This Node Returns The Given A itself but taking transpose of A's shape. If the computation node is like: [LazyTransposeNode] -> [MatmulNode], then transpose will be done with NO overhead.","title":"Description"},{"location":"base-impl-nodes/#backward_50","text":"\u2705 Already defined. ((self dout dx) (declare (ignore dx)) (values (!t dout))) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-argmax-node","text":"(A[~] OUT[OUT-SIZE] -> OUT[OUT-SIZE])","title":"[node] ARGMAX-NODE"},{"location":"base-impl-nodes/#description_51","text":"ArgMax-Node finds a maximum value of all elements in A. OUT is overwritten with the result. A is a target to find a maximum value, and OUT is a place to set the index.","title":"Description"},{"location":"base-impl-nodes/#constructor_10","text":"(ArgMax-Node out-size) out-size the reducted shape of out .","title":"Constructor"},{"location":"base-impl-nodes/#backward_51","text":"\u2705 Already defined. ((self dout da do) (declare (ignore dout da do)) (values nil nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-argmin-node","text":"(A[~] OUT[OUT-SIZE] -> OUT[OUT-SIZE])","title":"[node] ARGMIN-NODE"},{"location":"base-impl-nodes/#description_52","text":"ArgMin-Node finds a minimum value of all elements in A. OUT is overwritten with the result. A is a target to find a minimum value, and OUT is a place to set the index.","title":"Description"},{"location":"base-impl-nodes/#constructor_11","text":"(ArgMin-Node out-size) out-size the reducted shape of out .","title":"Constructor"},{"location":"base-impl-nodes/#backward_52","text":"\u2705 Already defined. ((self dout da do) (declare (ignore dout da do)) (values nil nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-matmulnode","text":"(A[~ I J] B[~ J K] C[~ I K] -> C[~ I K])","title":"[node] MATMULNODE"},{"location":"base-impl-nodes/#description_53","text":"MatmulNode Computes a matrix multiplication of given A and B, set the result to C. C \u2190 g e m m ( 1.0 , A , B , 0.0 , C ) C\\gets{gemm(1.0, A, B, 0.0, C)} C \u2190 g e mm ( 1.0 , A , B , 0.0 , C )","title":"Description"},{"location":"base-impl-nodes/#constructor_12","text":"(MatMulNode dtype &key transpose-a transpose-b) dtype dtype to use. transpose-a transpose-b set t to call with transposing (reversing the last two axes the matrix).","title":"Constructor"},{"location":"base-impl-nodes/#backward_53","text":"\u2705 Already defined. ((self dout da db do) (declare (ignore do)) (values (!matmul dout (!t db)) (!matmul (!t da) dout) nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-where-operation-node","text":"(A[~] OUT[~] -> OUT[~])","title":"[node] WHERE-OPERATION-NODE"},{"location":"base-impl-nodes/#description_54","text":"Where-Operation-Node is a node which set true-then , if the result of calling condition with each element of A, is t and if it is NIL, set false-then at corresponding position.","title":"Description"},{"location":"base-impl-nodes/#constructor_13","text":"(Where-Operation-Node condition true-then false-then) true-then and false-then is a number. condition a single argument function, each element of A is argument. (e.g.: this could be #'evenp #'oddp etc...)","title":"Constructor"},{"location":"base-impl-nodes/#backward_54","text":"\u2705 Already defined. ((self dout da do) (declare (ignore dout da do)) ;; todo: :no-grad t (values nil nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl-nodes/#node-compare-operation-node","text":"(A[~] B[~] OUT[~] -> OUT[~])","title":"[node] COMPARE-OPERATION-NODE"},{"location":"base-impl-nodes/#description_55","text":"Compare-Operation-Node is a node which set true-then , if the result of calling condition with each element of A and B, if it is NIl set false-then at corresponding position.","title":"Description"},{"location":"base-impl-nodes/#constructor_14","text":"(Compare-Operation-Node condition true-then false-then) true-then and false-then is a number. condition a two arguments function, each element of A and B is argument. (e.g.: this could be #'> or #'< etc...)","title":"Constructor"},{"location":"base-impl-nodes/#backward_55","text":"\u2705 Already defined. ((self dout da db do) (declare (ignore dout da db do)) ;; todo: :no-grad t (values nil nil nil)) No need to implement backwards at define-impl . (they'd be ignored.)","title":"Backward"},{"location":"base-impl/","text":".katex img { object-fit: fill; padding: unset; display: block; position: absolute; width: 100%; height: inherit; } Basic APIs [function] !matrix-add (!matrix-add x y) The function !matrix-add calls ADDNODE and adds X and Y element-wise, returning a new tensor. X c o p y \u2190 X + Y X_{copy}\\gets{X + Y} X co p y \u200b \u2190 X + Y Inputs X and Y must be a AbstractTensor (not a ScalarTensor), with the same shape. SideEffects None. [function] !matrix-sub (!matrix-sub x y) The function !matrix-sub calls SUBNODE and substracts X by Y element-wise, returning a new tensor. X c o p y \u2190 X \u2212 Y X_{copy}\\gets{X - Y} X co p y \u200b \u2190 X \u2212 Y Inputs X and Y must be a AbstractTensor (not a ScalarTensor), with the same shape. SideEffects None. [function] !matrix-mul (!matrix-mul x y) The function !matrix-mul calls MULNODE and multiplies X and Y element-wise, returning a new tensor. X c o p y \u2190 X \u2217 Y X_{copy}\\gets{X * Y} X co p y \u200b \u2190 X \u2217 Y Inputs X and Y must be a AbstractTensor (not a ScalarTensor), with the same shape. SideEffects None. [function] !matrix-div (!matrix-div x y) The function !matrix-div calls DIVNODE and divides X by Y element-wise, returning a new tensor. X c o p y \u2190 X / Y X_{copy}\\gets{X / Y} X co p y \u200b \u2190 X / Y Inputs X and Y must be a AbstractTensor (not a ScalarTensor), with the same shape. SideEffects None. [function] !inverse (!inverse tensor) The function !inverse calls InverseTensorNode , and finds the inverse of the received Tensor/Scalar, returning a new tensor. X c o p y \u2190 1 / X X_{copy}\\gets{1 / X} X co p y \u200b \u2190 1/ X Inputs tensor[ScalarTensor/AbstractTensor/Number] [function] !scalar-add (!scalar-add scalar x) The function !SCALAR-ADD computes following operation with calling SCALARADD , returning a new tensor. X c o p y \u2190 X + s c a l a r X_{copy}\\gets{X + scalar} X co p y \u200b \u2190 X + sc a l a r Inputs scalar could be one of ScalarTensor or number tensor AbstractTensor (should not be a scalar) [function] !scalar-sub (!scalar-sub scalar x) The function !SCALAR-SUB computes following operation with calling SCALARSUB , returning a new tensor. X c o p y \u2190 X \u2212 s c a l a r X_{copy}\\gets{X - scalar} X co p y \u200b \u2190 X \u2212 sc a l a r Inputs scalar could be one of ScalarTensor or number tensor AbstractTensor (should not be a scalar) [function] !scalar-mul (!scalar-mul scalar x) The function !SCALAR-MUL computes following operation with calling SCALARMUL , returning a new tensor. X c o p y \u2190 X \u2217 s c a l a r X_{copy}\\gets{X * scalar} X co p y \u200b \u2190 X \u2217 sc a l a r Inputs scalar could be one of ScalarTensor or number tensor AbstractTensor (should not be a scalar) [function] !scalar-div (!scalar-div scalar x) The function !SCALAR-DIV computes following operation with calling SCALARDIV , returning a new tensor. X c o p y \u2190 X / s c a l a r X_{copy}\\gets{X / scalar} X co p y \u200b \u2190 X / sc a l a r Inputs scalar could be one of ScalarTensor or number tensor AbstractTensor (should not be a scalar) [function] !sas-add The function !sas-add provides differentiable scalar-and-scalar operation. Calling a node SCALARANDSCALARADD , the function performs following operation: x c o p y \u2190 x + y x_{copy}\\gets{x + y} x co p y \u200b \u2190 x + y Inputs x y could be one of: ScalarTensor or number [function] !sas-sub The function !sas-sub provides differentiable scalar-and-scalar operation. Calling a node SCALARANDSCALARSUB , the function performs following operation: x c o p y \u2190 x \u2212 y x_{copy}\\gets{x - y} x co p y \u200b \u2190 x \u2212 y Inputs x y could be one of: ScalarTensor or number [function] !sas-mul The function !sas-mul provides differentiable scalar-and-scalar operation. Calling a node SCALARANDSCALARMUL , the function performs following operation: x c o p y \u2190 x \u2217 y x_{copy}\\gets{x * y} x co p y \u200b \u2190 x \u2217 y Inputs x y could be one of: ScalarTensor or number [function] !sas-div The function !sas-div provides differentiable scalar-and-scalar operation. Calling a node SCALARANDSCALARDIV , the function performs following operation: x c o p y \u2190 x / y x_{copy}\\gets{x / y} x co p y \u200b \u2190 x / y Inputs x y could be one of: ScalarTensor or number [function] !add (!add x y) The function provides general-purpose arithmetic operation. Given type of tensors, this function dispatches these functions automatically: !sas-add !scalar-add !matrix-add Inputs x y could be one of AbstractTensor number ScalarTensor SideEffects None [function] !sub (!sub x y) The function provides general-purpose arithmetic operation. Given type of tensors, this function dispatches these functions automatically: !sas-sub !scalar-sub !matrix-sub Inputs x y could be one of AbstractTensor number ScalarTensor SideEffects None [function] !mul (!mul x y) The function provides general-purpose arithmetic operation. Given type of tensors, this function dispatches these functions automatically: !sas-mul !scalar-mul !matrix-mul Inputs x y could be one of AbstractTensor number ScalarTensor SideEffects None [function] !div (!div x y) The function provides general-purpose arithmetic operation. Given type of tensors, this function dispatches these functions automatically: !sas-div !scalar-div !matrix-div Inputs x y could be one of AbstractTensor number ScalarTensor SideEffects None [function] !move (!move place tensor) A \u2190 B A\\gets{B} A \u2190 B The function !move returns a node which moves tensor's visible elements into place's visible elements. nodes one of: MoveTensorNode ScalarTensorNode Inputs place[AbstractTensor] tensor to be overwritten. tensor[AbstractTensor] tensor to be referred. Output Unevaluated Copied Tensor. [function] !copy (!copy tensor) The function !copy returns a node which makes a copy the tensor's visible area. Note that: the function !copy never creates a new tensor larger than (tensor-vec tensor) has, (i.e.: copying broadcasted tensor will return broadcasted and copied tensor). !copy is used to make a cache before calling destructive operation to avoid side effects, therefore if the copy is included to be useless by compiler, this operations is being ignored without changing its behaviour. And this is why !copy returns InputTensor , not AbstractTensor . See also: !copy-force never being ignored by compiler, and broadcasted axes will be padded. Input: Tensor[AbstractTensor] Output: Tensor[AbstractTensor] [function] !copy-force (!copy-force (tensor)) The function !copy-force returns a node which copies the given tensor forcibly while the function !copy sometimes ignored. This function is also used to adjust memory alignment of tensor. [function] !reshape (!reshape tensor &rest shapes) Changes the shape of given tensor. Before and after the operation, the total elements of tensors must correspond. Inputs tensor AbstractTensor but must not includes symbol in the shape. shapes could be one of: fixnum t . t can be used at one, but the value of t is automatically inferenced. [function] !view (!view tensor &rest subscripts) The function !view returns a tensor which is applied lazy-evaluated view. For Example, let A be a 4x8 Matrix, and we gonna create a view of A that portrays A[:, 2] . (!view A 2 t) A B 0 ++++++++ -------- 1 ++++++++ -------- 2 ++++++++ -> [make a view] -> ++++++++ 3 ++++++++ -------- Here, A and B shares the pointer. Calling (shape B) returns (1 8) . Subscripts Subscripts are following: t all elements in the axis. fixnum points out the specified index. (start end) slices the area. (start end step-by) slices the area by step-by . step-by can be a negative-fixnum. (Not tested) (:broadcast N-times) broadcasts the axis for N-times, the axis to be broadcasted must be 1 or broadcasted-axis. (:tflist ...) (TODO) (:indices ...) (TODO) Return (values sliced-tensor broadcast-reverser) Tips: Applying !view again to the returned sliced-tensor with broadcast-reverser will remove broadcasts from the tensor. [function] !flatten (!flatten tensor) equivalent to the (!reshape tensor t) [function] !rankup (!rankup tensor ntimes) The function !rankup appends/reduces 1 into the given tensor's shape for ntimes. If ntimes > 0, appends 1 If ntimes < 0, reduces 1, if the axis=1, otherwise returns error. [function] ->scal (->scal matrix-tensor) The function ->scal receives matrix-tensor with total-size = 1, returning a ScalarTensor. [function] ->mat (->mat scalar-tensor &key (dims 1)) The function ->mat receives ScalarTensor , returning a matrix with the number of axis=dims. [function] proceed (proceed tensor &key (measure-time nil)) The function proceed invokes special node, ProceedNode , which takes all the previous computation node before tensor, returning the result of it. The backward is created with the previous node. This function will be useful especially when debugging on REPL. Inputs If measure-time =t, ProceedNode wraps with time macro when calling COMPILED forward and backward propagation. Compiling time isn't included to the displayed time while (time (proceed tensor)) includes. [function] proceed-time (proceed-time tensor) An alias for (proceed tensor :measure-time t) [function] proceed-backward (proceed-backward tensor) The function proceed-backward calls forward and backwrd of the tensor. Output T (which indicates backward is succeed) [function] !flexible (!flexible tensor) The function !flexible returns a node which adds 1 (which is broadcastable) to the head of the shape of tensor. That is: Tensor = (10 10) -> [!flexible] -> Tensor' = (1 ... 1 10 10) ^ <1 x N> Note that added axes could be broadcasted automatically when the operation called with multiple arguments. [function] !abs (!abs x &key (-> nil)) The function !abs takes x as an argument, applying a abs function into each element and writes the result into -> . O U T c o p y \u2190 a b s ( X ) OUT_{copy}\\gets{abs(X)} O U T co p y \u200b \u2190 ab s ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-ABSNODE ABSNODE SideEffects -> is destructed. [function] !sign (!sign x &key (-> nil)) The function !sign takes x as an argument, applying a sign function into each element and writes the result into -> . O U T c o p y \u2190 s i g n ( X ) OUT_{copy}\\gets{sign(X)} O U T co p y \u200b \u2190 s i g n ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-SIGNNODE SIGNNODE SideEffects -> is destructed. [function] !sqrt (!sqrt x &key (-> nil)) The function !sqrt takes x as an argument, applying a sqrt function into each element and writes the result into -> . O U T c o p y \u2190 s q r t ( X ) OUT_{copy}\\gets{sqrt(X)} O U T co p y \u200b \u2190 s q r t ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-SQRTNODE SQRTNODE SideEffects -> is destructed. [function] !square (!square x &key (-> nil)) The function !square takes x as an argument, applying a square function into each element and writes the result into -> . O U T c o p y \u2190 s q u a r e ( X ) OUT_{copy}\\gets{square(X)} O U T co p y \u200b \u2190 s q u a re ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-SQUARENODE SQUARENODE SideEffects -> is destructed. [function] !sin (!sin x &key (-> nil)) The function !sin takes x as an argument, applying a sin function into each element and writes the result into -> . O U T c o p y \u2190 s i n ( X ) OUT_{copy}\\gets{sin(X)} O U T co p y \u200b \u2190 s in ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-SINNODE SINNODE SideEffects -> is destructed. [function] !cos (!cos x &key (-> nil)) The function !cos takes x as an argument, applying a cos function into each element and writes the result into -> . O U T c o p y \u2190 c o s ( X ) OUT_{copy}\\gets{cos(X)} O U T co p y \u200b \u2190 cos ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-COSNODE COSNODE SideEffects -> is destructed. [function] !tan (!tan x &key (-> nil)) The function !tan takes x as an argument, applying a tan function into each element and writes the result into -> . O U T c o p y \u2190 t a n ( X ) OUT_{copy}\\gets{tan(X)} O U T co p y \u200b \u2190 t an ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-TANNODE TANNODE SideEffects -> is destructed. [function] !asin (!asin x &key (-> nil)) The function !asin takes x as an argument, applying a asin function into each element and writes the result into -> . O U T c o p y \u2190 a s i n ( X ) OUT_{copy}\\gets{asin(X)} O U T co p y \u200b \u2190 a s in ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-ASINNODE ASINNODE SideEffects -> is destructed. [function] !acos (!acos x &key (-> nil)) The function !acos takes x as an argument, applying a acos function into each element and writes the result into -> . O U T c o p y \u2190 a c o s ( X ) OUT_{copy}\\gets{acos(X)} O U T co p y \u200b \u2190 a cos ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-ACOSNODE ACOSNODE SideEffects -> is destructed. [function] !atan (!atan x &key (-> nil)) The function !atan takes x as an argument, applying a atan function into each element and writes the result into -> . O U T c o p y \u2190 a t a n ( X ) OUT_{copy}\\gets{atan(X)} O U T co p y \u200b \u2190 a t an ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-ATANNODE ATANNODE SideEffects -> is destructed. [function] !sinh (!sinh x &key (-> nil)) The function !sinh takes x as an argument, applying a sinh function into each element and writes the result into -> . O U T c o p y \u2190 s i n h ( X ) OUT_{copy}\\gets{sinh(X)} O U T co p y \u200b \u2190 s inh ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-SINHNODE SINHNODE SideEffects -> is destructed. [function] !cosh (!cosh x &key (-> nil)) The function !cosh takes x as an argument, applying a cosh function into each element and writes the result into -> . O U T c o p y \u2190 c o s h ( X ) OUT_{copy}\\gets{cosh(X)} O U T co p y \u200b \u2190 cos h ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-COSHNODE COSHNODE SideEffects -> is destructed. [function] !tanh (!tanh x &key (-> nil)) The function !tanh takes x as an argument, applying a tanh function into each element and writes the result into -> . O U T c o p y \u2190 t a n h ( X ) OUT_{copy}\\gets{tanh(X)} O U T co p y \u200b \u2190 t anh ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-TANHNODE TANHNODE SideEffects -> is destructed. [function] !asinh (!asinh x &key (-> nil)) The function !asinh takes x as an argument, applying a asinh function into each element and writes the result into -> . O U T c o p y \u2190 a s i n h ( X ) OUT_{copy}\\gets{asinh(X)} O U T co p y \u200b \u2190 a s inh ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-ASINHNODE ASINHNODE SideEffects -> is destructed. [function] !acosh (!acosh x &key (-> nil)) The function !acosh takes x as an argument, applying a acosh function into each element and writes the result into -> . O U T c o p y \u2190 a c o s h ( X ) OUT_{copy}\\gets{acosh(X)} O U T co p y \u200b \u2190 a cos h ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-ACOSHNODE ACOSHNODE SideEffects -> is destructed. [function] !atanh (!atanh x &key (-> nil)) The function !atanh takes x as an argument, applying a atanh function into each element and writes the result into -> . O U T c o p y \u2190 a t a n h ( X ) OUT_{copy}\\gets{atanh(X)} O U T co p y \u200b \u2190 a t anh ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-ATANHNODE ATANHNODE SideEffects -> is destructed. [function] !exp (!exp x &key (-> nil)) The function !exp takes x as an argument, applying a exp function into each element and writes the result into -> . O U T c o p y \u2190 e x p ( X ) OUT_{copy}\\gets{exp(X)} O U T co p y \u200b \u2190 e x p ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-EXPNODE EXPNODE SideEffects -> is destructed. [function] !log2 (!log2 x &key (-> nil)) The function !log2 takes x as an argument, applying a log2 function into each element and writes the result into -> . O U T c o p y \u2190 l o g 2 ( X ) OUT_{copy}\\gets{log2(X)} O U T co p y \u200b \u2190 l o g 2 ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-LOG2NODE LOG2NODE SideEffects -> is destructed. [function] !log10 (!log10 x &key (-> nil)) The function !log10 takes x as an argument, applying a log10 function into each element and writes the result into -> . O U T c o p y \u2190 l o g 10 ( X ) OUT_{copy}\\gets{log10(X)} O U T co p y \u200b \u2190 l o g 10 ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-LOG10NODE LOG10NODE SideEffects -> is destructed. [function] !loge (!loge x &key (-> nil)) The function !loge takes x as an argument, applying a loge function into each element and writes the result into -> . O U T c o p y \u2190 l o g e ( X ) OUT_{copy}\\gets{loge(X)} O U T co p y \u200b \u2190 l o g e ( X ) (where OUT = -> ) Inputs x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated. Returns -> Nodes SCALAR-LOGENODE LOGENODE SideEffects -> is destructed. [function] !sum (!sum tensor &key (axis t) (-> nil) (keep-repeat nil)) The function !sum return a node which computes the sum of tensor along the given axis. Inputs tensor , a tensor to be reducted. axis [t or fixnum or list] the axis to be reducted. (-1, -2... is ok) -> [AbstractTensor or nil] the place to set the result. If nil, creates a new tensor. keep-repeat [boolean] If t, the axis reducted is repeated. Return: -> [AbstractTensor] the result. [function] !mean (!mean tensor &key (axis t) (-> nil) (keep-repeat nil)) The function !mean return a node which computes the average of tensor along the given axis. Inputs tensor , a tensor to be reducted. axis [t or fixnum or list] the axis to be reducted. (-1, -2... is ok) -> [AbstractTensor or nil] the place to set the result. If nil, creates a new tensor. keep-repeat [boolean] If t, the axis reducted is repeated. Return -> [AbstractTensor] the result. [function] !argmax (!argmax tensor &key (axis -1) (out nil)) The function !argmax computes the indices of maximum values of all elements below the axis dimension in the given tensor. Inputs tensor axis out Returns AbstractTensor[uint32] with dimensions behind axis is replaced with 1.## [function] !argmin (!argmin tensor &key (axis -1) (out nil)) The function !argmin computes the indices of minimum values of all elements below the axis dimension in the given tensor. Inputs tensor axis out Returns AbstractTensor[uint32] with dimensions behind axis is replaced with 1. [function] !t (!t tensor) Applies Lazy-Transpose to the given tensor. The function is matmul-dedicated, so cooperationg with other operations (e.g.: !add) will cause the wrong result. (Internally, it is the equivalent to calling !reshape ) Current Problem Inconsistency of operations: !flexible(!t(x)).is_transposed? = NIL !t(!flexible(x)).is_flexible? = T [function] !matmul (!matmul x y &key (out nil) (transpose-x nil) (transpose-y nil)) Computing a matrix multiplication of X and Y, the function set the result into out. o u t \u2190 g e m m ( 1.0 , x , y , 0.0 , o u t ) out\\gets{gemm(1.0, x, y, 0.0, out)} o u t \u2190 g e mm ( 1.0 , x , y , 0.0 , o u t ) Inputs transpose-x transpose-y If t, the tensor is called with (!t tensor) Lazy-Transpose Call the function (!t tensor) in advance to transpose the tensor without overheads. (!matmul (!t (randn `(5 3))) (randn `(5 3))) [function] !dot (!dot x y) Finds a dot product of x and y. Unlike numpy.dot , !dot intentionally only supports computing the dot product of two 1D tensors with the same number of elements. (proceed (!dot (randn `(100)) (randn `(10 10)))) {CPUTENSOR[float] :shape (1) -> :view (<0>) -> :visible-shape (1) :named ChainTMP115880 :vec-state [computed] (21.594929) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} [function] !where (!where tensor condition &key (true-then 1) (false-then 0) (out nil)) The function !where returns a elements selected-from true-then or false-then , depending on condition. The operation is defined as: o u t i = { true-then c o n d i t i o n ( X i ) false-then otherwise \\begin{equation} out_i= \\begin{cases} \\text{true-then} & condition(X_i) \\\\ \\text{false-then} & \\text{otherwise} \\end{cases} \\end{equation} o u t i \u200b = { true-then false-then \u200b co n d i t i o n ( X i \u200b ) otherwise \u200b \u200b \u200b (where X = tensor) Inputs out place to set the result condition an funcallable function. (e.g.: #'evenp #'oddp etc...) [function] !where (!compare tensor1 tensor2 condition &key (true-then 1) (false-then 0) (out nil)) The function !compare returns a elements selected-from true-then or false-then , depending on condition. The operation is defined as: o u t i = { true-then c o n d i t i o n ( X i , Y i ) false-then otherwise \\begin{equation} out_i= \\begin{cases} \\text{true-then} & condition(X_i, Y_i) \\\\ \\text{false-then} & \\text{otherwise} \\end{cases} \\end{equation} o u t i \u200b = { true-then false-then \u200b co n d i t i o n ( X i \u200b , Y i \u200b ) otherwise \u200b \u200b \u200b (where X = tensor1, Y=tensor2) Inputs out place to set the result condition an funcallable function. (e.g.: #'> #'< etc...) [function] a>scal (a>scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a>scal sets true-then if the equation: element > scal is t, otherwise set false-then at the corresponding positions. Inputs A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as scal) [function] a<scal (a<scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a<scal sets true-then if the equation: element < scal is t, otherwise set false-then at the corresponding positions. Inputs A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as scal) [function] a>=scal (a>=scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a>=scal sets true-then if the equation: element >= scal is t, otherwise set false-then at the corresponding positions. Inputs A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as scal) [function] a<=scal (a<=scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a<=scal sets true-then if the equation: element <= scal is t, otherwise set false-then at the corresponding positions. Inputs A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as scal) [function] a>b (a>b A B &key (out nil) (true-then 1) (false-then 0)) The function a>b sets true-then if the equation: A > B is t, otherwise set false-then at the corresponding positions. Inputs A B AbstractTensor to be compared. [function] a<b (a<b A B &key (out nil) (true-then 1) (false-then 0)) The function a<b sets true-then if the equation: A < B is t, otherwise set false-then at the corresponding positions. Inputs A B AbstractTensor to be compared. [function] a>=b (a>=b A B &key (out nil) (true-then 1) (false-then 0)) The function a>=b sets true-then if the equation: A >= B is t, otherwise set false-then at the corresponding positions. Inputs A B AbstractTensor to be compared. [function] a<=b (a<=b A B &key (out nil) (true-then 1) (false-then 0)) The function a<=b sets true-then if the equation: A <= B is t, otherwise set false-then at the corresponding positions. Inputs A B AbstractTensor to be compared.","title":"[Functions] cl-waffe2/base-impl"},{"location":"base-impl/#basic-apis","text":"","title":"Basic APIs"},{"location":"base-impl/#function-matrix-add","text":"(!matrix-add x y) The function !matrix-add calls ADDNODE and adds X and Y element-wise, returning a new tensor. X c o p y \u2190 X + Y X_{copy}\\gets{X + Y} X co p y \u200b \u2190 X + Y","title":"[function] !matrix-add"},{"location":"base-impl/#inputs","text":"X and Y must be a AbstractTensor (not a ScalarTensor), with the same shape.","title":"Inputs"},{"location":"base-impl/#sideeffects","text":"None.","title":"SideEffects"},{"location":"base-impl/#function-matrix-sub","text":"(!matrix-sub x y) The function !matrix-sub calls SUBNODE and substracts X by Y element-wise, returning a new tensor. X c o p y \u2190 X \u2212 Y X_{copy}\\gets{X - Y} X co p y \u200b \u2190 X \u2212 Y","title":"[function] !matrix-sub"},{"location":"base-impl/#inputs_1","text":"X and Y must be a AbstractTensor (not a ScalarTensor), with the same shape.","title":"Inputs"},{"location":"base-impl/#sideeffects_1","text":"None.","title":"SideEffects"},{"location":"base-impl/#function-matrix-mul","text":"(!matrix-mul x y) The function !matrix-mul calls MULNODE and multiplies X and Y element-wise, returning a new tensor. X c o p y \u2190 X \u2217 Y X_{copy}\\gets{X * Y} X co p y \u200b \u2190 X \u2217 Y","title":"[function] !matrix-mul"},{"location":"base-impl/#inputs_2","text":"X and Y must be a AbstractTensor (not a ScalarTensor), with the same shape.","title":"Inputs"},{"location":"base-impl/#sideeffects_2","text":"None.","title":"SideEffects"},{"location":"base-impl/#function-matrix-div","text":"(!matrix-div x y) The function !matrix-div calls DIVNODE and divides X by Y element-wise, returning a new tensor. X c o p y \u2190 X / Y X_{copy}\\gets{X / Y} X co p y \u200b \u2190 X / Y","title":"[function] !matrix-div"},{"location":"base-impl/#inputs_3","text":"X and Y must be a AbstractTensor (not a ScalarTensor), with the same shape.","title":"Inputs"},{"location":"base-impl/#sideeffects_3","text":"None.","title":"SideEffects"},{"location":"base-impl/#function-inverse","text":"(!inverse tensor) The function !inverse calls InverseTensorNode , and finds the inverse of the received Tensor/Scalar, returning a new tensor. X c o p y \u2190 1 / X X_{copy}\\gets{1 / X} X co p y \u200b \u2190 1/ X","title":"[function] !inverse"},{"location":"base-impl/#inputs_4","text":"tensor[ScalarTensor/AbstractTensor/Number]","title":"Inputs"},{"location":"base-impl/#function-scalar-add","text":"(!scalar-add scalar x) The function !SCALAR-ADD computes following operation with calling SCALARADD , returning a new tensor. X c o p y \u2190 X + s c a l a r X_{copy}\\gets{X + scalar} X co p y \u200b \u2190 X + sc a l a r","title":"[function] !scalar-add"},{"location":"base-impl/#inputs_5","text":"scalar could be one of ScalarTensor or number tensor AbstractTensor (should not be a scalar)","title":"Inputs"},{"location":"base-impl/#function-scalar-sub","text":"(!scalar-sub scalar x) The function !SCALAR-SUB computes following operation with calling SCALARSUB , returning a new tensor. X c o p y \u2190 X \u2212 s c a l a r X_{copy}\\gets{X - scalar} X co p y \u200b \u2190 X \u2212 sc a l a r","title":"[function] !scalar-sub"},{"location":"base-impl/#inputs_6","text":"scalar could be one of ScalarTensor or number tensor AbstractTensor (should not be a scalar)","title":"Inputs"},{"location":"base-impl/#function-scalar-mul","text":"(!scalar-mul scalar x) The function !SCALAR-MUL computes following operation with calling SCALARMUL , returning a new tensor. X c o p y \u2190 X \u2217 s c a l a r X_{copy}\\gets{X * scalar} X co p y \u200b \u2190 X \u2217 sc a l a r","title":"[function] !scalar-mul"},{"location":"base-impl/#inputs_7","text":"scalar could be one of ScalarTensor or number tensor AbstractTensor (should not be a scalar)","title":"Inputs"},{"location":"base-impl/#function-scalar-div","text":"(!scalar-div scalar x) The function !SCALAR-DIV computes following operation with calling SCALARDIV , returning a new tensor. X c o p y \u2190 X / s c a l a r X_{copy}\\gets{X / scalar} X co p y \u200b \u2190 X / sc a l a r","title":"[function] !scalar-div"},{"location":"base-impl/#inputs_8","text":"scalar could be one of ScalarTensor or number tensor AbstractTensor (should not be a scalar)","title":"Inputs"},{"location":"base-impl/#function-sas-add","text":"The function !sas-add provides differentiable scalar-and-scalar operation. Calling a node SCALARANDSCALARADD , the function performs following operation: x c o p y \u2190 x + y x_{copy}\\gets{x + y} x co p y \u200b \u2190 x + y","title":"[function] !sas-add"},{"location":"base-impl/#inputs_9","text":"x y could be one of: ScalarTensor or number","title":"Inputs"},{"location":"base-impl/#function-sas-sub","text":"The function !sas-sub provides differentiable scalar-and-scalar operation. Calling a node SCALARANDSCALARSUB , the function performs following operation: x c o p y \u2190 x \u2212 y x_{copy}\\gets{x - y} x co p y \u200b \u2190 x \u2212 y","title":"[function] !sas-sub"},{"location":"base-impl/#inputs_10","text":"x y could be one of: ScalarTensor or number","title":"Inputs"},{"location":"base-impl/#function-sas-mul","text":"The function !sas-mul provides differentiable scalar-and-scalar operation. Calling a node SCALARANDSCALARMUL , the function performs following operation: x c o p y \u2190 x \u2217 y x_{copy}\\gets{x * y} x co p y \u200b \u2190 x \u2217 y","title":"[function] !sas-mul"},{"location":"base-impl/#inputs_11","text":"x y could be one of: ScalarTensor or number","title":"Inputs"},{"location":"base-impl/#function-sas-div","text":"The function !sas-div provides differentiable scalar-and-scalar operation. Calling a node SCALARANDSCALARDIV , the function performs following operation: x c o p y \u2190 x / y x_{copy}\\gets{x / y} x co p y \u200b \u2190 x / y","title":"[function] !sas-div"},{"location":"base-impl/#inputs_12","text":"x y could be one of: ScalarTensor or number","title":"Inputs"},{"location":"base-impl/#function-add","text":"(!add x y) The function provides general-purpose arithmetic operation. Given type of tensors, this function dispatches these functions automatically: !sas-add !scalar-add !matrix-add","title":"[function] !add"},{"location":"base-impl/#inputs_13","text":"x y could be one of AbstractTensor number ScalarTensor","title":"Inputs"},{"location":"base-impl/#sideeffects_4","text":"None","title":"SideEffects"},{"location":"base-impl/#function-sub","text":"(!sub x y) The function provides general-purpose arithmetic operation. Given type of tensors, this function dispatches these functions automatically: !sas-sub !scalar-sub !matrix-sub","title":"[function] !sub"},{"location":"base-impl/#inputs_14","text":"x y could be one of AbstractTensor number ScalarTensor","title":"Inputs"},{"location":"base-impl/#sideeffects_5","text":"None","title":"SideEffects"},{"location":"base-impl/#function-mul","text":"(!mul x y) The function provides general-purpose arithmetic operation. Given type of tensors, this function dispatches these functions automatically: !sas-mul !scalar-mul !matrix-mul","title":"[function] !mul"},{"location":"base-impl/#inputs_15","text":"x y could be one of AbstractTensor number ScalarTensor","title":"Inputs"},{"location":"base-impl/#sideeffects_6","text":"None","title":"SideEffects"},{"location":"base-impl/#function-div","text":"(!div x y) The function provides general-purpose arithmetic operation. Given type of tensors, this function dispatches these functions automatically: !sas-div !scalar-div !matrix-div","title":"[function] !div"},{"location":"base-impl/#inputs_16","text":"x y could be one of AbstractTensor number ScalarTensor","title":"Inputs"},{"location":"base-impl/#sideeffects_7","text":"None","title":"SideEffects"},{"location":"base-impl/#function-move","text":"(!move place tensor) A \u2190 B A\\gets{B} A \u2190 B The function !move returns a node which moves tensor's visible elements into place's visible elements.","title":"[function] !move"},{"location":"base-impl/#nodes","text":"one of: MoveTensorNode ScalarTensorNode","title":"nodes"},{"location":"base-impl/#inputs_17","text":"place[AbstractTensor] tensor to be overwritten. tensor[AbstractTensor] tensor to be referred.","title":"Inputs"},{"location":"base-impl/#output","text":"Unevaluated Copied Tensor.","title":"Output"},{"location":"base-impl/#function-copy","text":"(!copy tensor) The function !copy returns a node which makes a copy the tensor's visible area. Note that: the function !copy never creates a new tensor larger than (tensor-vec tensor) has, (i.e.: copying broadcasted tensor will return broadcasted and copied tensor). !copy is used to make a cache before calling destructive operation to avoid side effects, therefore if the copy is included to be useless by compiler, this operations is being ignored without changing its behaviour. And this is why !copy returns InputTensor , not AbstractTensor . See also: !copy-force never being ignored by compiler, and broadcasted axes will be padded. Input: Tensor[AbstractTensor] Output: Tensor[AbstractTensor]","title":"[function] !copy"},{"location":"base-impl/#function-copy-force","text":"(!copy-force (tensor)) The function !copy-force returns a node which copies the given tensor forcibly while the function !copy sometimes ignored. This function is also used to adjust memory alignment of tensor.","title":"[function] !copy-force"},{"location":"base-impl/#function-reshape","text":"(!reshape tensor &rest shapes) Changes the shape of given tensor. Before and after the operation, the total elements of tensors must correspond.","title":"[function] !reshape"},{"location":"base-impl/#inputs_18","text":"tensor AbstractTensor but must not includes symbol in the shape. shapes could be one of: fixnum t . t can be used at one, but the value of t is automatically inferenced.","title":"Inputs"},{"location":"base-impl/#function-view","text":"(!view tensor &rest subscripts) The function !view returns a tensor which is applied lazy-evaluated view. For Example, let A be a 4x8 Matrix, and we gonna create a view of A that portrays A[:, 2] . (!view A 2 t) A B 0 ++++++++ -------- 1 ++++++++ -------- 2 ++++++++ -> [make a view] -> ++++++++ 3 ++++++++ -------- Here, A and B shares the pointer. Calling (shape B) returns (1 8) .","title":"[function] !view"},{"location":"base-impl/#subscripts","text":"Subscripts are following: t all elements in the axis. fixnum points out the specified index. (start end) slices the area. (start end step-by) slices the area by step-by . step-by can be a negative-fixnum. (Not tested) (:broadcast N-times) broadcasts the axis for N-times, the axis to be broadcasted must be 1 or broadcasted-axis. (:tflist ...) (TODO) (:indices ...) (TODO)","title":"Subscripts"},{"location":"base-impl/#return","text":"(values sliced-tensor broadcast-reverser) Tips: Applying !view again to the returned sliced-tensor with broadcast-reverser will remove broadcasts from the tensor.","title":"Return"},{"location":"base-impl/#function-flatten","text":"(!flatten tensor) equivalent to the (!reshape tensor t)","title":"[function] !flatten"},{"location":"base-impl/#function-rankup","text":"(!rankup tensor ntimes) The function !rankup appends/reduces 1 into the given tensor's shape for ntimes. If ntimes > 0, appends 1 If ntimes < 0, reduces 1, if the axis=1, otherwise returns error.","title":"[function] !rankup"},{"location":"base-impl/#function-scal","text":"(->scal matrix-tensor) The function ->scal receives matrix-tensor with total-size = 1, returning a ScalarTensor.","title":"[function] -&gt;scal"},{"location":"base-impl/#function-mat","text":"(->mat scalar-tensor &key (dims 1)) The function ->mat receives ScalarTensor , returning a matrix with the number of axis=dims.","title":"[function] -&gt;mat"},{"location":"base-impl/#function-proceed","text":"(proceed tensor &key (measure-time nil)) The function proceed invokes special node, ProceedNode , which takes all the previous computation node before tensor, returning the result of it. The backward is created with the previous node. This function will be useful especially when debugging on REPL.","title":"[function] proceed"},{"location":"base-impl/#inputs_19","text":"If measure-time =t, ProceedNode wraps with time macro when calling COMPILED forward and backward propagation. Compiling time isn't included to the displayed time while (time (proceed tensor)) includes.","title":"Inputs"},{"location":"base-impl/#function-proceed-time","text":"(proceed-time tensor) An alias for (proceed tensor :measure-time t)","title":"[function] proceed-time"},{"location":"base-impl/#function-proceed-backward","text":"(proceed-backward tensor) The function proceed-backward calls forward and backwrd of the tensor.","title":"[function] proceed-backward"},{"location":"base-impl/#output_1","text":"T (which indicates backward is succeed)","title":"Output"},{"location":"base-impl/#function-flexible","text":"(!flexible tensor) The function !flexible returns a node which adds 1 (which is broadcastable) to the head of the shape of tensor. That is: Tensor = (10 10) -> [!flexible] -> Tensor' = (1 ... 1 10 10) ^ <1 x N> Note that added axes could be broadcasted automatically when the operation called with multiple arguments.","title":"[function] !flexible"},{"location":"base-impl/#function-abs","text":"(!abs x &key (-> nil)) The function !abs takes x as an argument, applying a abs function into each element and writes the result into -> . O U T c o p y \u2190 a b s ( X ) OUT_{copy}\\gets{abs(X)} O U T co p y \u200b \u2190 ab s ( X ) (where OUT = -> )","title":"[function] !abs"},{"location":"base-impl/#inputs_20","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns","text":"->","title":"Returns"},{"location":"base-impl/#nodes_1","text":"SCALAR-ABSNODE ABSNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_8","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-sign","text":"(!sign x &key (-> nil)) The function !sign takes x as an argument, applying a sign function into each element and writes the result into -> . O U T c o p y \u2190 s i g n ( X ) OUT_{copy}\\gets{sign(X)} O U T co p y \u200b \u2190 s i g n ( X ) (where OUT = -> )","title":"[function] !sign"},{"location":"base-impl/#inputs_21","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_1","text":"->","title":"Returns"},{"location":"base-impl/#nodes_2","text":"SCALAR-SIGNNODE SIGNNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_9","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-sqrt","text":"(!sqrt x &key (-> nil)) The function !sqrt takes x as an argument, applying a sqrt function into each element and writes the result into -> . O U T c o p y \u2190 s q r t ( X ) OUT_{copy}\\gets{sqrt(X)} O U T co p y \u200b \u2190 s q r t ( X ) (where OUT = -> )","title":"[function] !sqrt"},{"location":"base-impl/#inputs_22","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_2","text":"->","title":"Returns"},{"location":"base-impl/#nodes_3","text":"SCALAR-SQRTNODE SQRTNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_10","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-square","text":"(!square x &key (-> nil)) The function !square takes x as an argument, applying a square function into each element and writes the result into -> . O U T c o p y \u2190 s q u a r e ( X ) OUT_{copy}\\gets{square(X)} O U T co p y \u200b \u2190 s q u a re ( X ) (where OUT = -> )","title":"[function] !square"},{"location":"base-impl/#inputs_23","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_3","text":"->","title":"Returns"},{"location":"base-impl/#nodes_4","text":"SCALAR-SQUARENODE SQUARENODE","title":"Nodes"},{"location":"base-impl/#sideeffects_11","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-sin","text":"(!sin x &key (-> nil)) The function !sin takes x as an argument, applying a sin function into each element and writes the result into -> . O U T c o p y \u2190 s i n ( X ) OUT_{copy}\\gets{sin(X)} O U T co p y \u200b \u2190 s in ( X ) (where OUT = -> )","title":"[function] !sin"},{"location":"base-impl/#inputs_24","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_4","text":"->","title":"Returns"},{"location":"base-impl/#nodes_5","text":"SCALAR-SINNODE SINNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_12","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-cos","text":"(!cos x &key (-> nil)) The function !cos takes x as an argument, applying a cos function into each element and writes the result into -> . O U T c o p y \u2190 c o s ( X ) OUT_{copy}\\gets{cos(X)} O U T co p y \u200b \u2190 cos ( X ) (where OUT = -> )","title":"[function] !cos"},{"location":"base-impl/#inputs_25","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_5","text":"->","title":"Returns"},{"location":"base-impl/#nodes_6","text":"SCALAR-COSNODE COSNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_13","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-tan","text":"(!tan x &key (-> nil)) The function !tan takes x as an argument, applying a tan function into each element and writes the result into -> . O U T c o p y \u2190 t a n ( X ) OUT_{copy}\\gets{tan(X)} O U T co p y \u200b \u2190 t an ( X ) (where OUT = -> )","title":"[function] !tan"},{"location":"base-impl/#inputs_26","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_6","text":"->","title":"Returns"},{"location":"base-impl/#nodes_7","text":"SCALAR-TANNODE TANNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_14","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-asin","text":"(!asin x &key (-> nil)) The function !asin takes x as an argument, applying a asin function into each element and writes the result into -> . O U T c o p y \u2190 a s i n ( X ) OUT_{copy}\\gets{asin(X)} O U T co p y \u200b \u2190 a s in ( X ) (where OUT = -> )","title":"[function] !asin"},{"location":"base-impl/#inputs_27","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_7","text":"->","title":"Returns"},{"location":"base-impl/#nodes_8","text":"SCALAR-ASINNODE ASINNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_15","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-acos","text":"(!acos x &key (-> nil)) The function !acos takes x as an argument, applying a acos function into each element and writes the result into -> . O U T c o p y \u2190 a c o s ( X ) OUT_{copy}\\gets{acos(X)} O U T co p y \u200b \u2190 a cos ( X ) (where OUT = -> )","title":"[function] !acos"},{"location":"base-impl/#inputs_28","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_8","text":"->","title":"Returns"},{"location":"base-impl/#nodes_9","text":"SCALAR-ACOSNODE ACOSNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_16","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-atan","text":"(!atan x &key (-> nil)) The function !atan takes x as an argument, applying a atan function into each element and writes the result into -> . O U T c o p y \u2190 a t a n ( X ) OUT_{copy}\\gets{atan(X)} O U T co p y \u200b \u2190 a t an ( X ) (where OUT = -> )","title":"[function] !atan"},{"location":"base-impl/#inputs_29","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_9","text":"->","title":"Returns"},{"location":"base-impl/#nodes_10","text":"SCALAR-ATANNODE ATANNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_17","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-sinh","text":"(!sinh x &key (-> nil)) The function !sinh takes x as an argument, applying a sinh function into each element and writes the result into -> . O U T c o p y \u2190 s i n h ( X ) OUT_{copy}\\gets{sinh(X)} O U T co p y \u200b \u2190 s inh ( X ) (where OUT = -> )","title":"[function] !sinh"},{"location":"base-impl/#inputs_30","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_10","text":"->","title":"Returns"},{"location":"base-impl/#nodes_11","text":"SCALAR-SINHNODE SINHNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_18","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-cosh","text":"(!cosh x &key (-> nil)) The function !cosh takes x as an argument, applying a cosh function into each element and writes the result into -> . O U T c o p y \u2190 c o s h ( X ) OUT_{copy}\\gets{cosh(X)} O U T co p y \u200b \u2190 cos h ( X ) (where OUT = -> )","title":"[function] !cosh"},{"location":"base-impl/#inputs_31","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_11","text":"->","title":"Returns"},{"location":"base-impl/#nodes_12","text":"SCALAR-COSHNODE COSHNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_19","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-tanh","text":"(!tanh x &key (-> nil)) The function !tanh takes x as an argument, applying a tanh function into each element and writes the result into -> . O U T c o p y \u2190 t a n h ( X ) OUT_{copy}\\gets{tanh(X)} O U T co p y \u200b \u2190 t anh ( X ) (where OUT = -> )","title":"[function] !tanh"},{"location":"base-impl/#inputs_32","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_12","text":"->","title":"Returns"},{"location":"base-impl/#nodes_13","text":"SCALAR-TANHNODE TANHNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_20","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-asinh","text":"(!asinh x &key (-> nil)) The function !asinh takes x as an argument, applying a asinh function into each element and writes the result into -> . O U T c o p y \u2190 a s i n h ( X ) OUT_{copy}\\gets{asinh(X)} O U T co p y \u200b \u2190 a s inh ( X ) (where OUT = -> )","title":"[function] !asinh"},{"location":"base-impl/#inputs_33","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_13","text":"->","title":"Returns"},{"location":"base-impl/#nodes_14","text":"SCALAR-ASINHNODE ASINHNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_21","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-acosh","text":"(!acosh x &key (-> nil)) The function !acosh takes x as an argument, applying a acosh function into each element and writes the result into -> . O U T c o p y \u2190 a c o s h ( X ) OUT_{copy}\\gets{acosh(X)} O U T co p y \u200b \u2190 a cos h ( X ) (where OUT = -> )","title":"[function] !acosh"},{"location":"base-impl/#inputs_34","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_14","text":"->","title":"Returns"},{"location":"base-impl/#nodes_15","text":"SCALAR-ACOSHNODE ACOSHNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_22","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-atanh","text":"(!atanh x &key (-> nil)) The function !atanh takes x as an argument, applying a atanh function into each element and writes the result into -> . O U T c o p y \u2190 a t a n h ( X ) OUT_{copy}\\gets{atanh(X)} O U T co p y \u200b \u2190 a t anh ( X ) (where OUT = -> )","title":"[function] !atanh"},{"location":"base-impl/#inputs_35","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_15","text":"->","title":"Returns"},{"location":"base-impl/#nodes_16","text":"SCALAR-ATANHNODE ATANHNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_23","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-exp","text":"(!exp x &key (-> nil)) The function !exp takes x as an argument, applying a exp function into each element and writes the result into -> . O U T c o p y \u2190 e x p ( X ) OUT_{copy}\\gets{exp(X)} O U T co p y \u200b \u2190 e x p ( X ) (where OUT = -> )","title":"[function] !exp"},{"location":"base-impl/#inputs_36","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_16","text":"->","title":"Returns"},{"location":"base-impl/#nodes_17","text":"SCALAR-EXPNODE EXPNODE","title":"Nodes"},{"location":"base-impl/#sideeffects_24","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-log2","text":"(!log2 x &key (-> nil)) The function !log2 takes x as an argument, applying a log2 function into each element and writes the result into -> . O U T c o p y \u2190 l o g 2 ( X ) OUT_{copy}\\gets{log2(X)} O U T co p y \u200b \u2190 l o g 2 ( X ) (where OUT = -> )","title":"[function] !log2"},{"location":"base-impl/#inputs_37","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_17","text":"->","title":"Returns"},{"location":"base-impl/#nodes_18","text":"SCALAR-LOG2NODE LOG2NODE","title":"Nodes"},{"location":"base-impl/#sideeffects_25","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-log10","text":"(!log10 x &key (-> nil)) The function !log10 takes x as an argument, applying a log10 function into each element and writes the result into -> . O U T c o p y \u2190 l o g 10 ( X ) OUT_{copy}\\gets{log10(X)} O U T co p y \u200b \u2190 l o g 10 ( X ) (where OUT = -> )","title":"[function] !log10"},{"location":"base-impl/#inputs_38","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_18","text":"->","title":"Returns"},{"location":"base-impl/#nodes_19","text":"SCALAR-LOG10NODE LOG10NODE","title":"Nodes"},{"location":"base-impl/#sideeffects_26","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-loge","text":"(!loge x &key (-> nil)) The function !loge takes x as an argument, applying a loge function into each element and writes the result into -> . O U T c o p y \u2190 l o g e ( X ) OUT_{copy}\\gets{loge(X)} O U T co p y \u200b \u2190 l o g e ( X ) (where OUT = -> )","title":"[function] !loge"},{"location":"base-impl/#inputs_39","text":"x [AbstractTensor or ScalarTensor or number] -> (nil or AbstractTensor). the place to set the result. If nil, a new tensor is allocated.","title":"Inputs"},{"location":"base-impl/#returns_19","text":"->","title":"Returns"},{"location":"base-impl/#nodes_20","text":"SCALAR-LOGENODE LOGENODE","title":"Nodes"},{"location":"base-impl/#sideeffects_27","text":"-> is destructed.","title":"SideEffects"},{"location":"base-impl/#function-sum","text":"(!sum tensor &key (axis t) (-> nil) (keep-repeat nil)) The function !sum return a node which computes the sum of tensor along the given axis.","title":"[function] !sum"},{"location":"base-impl/#inputs_40","text":"tensor , a tensor to be reducted. axis [t or fixnum or list] the axis to be reducted. (-1, -2... is ok) -> [AbstractTensor or nil] the place to set the result. If nil, creates a new tensor. keep-repeat [boolean] If t, the axis reducted is repeated. Return: -> [AbstractTensor] the result.","title":"Inputs"},{"location":"base-impl/#function-mean","text":"(!mean tensor &key (axis t) (-> nil) (keep-repeat nil)) The function !mean return a node which computes the average of tensor along the given axis.","title":"[function] !mean"},{"location":"base-impl/#inputs_41","text":"tensor , a tensor to be reducted. axis [t or fixnum or list] the axis to be reducted. (-1, -2... is ok) -> [AbstractTensor or nil] the place to set the result. If nil, creates a new tensor. keep-repeat [boolean] If t, the axis reducted is repeated.","title":"Inputs"},{"location":"base-impl/#return_1","text":"-> [AbstractTensor] the result.","title":"Return"},{"location":"base-impl/#function-argmax","text":"(!argmax tensor &key (axis -1) (out nil)) The function !argmax computes the indices of maximum values of all elements below the axis dimension in the given tensor.","title":"[function] !argmax"},{"location":"base-impl/#inputs_42","text":"tensor axis out","title":"Inputs"},{"location":"base-impl/#returns_20","text":"AbstractTensor[uint32] with dimensions behind axis is replaced with 1.## [function] !argmin (!argmin tensor &key (axis -1) (out nil)) The function !argmin computes the indices of minimum values of all elements below the axis dimension in the given tensor.","title":"Returns"},{"location":"base-impl/#inputs_43","text":"tensor axis out","title":"Inputs"},{"location":"base-impl/#returns_21","text":"AbstractTensor[uint32] with dimensions behind axis is replaced with 1.","title":"Returns"},{"location":"base-impl/#function-t","text":"(!t tensor) Applies Lazy-Transpose to the given tensor. The function is matmul-dedicated, so cooperationg with other operations (e.g.: !add) will cause the wrong result. (Internally, it is the equivalent to calling !reshape )","title":"[function] !t"},{"location":"base-impl/#current-problem","text":"Inconsistency of operations: !flexible(!t(x)).is_transposed? = NIL !t(!flexible(x)).is_flexible? = T","title":"Current Problem"},{"location":"base-impl/#function-matmul","text":"(!matmul x y &key (out nil) (transpose-x nil) (transpose-y nil)) Computing a matrix multiplication of X and Y, the function set the result into out. o u t \u2190 g e m m ( 1.0 , x , y , 0.0 , o u t ) out\\gets{gemm(1.0, x, y, 0.0, out)} o u t \u2190 g e mm ( 1.0 , x , y , 0.0 , o u t )","title":"[function] !matmul"},{"location":"base-impl/#inputs_44","text":"transpose-x transpose-y If t, the tensor is called with (!t tensor)","title":"Inputs"},{"location":"base-impl/#lazy-transpose","text":"Call the function (!t tensor) in advance to transpose the tensor without overheads. (!matmul (!t (randn `(5 3))) (randn `(5 3)))","title":"Lazy-Transpose"},{"location":"base-impl/#function-dot","text":"(!dot x y) Finds a dot product of x and y. Unlike numpy.dot , !dot intentionally only supports computing the dot product of two 1D tensors with the same number of elements. (proceed (!dot (randn `(100)) (randn `(10 10)))) {CPUTENSOR[float] :shape (1) -> :view (<0>) -> :visible-shape (1) :named ChainTMP115880 :vec-state [computed] (21.594929) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>}","title":"[function] !dot"},{"location":"base-impl/#function-where","text":"(!where tensor condition &key (true-then 1) (false-then 0) (out nil)) The function !where returns a elements selected-from true-then or false-then , depending on condition. The operation is defined as: o u t i = { true-then c o n d i t i o n ( X i ) false-then otherwise \\begin{equation} out_i= \\begin{cases} \\text{true-then} & condition(X_i) \\\\ \\text{false-then} & \\text{otherwise} \\end{cases} \\end{equation} o u t i \u200b = { true-then false-then \u200b co n d i t i o n ( X i \u200b ) otherwise \u200b \u200b \u200b (where X = tensor)","title":"[function] !where"},{"location":"base-impl/#inputs_45","text":"out place to set the result condition an funcallable function. (e.g.: #'evenp #'oddp etc...)","title":"Inputs"},{"location":"base-impl/#function-where_1","text":"(!compare tensor1 tensor2 condition &key (true-then 1) (false-then 0) (out nil)) The function !compare returns a elements selected-from true-then or false-then , depending on condition. The operation is defined as: o u t i = { true-then c o n d i t i o n ( X i , Y i ) false-then otherwise \\begin{equation} out_i= \\begin{cases} \\text{true-then} & condition(X_i, Y_i) \\\\ \\text{false-then} & \\text{otherwise} \\end{cases} \\end{equation} o u t i \u200b = { true-then false-then \u200b co n d i t i o n ( X i \u200b , Y i \u200b ) otherwise \u200b \u200b \u200b (where X = tensor1, Y=tensor2)","title":"[function] !where"},{"location":"base-impl/#inputs_46","text":"out place to set the result condition an funcallable function. (e.g.: #'> #'< etc...)","title":"Inputs"},{"location":"base-impl/#function-ascal","text":"(a>scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a>scal sets true-then if the equation: element > scal is t, otherwise set false-then at the corresponding positions.","title":"[function] a&gt;scal"},{"location":"base-impl/#inputs_47","text":"A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as scal)","title":"Inputs"},{"location":"base-impl/#function-ascal_1","text":"(a<scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a<scal sets true-then if the equation: element < scal is t, otherwise set false-then at the corresponding positions.","title":"[function] a&lt;scal"},{"location":"base-impl/#inputs_48","text":"A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as scal)","title":"Inputs"},{"location":"base-impl/#function-ascal_2","text":"(a>=scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a>=scal sets true-then if the equation: element >= scal is t, otherwise set false-then at the corresponding positions.","title":"[function] a&gt;=scal"},{"location":"base-impl/#inputs_49","text":"A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as scal)","title":"Inputs"},{"location":"base-impl/#function-ascal_3","text":"(a<=scal A scal &key (out nil) (true-then 1) (false-then 0)) The function a<=scal sets true-then if the equation: element <= scal is t, otherwise set false-then at the corresponding positions.","title":"[function] a&lt;=scal"},{"location":"base-impl/#inputs_50","text":"A AbstractTensor scal number (not a ScalarTensor) (TODO: ScalarTensor as scal)","title":"Inputs"},{"location":"base-impl/#function-ab","text":"(a>b A B &key (out nil) (true-then 1) (false-then 0)) The function a>b sets true-then if the equation: A > B is t, otherwise set false-then at the corresponding positions.","title":"[function] a&gt;b"},{"location":"base-impl/#inputs_51","text":"A B AbstractTensor to be compared.","title":"Inputs"},{"location":"base-impl/#function-ab_1","text":"(a<b A B &key (out nil) (true-then 1) (false-then 0)) The function a<b sets true-then if the equation: A < B is t, otherwise set false-then at the corresponding positions.","title":"[function] a&lt;b"},{"location":"base-impl/#inputs_52","text":"A B AbstractTensor to be compared.","title":"Inputs"},{"location":"base-impl/#function-ab_2","text":"(a>=b A B &key (out nil) (true-then 1) (false-then 0)) The function a>=b sets true-then if the equation: A >= B is t, otherwise set false-then at the corresponding positions.","title":"[function] a&gt;=b"},{"location":"base-impl/#inputs_53","text":"A B AbstractTensor to be compared.","title":"Inputs"},{"location":"base-impl/#function-ab_3","text":"(a<=b A B &key (out nil) (true-then 1) (false-then 0)) The function a<=b sets true-then if the equation: A <= B is t, otherwise set false-then at the corresponding positions.","title":"[function] a&lt;=b"},{"location":"base-impl/#inputs_54","text":"A B AbstractTensor to be compared.","title":"Inputs"},{"location":"distributions/","text":".katex img { object-fit: fill; padding: unset; display: block; position: absolute; width: 100%; height: inherit; } Distributions Samples matrices from distribution cl-waffe2 provides a package :cl-waffe2/distributions which is used to sample matrices from the distributions. Common Format to the APIs All sampling functions are defined in the following format via define-tensor-initializer macro. (function-name shape [Optional Arguments] &rest args &keys &allow-other-keys) That is, arguments passed to the make-tensor function can also be passed directly to the initializer functions. Example (normal `(10 10) 0.0 1.0 :requires-grad t) {CPUTENSOR[float] :shape (10 10) ((1.4823159 -0.46964306 -0.66347426 ~ 0.1336862 -0.17317852 -0.23254742) (1.5884124 -1.1133975 1.9798037 ~ 0.7578572 0.66243136 0.4400302) ... (1.2024113 0.19228306 -0.69422823 ~ -1.1183329 -2.4647195 0.12977293) (-0.41061524 0.69448847 0.45885974 ~ 1.8937484 -0.4626773 0.7627691)) :facet :exist :requires-grad T :backward NIL} Example (ax+b `(10 10) 1 0 :dtype :uint8) {CPUTENSOR[uint8] :shape (10 10) ((0 1 2 ~ 7 8 9) (10 11 12 ~ 17 18 19) ... (80 81 82 ~ 87 88 89) (90 91 92 ~ 97 98 99)) :facet :exist :requires-grad NIL :backward NIL} define-tensor-initializer (define-tensor-initializer (function-name (&rest args) initializer-lambda document &key (keep-order? nil))) define-tensor-initializer is a macro which is used to define a initializer function. Initializer function is a function whose arguments follow this format: (function-name shape <Initializer's Arguments> &rest initargs &key &allow-other-keys) Input: function-name - the function is defined after this argument args - Initializer's Arguments initializer-lambda - A form to be expanded as the sampling function, which must return a function of #'(lambda (i) ...) where i is the index of element. keep-order? - set t if the index is needed to sampling matrices. Example: (define-initializer-function uniform-random (upfrom below) (let ((upfrom (coerce upfrom (dtype->lisp-type (dtype tensor)))) (below (coerce below (dtype->lisp-type (dtype tensor))))) #'(lambda (i) (declare (ignore i)) (sample-uniform-random upfrom below))) \"\") (uniform-random `(10 10) 0.1 0.3 :requires-grad t) {CPUTENSOR[float] :shape (10 10) ((0.13149574 0.15135926 0.1569588 ~ 0.103781514 0.20610212 0.19365484) (0.2638953 0.12672275 0.21630599 ~ 0.16542184 0.10228193 0.12928057) ... (0.20429519 0.12252951 0.17538154 ~ 0.22072719 0.18642941 0.11027551) (0.14372297 0.11097031 0.25514898 ~ 0.28739202 0.18398522 0.15176433)) :facet :exist :requires-grad T :backward NIL} (Note that new tensor is binded to tensor, being used to determined dtype etc...) ax+b (ax+b shape a b &rest initargs &key &allow-other-keys) The function ax+b is a family of initializer functions, and samples matrices from arithmetic progression. o u t n = a n + b out_n = an + b o u t n \u200b = an + b Inputs: a, b - Coefficients of the above formula. Example (ax+b `(3 3) 1.0 0.0) {CPUTENSOR[float] :shape (3 3) ((0.0 1.0 2.0) (3.0 4.0 5.0) (6.0 7.0 8.0)) :facet :exist :requires-grad NIL :backward NIL} beta (beta shape alpha beta &rest initargs &key &allow-other-keys) The function beta is a family of initializer functions, and sample matrices from beta distribution. Reference Generating Beta Variates with Nonintegral Shape Parameters (R. C. H. Cheng University of Wales Institute of Science and Technology) https://dl.acm.org/doi/pdf/10.1145/359460.359482 Note: My implementation is unstable, being occurs floating-overflow constantly..., especially when min(alpha, beta) < 1.0 (i.e.: beta-bc) Example (beta `(3 3) 5.0 1.0) {CPUTENSOR[float] :shape (3 3) ((0.95779675 0.78534156 0.97238594) (0.5043175 0.8153589 0.84215444) (0.9688498 0.79584134 0.8444515)) :facet :exist :requires-grad NIL :backward NIL} bernoulli (bernoulli shape p &rest initargs &key &allow-other-keys) The bernoulli is a family of initializer functions, and samples matrices from bernoulli distribution. Inputs p - Takes 1 with probability p and 0 with probalibity (1-p). Example (bernoulli `(3 3) 0.3) {CPUTENSOR[float] :shape (3 3) ((0.0 1.0 1.0) (0.0 0.0 0.0) (1.0 0.0 0.0)) :facet :exist :requires-grad NIL :backward NIL} chisquare (chisquare shape df &rest initargs &key &allow-other-keys) The function chisquare is a family of initializer functions, and samples matrices from chisquare distributions. Inputs df - degree of freedom. References https://github.com/lvaruzza/cl-randist Example (chisquare `(3 3) 1.0) {CPUTENSOR[float] :shape (3 3) ((0.26887837 0.075598314 0.038562592) (0.15240487 0.020331385 0.0014944144) (0.04257775 1.7627947 1.0678082)) :facet :exist :requires-grad NIL :backward NIL} expotential (expotential shape &rest initargs &key &allow-other-keys) The function expotential is a family of initializer functions, and samples the expotential distribution using ziggurat algorithm with table-size=256. References https://andantesoft.hatenablog.com/entry/2023/04/30/183032 Marsaglia, G., & Tsang, W. W. (2000). The ziggurat method for generating random variables. Journal of statistical software. https://marui.hatenablog.com/entry/2023/01/23/194507 Example (expotential `(3 3)) {CPUTENSOR[float] :shape (3 3) ((0.52079695 2.3378499 0.532929) (0.3156159 0.94859123 1.0433507) (0.32960054 0.014408455 0.53494173)) :facet :exist :requires-grad NIL :backward NIL} gamma (gamma shape k &rest initargs &key &allow-other-keys) The function gamma is a family of initializer functions, and samples matrices from the gamma distribution. References https://github.com/lvaruzza/cl-randist Example (gamma `(3 3) 1.0) {CPUTENSOR[float] :shape (3 3) ((2.0138345 0.46096373 0.47513118) (0.44766566 0.070686094 1.0288159) (0.24305491 0.5462445 0.32319143)) :facet :exist :requires-grad NIL :backward NIL} normal (normal shape mean stddev &rest initargs &key &allow-other-keys) The function normal is a family of initializer functions, and samples matrices from normal distribution. Reference https://github.com/lvaruzza/cl-randist (seems to create ziggurat table with size=128) Inputs mean stddev - Standard Deviation, \u03c3. Example (normal `(3 3) 1.0 0.0) {CPUTENSOR[float] :shape (3 3) ((1.0 1.0 1.0) (1.0 1.0 1.0) (1.0 1.0 1.0)) :facet :exist :requires-grad NIL :backward NIL} uniform-random (uniform-random shape upfrom below &rest initargs &key &allow-other-keys) The function uniform-random is a family of initializer funtions, and samples matrices from uniform random distribution using Common Lisp's standard function, (random arg) . Input: upfrom, below. Each elements of returned tensor is in the range of: `[upfrom, below)` Example (uniform-random `(3 3) 2 4) {CPUTENSOR[float] :shape (3 3) ((2.081888 3.5663512 2.6103349) (2.2481 2.4701407 3.9164069) (2.9929533 2.531603 2.844342)) :facet :exist :requires-grad NIL :backward NIL} randn (randn shape &rest initargs &key &allow-other-keys) The function randn is a family of initializer functions, and samples the gaussian distributions using ziggurat algorithm with table-size=256. References https://andantesoft.hatenablog.com/entry/2023/04/30/183032 Marsaglia, G., & Tsang, W. W. (2000). The ziggurat method for generating random variables. Journal of statistical software. https://marui.hatenablog.com/entry/2023/01/23/194507 Example (randn `(3 3)) {CPUTENSOR[float] :shape (3 3) ((0.29577962 1.9735839 -0.62287575) (-0.20879823 -0.008091144 0.07309098) (-2.0688834 0.8136456 -1.0385108)) :facet :exist :requires-grad NIL :backward NIL}","title":"cl-waffe2/distributions"},{"location":"distributions/#distributions","text":"","title":"Distributions"},{"location":"distributions/#samples-matrices-from-distribution","text":"cl-waffe2 provides a package :cl-waffe2/distributions which is used to sample matrices from the distributions.","title":"Samples matrices from distribution"},{"location":"distributions/#common-format-to-the-apis","text":"All sampling functions are defined in the following format via define-tensor-initializer macro. (function-name shape [Optional Arguments] &rest args &keys &allow-other-keys) That is, arguments passed to the make-tensor function can also be passed directly to the initializer functions.","title":"Common Format to the APIs"},{"location":"distributions/#example","text":"(normal `(10 10) 0.0 1.0 :requires-grad t) {CPUTENSOR[float] :shape (10 10) ((1.4823159 -0.46964306 -0.66347426 ~ 0.1336862 -0.17317852 -0.23254742) (1.5884124 -1.1133975 1.9798037 ~ 0.7578572 0.66243136 0.4400302) ... (1.2024113 0.19228306 -0.69422823 ~ -1.1183329 -2.4647195 0.12977293) (-0.41061524 0.69448847 0.45885974 ~ 1.8937484 -0.4626773 0.7627691)) :facet :exist :requires-grad T :backward NIL}","title":"Example"},{"location":"distributions/#example_1","text":"(ax+b `(10 10) 1 0 :dtype :uint8) {CPUTENSOR[uint8] :shape (10 10) ((0 1 2 ~ 7 8 9) (10 11 12 ~ 17 18 19) ... (80 81 82 ~ 87 88 89) (90 91 92 ~ 97 98 99)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#define-tensor-initializer","text":"(define-tensor-initializer (function-name (&rest args) initializer-lambda document &key (keep-order? nil))) define-tensor-initializer is a macro which is used to define a initializer function. Initializer function is a function whose arguments follow this format: (function-name shape <Initializer's Arguments> &rest initargs &key &allow-other-keys) Input: function-name - the function is defined after this argument args - Initializer's Arguments initializer-lambda - A form to be expanded as the sampling function, which must return a function of #'(lambda (i) ...) where i is the index of element. keep-order? - set t if the index is needed to sampling matrices. Example: (define-initializer-function uniform-random (upfrom below) (let ((upfrom (coerce upfrom (dtype->lisp-type (dtype tensor)))) (below (coerce below (dtype->lisp-type (dtype tensor))))) #'(lambda (i) (declare (ignore i)) (sample-uniform-random upfrom below))) \"\") (uniform-random `(10 10) 0.1 0.3 :requires-grad t) {CPUTENSOR[float] :shape (10 10) ((0.13149574 0.15135926 0.1569588 ~ 0.103781514 0.20610212 0.19365484) (0.2638953 0.12672275 0.21630599 ~ 0.16542184 0.10228193 0.12928057) ... (0.20429519 0.12252951 0.17538154 ~ 0.22072719 0.18642941 0.11027551) (0.14372297 0.11097031 0.25514898 ~ 0.28739202 0.18398522 0.15176433)) :facet :exist :requires-grad T :backward NIL} (Note that new tensor is binded to tensor, being used to determined dtype etc...)","title":"define-tensor-initializer"},{"location":"distributions/#axb","text":"(ax+b shape a b &rest initargs &key &allow-other-keys) The function ax+b is a family of initializer functions, and samples matrices from arithmetic progression. o u t n = a n + b out_n = an + b o u t n \u200b = an + b Inputs: a, b - Coefficients of the above formula.","title":"ax+b"},{"location":"distributions/#example_2","text":"(ax+b `(3 3) 1.0 0.0) {CPUTENSOR[float] :shape (3 3) ((0.0 1.0 2.0) (3.0 4.0 5.0) (6.0 7.0 8.0)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#beta","text":"(beta shape alpha beta &rest initargs &key &allow-other-keys) The function beta is a family of initializer functions, and sample matrices from beta distribution.","title":"beta"},{"location":"distributions/#reference","text":"Generating Beta Variates with Nonintegral Shape Parameters (R. C. H. Cheng University of Wales Institute of Science and Technology) https://dl.acm.org/doi/pdf/10.1145/359460.359482 Note: My implementation is unstable, being occurs floating-overflow constantly..., especially when min(alpha, beta) < 1.0 (i.e.: beta-bc)","title":"Reference"},{"location":"distributions/#example_3","text":"(beta `(3 3) 5.0 1.0) {CPUTENSOR[float] :shape (3 3) ((0.95779675 0.78534156 0.97238594) (0.5043175 0.8153589 0.84215444) (0.9688498 0.79584134 0.8444515)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#bernoulli","text":"(bernoulli shape p &rest initargs &key &allow-other-keys) The bernoulli is a family of initializer functions, and samples matrices from bernoulli distribution.","title":"bernoulli"},{"location":"distributions/#inputs","text":"p - Takes 1 with probability p and 0 with probalibity (1-p).","title":"Inputs"},{"location":"distributions/#example_4","text":"(bernoulli `(3 3) 0.3) {CPUTENSOR[float] :shape (3 3) ((0.0 1.0 1.0) (0.0 0.0 0.0) (1.0 0.0 0.0)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#chisquare","text":"(chisquare shape df &rest initargs &key &allow-other-keys) The function chisquare is a family of initializer functions, and samples matrices from chisquare distributions.","title":"chisquare"},{"location":"distributions/#inputs_1","text":"df - degree of freedom.","title":"Inputs"},{"location":"distributions/#references","text":"https://github.com/lvaruzza/cl-randist","title":"References"},{"location":"distributions/#example_5","text":"(chisquare `(3 3) 1.0) {CPUTENSOR[float] :shape (3 3) ((0.26887837 0.075598314 0.038562592) (0.15240487 0.020331385 0.0014944144) (0.04257775 1.7627947 1.0678082)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#expotential","text":"(expotential shape &rest initargs &key &allow-other-keys) The function expotential is a family of initializer functions, and samples the expotential distribution using ziggurat algorithm with table-size=256.","title":"expotential"},{"location":"distributions/#references_1","text":"https://andantesoft.hatenablog.com/entry/2023/04/30/183032 Marsaglia, G., & Tsang, W. W. (2000). The ziggurat method for generating random variables. Journal of statistical software. https://marui.hatenablog.com/entry/2023/01/23/194507","title":"References"},{"location":"distributions/#example_6","text":"(expotential `(3 3)) {CPUTENSOR[float] :shape (3 3) ((0.52079695 2.3378499 0.532929) (0.3156159 0.94859123 1.0433507) (0.32960054 0.014408455 0.53494173)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#gamma","text":"(gamma shape k &rest initargs &key &allow-other-keys) The function gamma is a family of initializer functions, and samples matrices from the gamma distribution.","title":"gamma"},{"location":"distributions/#references_2","text":"https://github.com/lvaruzza/cl-randist","title":"References"},{"location":"distributions/#example_7","text":"(gamma `(3 3) 1.0) {CPUTENSOR[float] :shape (3 3) ((2.0138345 0.46096373 0.47513118) (0.44766566 0.070686094 1.0288159) (0.24305491 0.5462445 0.32319143)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#normal","text":"(normal shape mean stddev &rest initargs &key &allow-other-keys) The function normal is a family of initializer functions, and samples matrices from normal distribution.","title":"normal"},{"location":"distributions/#reference_1","text":"https://github.com/lvaruzza/cl-randist (seems to create ziggurat table with size=128)","title":"Reference"},{"location":"distributions/#inputs_2","text":"mean stddev - Standard Deviation, \u03c3.","title":"Inputs"},{"location":"distributions/#example_8","text":"(normal `(3 3) 1.0 0.0) {CPUTENSOR[float] :shape (3 3) ((1.0 1.0 1.0) (1.0 1.0 1.0) (1.0 1.0 1.0)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#uniform-random","text":"(uniform-random shape upfrom below &rest initargs &key &allow-other-keys) The function uniform-random is a family of initializer funtions, and samples matrices from uniform random distribution using Common Lisp's standard function, (random arg) . Input: upfrom, below. Each elements of returned tensor is in the range of: `[upfrom, below)`","title":"uniform-random"},{"location":"distributions/#example_9","text":"(uniform-random `(3 3) 2 4) {CPUTENSOR[float] :shape (3 3) ((2.081888 3.5663512 2.6103349) (2.2481 2.4701407 3.9164069) (2.9929533 2.531603 2.844342)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"distributions/#randn","text":"(randn shape &rest initargs &key &allow-other-keys) The function randn is a family of initializer functions, and samples the gaussian distributions using ziggurat algorithm with table-size=256.","title":"randn"},{"location":"distributions/#references_3","text":"https://andantesoft.hatenablog.com/entry/2023/04/30/183032 Marsaglia, G., & Tsang, W. W. (2000). The ziggurat method for generating random variables. Journal of statistical software. https://marui.hatenablog.com/entry/2023/01/23/194507","title":"References"},{"location":"distributions/#example_10","text":"(randn `(3 3)) {CPUTENSOR[float] :shape (3 3) ((0.29577962 1.9735839 -0.62287575) (-0.20879823 -0.008091144 0.07309098) (-2.0688834 0.8136456 -1.0385108)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"generic-tensor/","text":"AbstractTensor AbstractTensor [class] AbstractTensor AbstractTensor is a primal class for all devices. Each devices (e.g.: ScalarTensor LispTensor CPUTensor etc...) is a subclass of this. The class provides the fundamental and necessary features for tensors. Lazy-Evaluated and Multi-Dimensional APIs, stride computations. View APIs multi-dimensional offsets To construct backward, AbstractTensor records variables called with. vec container. an space for saving gradients, copies for backward. Lazy-Evaluated Shapings Trace Informations for JIT to create well-optimized computation node. Create a new backend. Users can create a new backend by extending this abstract class. (defclass MyBackend (AbstractNode) nil) To use the MyBackend as a tensor, users also has to override these methods: initialize-instance ... An allocator for tensor's vec. vref (setf vref) ... an generic function to access/write tensor's vec. ;; TODO: Establish a common API for initargs (defmethod initialize-instance :before ((tensor MyBackend) &rest initargs &key &allow-other-keys) ;; if projected-p -> alloc new vec (let* ((shape (getf initargs :shape)) (dtype (dtype->lisp-type (getf initargs :dtype))) (vec (getf initargs :vec)) (facet (getf initargs :facet)) (initial-element (coerce (or (getf initargs :initial-element) 0) dtype))) (when (eql facet :exist) (if vec (setf (tensor-vec tensor) vec) (setf (tensor-vec tensor) ;; vec can be anything. (make-array (apply #'* shape) :element-type dtype :initial-element initial-element)))))) (defmethod vref ((tensor MyBackend) index) (aref (tensor-vec tensor) index)) (defmethod (setf vref) (new-value (tensor MyBackend) index) (setf (aref (tensor-vec tensor) index) new-value)) Now, the name MyBackend is available as a brand-new cl-waffe2 backend! Users can define a new implementation following (define-impl (Name :device MyBackend) ...) (See the examples to understand how this could be achieved at ./source/backends/lisp/tensor.lisp. or ./source/backends/cpu.) [slot] orig-shape (List) the original shape of vec . (apply #'* orig-shape) must correspond with the number of total elements of vec . [slot] stride (list) An stride of tensor, can be chosen from :column :row . This slot can be accessed by (tensor-stride object) . [slot] visible-shape (list) An shape of visible-area of tensor, visible-area is that an viewed size of tensor. Can be accessed by (shape object) [slot] view (list) An list of multidimensional offsets, view. Can be accessed by (tensor-view object) [slot] projected-p (boolean) Set t if (apply #'* orig-shape) == (apply #'* visible-shape) otherwise set nil. If t, the tensor is produced by !view or view functions. [slot] scalar-p If t, the tensor is regarded as a Scalar. [slot] detach-p If t, JIT compilers stop tracing at the tensor. [slot] state Stores a corresponding StateContainer . [slot] variables (tensor-variables object) Records variables called with the tensor. [slot] tensor-id (symbol) Corresponding variable name that used in JIT compiler. [slot] grad (AbstractTensor) If the tensor is a parameter, (i.e.: requires-grad t) and backward propagation has called, the gradients has set to this slot. Reader: (grad object) . Writer: (set-grad object value) [slot] backward (AbstractNode) the node called with. [slot] requires-grad (Boolean) If t, the tensor become a parameter that gradients are saved. [slot] ancestor-param-p (Boolean) If t, the tensor has created by parameter or tensors whose ancestor-param-p=t. [slot] flexible-p (Boolean) If t, the tensor is broadcastable [slot] facet (keyword) Tensors has a two state: :input :exist :exist tensor is a just normal state, which vec is already allocated. :input tensor is a lazy-evaluated tensor, which allocation will be done until they're really needed. (often used as a cache, or training data.) ... make-tensor [function] make-tensor (make-tensor shape-or-scalar &key (requires-grad nil) (dtype *default-dtype*) (vec nil) (view nil) (order *default-order*) (initial-element)) Refering a first-priority of using-backends (i.e.: car of *using-backends* ) to know what device to use, the function make-tensor creates and allocate a new matrix instantly. Input shape-or-scalar (Any) set list (consisted of fixnum) here to create a matrix, otherwise the ScalarTensor is forcibly created. requires-grad (Boolean) Set t to create gradient. (e.g.: the tensor is needed to be optimized.) dtype (keyword) Set dtype you wanna use. See also: (Dtype API) vec (Anything) If you wanna pass the make-instance to already-allocated matrix, use this parameter. order (member :column :row) initial-element (Optional) Example (make-tensor `(10 10) :initial-element 1.0) {CPUTENSOR[float] :shape (10 10) ((1.0 1.0 1.0 ~ 1.0 1.0 1.0) (1.0 1.0 1.0 ~ 1.0 1.0 1.0) ... (1.0 1.0 1.0 ~ 1.0 1.0 1.0) (1.0 1.0 1.0 ~ 1.0 1.0 1.0)) :facet :exist :requires-grad NIL :backward NIL} make-input [function] make-input Referring a first-priority of using-backend (i.e.: car part), the function make-input creates a InputTensor. In contrast to make-tensor , allocation of vec is lazy-evaluated, and shape can include symbols. (Lazy-Evaluated Shape). For example, whichever (make-input (list 256 256 256 256 256 256) nil) or (make-input (list 256) nil) is called, the memory-usage is the same until (tensor-vec tensor) is called but the moment (tensor-vec tensor) is called, the first one would cause CUDA OUT OF MEMORY or something :(. Inputs Shape [list] consisted of fixnum or symbol. (e.g.: (a 10) is a valid shape.) Named [keyword] the name of input. If nil, the tensor is regarded as just cache. If you want to change the content of inputs later (e.g.: training data), set an appropriate name. scalar-p [boolean] set t is the input is scalar. dtype [keyword] as it is. order [keyword] an member of :column :row Example (make-input `(a 10) :train-x) {CPUTENSOR[float] :shape (A 10) :named TRAIN-X <<Not-Embodied (A 10) Tensor>> :facet :input :requires-grad NIL :backward NIL} The InputTensor named with a keyword is called not-embodied tensor , and can be changed its vec with embody-input embody-input (embody-input variables :a tensor) Example REPL: > (setq out (!add (randn `(10 10)) (make-input `(a 10) :x))) {CPUTENSOR[float] :shape (10 10) :named ChainTMP35895 :vec-state [maybe-not-computed] <<Not-Embodied (10 10) Tensor>> :facet :input :requires-grad NIL :backward <Node: ADDNODE-CPUTENSOR (A[~] B[~] -> A[~])>} REPL: > (with-build (fw bw vars params) out (embody-input vars :x (randn `(10 10))) ;; :X = (randn `(10 10)) (funcall fw)) {CPUTENSOR[float] :shape (10 10) :named ChainTMP35884 ((-2.1486177 1.4877725 -1.7822108 ~ 0.30888113 -3.668074 -1.4501324) (0.90827906 -3.6974688 -0.7262471 ~ 2.153652 0.7110309 1.2819712) ... (-2.6074939 0.04147309 -0.97653854 ~ 0.3843904 -0.20308924 -0.614793) (1.7244194 1.5219165 0.3820825 ~ -0.41161555 0.5861892 0.18113303)) :facet :input :requires-grad NIL :backward NIL} build Return: (values forward backward variables parameters) Example REPL: > (setq out (!add (randn `(10 10)) (make-input `(a 10) :X))) {CPUTENSOR[float] :shape (10 10) :named ChainTMP35924 :vec-state [maybe-not-computed] <<Not-Embodied (10 10) Tensor>> :facet :input :requires-grad NIL :backward <Node: ADDNODE-CPUTENSOR (A[~] B[~] -> A[~])>} REPL: > (multiple-value-list (build out)) (#<FUNCTION (LAMBDA () :IN \"/private/var/tmp/slimemU2Krr.fasl\") {53A5B54B}> #<FUNCTION (LAMBDA () :IN \"/private/var/tmp/slimemU2Krr.fasl\") {53A8776B}> += [Computation Node Information] =======+ Subscripts: [A -> ?] Variables NAMES | SIZE | \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013 X | (A 10) | - The number of tmp variables: 4 +========================================+ #S(NODEPARAMETERS :PARAMETERS (omitted) :ntensors 3)) tensor-vec (tensor-vec tensor) Accessing the pointer/array the tensor has. Not until tensor-vec is called, the new area isn't allocated. mref (mref tensor &rest subscripts) Read-only. Only used for printing the tensor. Whether you cares about performance or not, this function shouldn't be used ignoring for printing tensors. vref (vref tensor index) vref is a generic-function to access tensor's vec. Whether you cares about performance or not, this function shouldn't be used ignoring for printing tensors. If you've created a new backend with having different ptr-type (can't be accessed by aref), only you have to do is to redefine vref. set-save-for-backward NIL read-save-for-backward NIL *no-grad* [parameter] *no-grad* If t, all operations don't create gradients. with-no-grad [macro] with-no-grad (with-no-grad &body body) Set *np-grad* t under the body execution, no gradients are made for backward. parameter The function parameter computes all the previous nodes of the given tensor, returning the new tensor with requires-grad=t. Example: (parameter (randn `(3 3))) dtype->lisp-type NIL call-with-view NIL stride-of NIL size-of NIL offset-of NIL shape-equal [function] shape-equal a=1, b=k => T a=1, b=2 => NIL ... force-list Returns subscript-t if view is Subscript otherwise returns a view","title":"cl-waffe2/vm.generic-tensor"},{"location":"generic-tensor/#abstracttensor","text":"","title":"AbstractTensor"},{"location":"generic-tensor/#abstracttensor_1","text":"[class] AbstractTensor AbstractTensor is a primal class for all devices. Each devices (e.g.: ScalarTensor LispTensor CPUTensor etc...) is a subclass of this. The class provides the fundamental and necessary features for tensors. Lazy-Evaluated and Multi-Dimensional APIs, stride computations. View APIs multi-dimensional offsets To construct backward, AbstractTensor records variables called with. vec container. an space for saving gradients, copies for backward. Lazy-Evaluated Shapings Trace Informations for JIT to create well-optimized computation node.","title":"AbstractTensor"},{"location":"generic-tensor/#create-a-new-backend","text":"Users can create a new backend by extending this abstract class. (defclass MyBackend (AbstractNode) nil) To use the MyBackend as a tensor, users also has to override these methods: initialize-instance ... An allocator for tensor's vec. vref (setf vref) ... an generic function to access/write tensor's vec. ;; TODO: Establish a common API for initargs (defmethod initialize-instance :before ((tensor MyBackend) &rest initargs &key &allow-other-keys) ;; if projected-p -> alloc new vec (let* ((shape (getf initargs :shape)) (dtype (dtype->lisp-type (getf initargs :dtype))) (vec (getf initargs :vec)) (facet (getf initargs :facet)) (initial-element (coerce (or (getf initargs :initial-element) 0) dtype))) (when (eql facet :exist) (if vec (setf (tensor-vec tensor) vec) (setf (tensor-vec tensor) ;; vec can be anything. (make-array (apply #'* shape) :element-type dtype :initial-element initial-element)))))) (defmethod vref ((tensor MyBackend) index) (aref (tensor-vec tensor) index)) (defmethod (setf vref) (new-value (tensor MyBackend) index) (setf (aref (tensor-vec tensor) index) new-value)) Now, the name MyBackend is available as a brand-new cl-waffe2 backend! Users can define a new implementation following (define-impl (Name :device MyBackend) ...) (See the examples to understand how this could be achieved at ./source/backends/lisp/tensor.lisp. or ./source/backends/cpu.)","title":"Create a new backend."},{"location":"generic-tensor/#slot-orig-shape-list","text":"the original shape of vec . (apply #'* orig-shape) must correspond with the number of total elements of vec .","title":"[slot] orig-shape (List)"},{"location":"generic-tensor/#slot-stride-list","text":"An stride of tensor, can be chosen from :column :row . This slot can be accessed by (tensor-stride object) .","title":"[slot] stride (list)"},{"location":"generic-tensor/#slot-visible-shape-list","text":"An shape of visible-area of tensor, visible-area is that an viewed size of tensor. Can be accessed by (shape object)","title":"[slot] visible-shape (list)"},{"location":"generic-tensor/#slot-view-list","text":"An list of multidimensional offsets, view. Can be accessed by (tensor-view object)","title":"[slot] view (list)"},{"location":"generic-tensor/#slot-projected-p-boolean","text":"Set t if (apply #'* orig-shape) == (apply #'* visible-shape) otherwise set nil. If t, the tensor is produced by !view or view functions.","title":"[slot] projected-p (boolean)"},{"location":"generic-tensor/#slot-scalar-p","text":"If t, the tensor is regarded as a Scalar.","title":"[slot] scalar-p"},{"location":"generic-tensor/#slot-detach-p","text":"If t, JIT compilers stop tracing at the tensor.","title":"[slot] detach-p"},{"location":"generic-tensor/#slot-state","text":"Stores a corresponding StateContainer .","title":"[slot] state"},{"location":"generic-tensor/#slot-variables","text":"(tensor-variables object) Records variables called with the tensor.","title":"[slot] variables"},{"location":"generic-tensor/#slot-tensor-id-symbol","text":"Corresponding variable name that used in JIT compiler.","title":"[slot] tensor-id (symbol)"},{"location":"generic-tensor/#slot-grad-abstracttensor","text":"If the tensor is a parameter, (i.e.: requires-grad t) and backward propagation has called, the gradients has set to this slot. Reader: (grad object) . Writer: (set-grad object value)","title":"[slot] grad (AbstractTensor)"},{"location":"generic-tensor/#slot-backward-abstractnode","text":"the node called with.","title":"[slot] backward (AbstractNode)"},{"location":"generic-tensor/#slot-requires-grad-boolean","text":"If t, the tensor become a parameter that gradients are saved.","title":"[slot] requires-grad (Boolean)"},{"location":"generic-tensor/#slot-ancestor-param-p-boolean","text":"If t, the tensor has created by parameter or tensors whose ancestor-param-p=t.","title":"[slot] ancestor-param-p (Boolean)"},{"location":"generic-tensor/#slot-flexible-p-boolean","text":"If t, the tensor is broadcastable","title":"[slot] flexible-p (Boolean)"},{"location":"generic-tensor/#slot-facet-keyword","text":"Tensors has a two state: :input :exist :exist tensor is a just normal state, which vec is already allocated. :input tensor is a lazy-evaluated tensor, which allocation will be done until they're really needed. (often used as a cache, or training data.) ...","title":"[slot] facet (keyword)"},{"location":"generic-tensor/#make-tensor","text":"","title":"make-tensor"},{"location":"generic-tensor/#function-make-tensor","text":"(make-tensor shape-or-scalar &key (requires-grad nil) (dtype *default-dtype*) (vec nil) (view nil) (order *default-order*) (initial-element)) Refering a first-priority of using-backends (i.e.: car of *using-backends* ) to know what device to use, the function make-tensor creates and allocate a new matrix instantly.","title":"[function] make-tensor"},{"location":"generic-tensor/#input","text":"shape-or-scalar (Any) set list (consisted of fixnum) here to create a matrix, otherwise the ScalarTensor is forcibly created. requires-grad (Boolean) Set t to create gradient. (e.g.: the tensor is needed to be optimized.) dtype (keyword) Set dtype you wanna use. See also: (Dtype API) vec (Anything) If you wanna pass the make-instance to already-allocated matrix, use this parameter. order (member :column :row) initial-element (Optional)","title":"Input"},{"location":"generic-tensor/#example","text":"(make-tensor `(10 10) :initial-element 1.0) {CPUTENSOR[float] :shape (10 10) ((1.0 1.0 1.0 ~ 1.0 1.0 1.0) (1.0 1.0 1.0 ~ 1.0 1.0 1.0) ... (1.0 1.0 1.0 ~ 1.0 1.0 1.0) (1.0 1.0 1.0 ~ 1.0 1.0 1.0)) :facet :exist :requires-grad NIL :backward NIL}","title":"Example"},{"location":"generic-tensor/#make-input","text":"","title":"make-input"},{"location":"generic-tensor/#function-make-input","text":"Referring a first-priority of using-backend (i.e.: car part), the function make-input creates a InputTensor. In contrast to make-tensor , allocation of vec is lazy-evaluated, and shape can include symbols. (Lazy-Evaluated Shape). For example, whichever (make-input (list 256 256 256 256 256 256) nil) or (make-input (list 256) nil) is called, the memory-usage is the same until (tensor-vec tensor) is called but the moment (tensor-vec tensor) is called, the first one would cause CUDA OUT OF MEMORY or something :(.","title":"[function] make-input"},{"location":"generic-tensor/#inputs","text":"Shape [list] consisted of fixnum or symbol. (e.g.: (a 10) is a valid shape.) Named [keyword] the name of input. If nil, the tensor is regarded as just cache. If you want to change the content of inputs later (e.g.: training data), set an appropriate name. scalar-p [boolean] set t is the input is scalar. dtype [keyword] as it is. order [keyword] an member of :column :row","title":"Inputs"},{"location":"generic-tensor/#example_1","text":"(make-input `(a 10) :train-x) {CPUTENSOR[float] :shape (A 10) :named TRAIN-X <<Not-Embodied (A 10) Tensor>> :facet :input :requires-grad NIL :backward NIL} The InputTensor named with a keyword is called not-embodied tensor , and can be changed its vec with embody-input","title":"Example"},{"location":"generic-tensor/#embody-input","text":"(embody-input variables :a tensor)","title":"embody-input"},{"location":"generic-tensor/#example_2","text":"REPL: > (setq out (!add (randn `(10 10)) (make-input `(a 10) :x))) {CPUTENSOR[float] :shape (10 10) :named ChainTMP35895 :vec-state [maybe-not-computed] <<Not-Embodied (10 10) Tensor>> :facet :input :requires-grad NIL :backward <Node: ADDNODE-CPUTENSOR (A[~] B[~] -> A[~])>} REPL: > (with-build (fw bw vars params) out (embody-input vars :x (randn `(10 10))) ;; :X = (randn `(10 10)) (funcall fw)) {CPUTENSOR[float] :shape (10 10) :named ChainTMP35884 ((-2.1486177 1.4877725 -1.7822108 ~ 0.30888113 -3.668074 -1.4501324) (0.90827906 -3.6974688 -0.7262471 ~ 2.153652 0.7110309 1.2819712) ... (-2.6074939 0.04147309 -0.97653854 ~ 0.3843904 -0.20308924 -0.614793) (1.7244194 1.5219165 0.3820825 ~ -0.41161555 0.5861892 0.18113303)) :facet :input :requires-grad NIL :backward NIL}","title":"Example"},{"location":"generic-tensor/#build","text":"Return: (values forward backward variables parameters)","title":"build"},{"location":"generic-tensor/#example_3","text":"REPL: > (setq out (!add (randn `(10 10)) (make-input `(a 10) :X))) {CPUTENSOR[float] :shape (10 10) :named ChainTMP35924 :vec-state [maybe-not-computed] <<Not-Embodied (10 10) Tensor>> :facet :input :requires-grad NIL :backward <Node: ADDNODE-CPUTENSOR (A[~] B[~] -> A[~])>} REPL: > (multiple-value-list (build out)) (#<FUNCTION (LAMBDA () :IN \"/private/var/tmp/slimemU2Krr.fasl\") {53A5B54B}> #<FUNCTION (LAMBDA () :IN \"/private/var/tmp/slimemU2Krr.fasl\") {53A8776B}> += [Computation Node Information] =======+ Subscripts: [A -> ?] Variables NAMES | SIZE | \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013 X | (A 10) | - The number of tmp variables: 4 +========================================+ #S(NODEPARAMETERS :PARAMETERS (omitted) :ntensors 3))","title":"Example"},{"location":"generic-tensor/#tensor-vec","text":"(tensor-vec tensor) Accessing the pointer/array the tensor has. Not until tensor-vec is called, the new area isn't allocated.","title":"tensor-vec"},{"location":"generic-tensor/#mref","text":"(mref tensor &rest subscripts) Read-only. Only used for printing the tensor. Whether you cares about performance or not, this function shouldn't be used ignoring for printing tensors.","title":"mref"},{"location":"generic-tensor/#vref","text":"(vref tensor index) vref is a generic-function to access tensor's vec. Whether you cares about performance or not, this function shouldn't be used ignoring for printing tensors. If you've created a new backend with having different ptr-type (can't be accessed by aref), only you have to do is to redefine vref.","title":"vref"},{"location":"generic-tensor/#set-save-for-backward","text":"NIL","title":"set-save-for-backward"},{"location":"generic-tensor/#read-save-for-backward","text":"NIL","title":"read-save-for-backward"},{"location":"generic-tensor/#no-grad","text":"[parameter] *no-grad* If t, all operations don't create gradients.","title":"*no-grad*"},{"location":"generic-tensor/#with-no-grad","text":"","title":"with-no-grad"},{"location":"generic-tensor/#macro-with-no-grad","text":"(with-no-grad &body body) Set *np-grad* t under the body execution, no gradients are made for backward.","title":"[macro] with-no-grad"},{"location":"generic-tensor/#parameter","text":"The function parameter computes all the previous nodes of the given tensor, returning the new tensor with requires-grad=t. Example: (parameter (randn `(3 3)))","title":"parameter"},{"location":"generic-tensor/#dtype-lisp-type","text":"NIL","title":"dtype-&gt;lisp-type"},{"location":"generic-tensor/#call-with-view","text":"NIL","title":"call-with-view"},{"location":"generic-tensor/#stride-of","text":"NIL","title":"stride-of"},{"location":"generic-tensor/#size-of","text":"NIL","title":"size-of"},{"location":"generic-tensor/#offset-of","text":"NIL","title":"offset-of"},{"location":"generic-tensor/#shape-equal","text":"","title":"shape-equal"},{"location":"generic-tensor/#function-shape-equal","text":"a=1, b=k => T a=1, b=2 => NIL ...","title":"[function] shape-equal"},{"location":"generic-tensor/#force-list","text":"Returns subscript-t if view is Subscript otherwise returns a view","title":"force-list"},{"location":"nodes/","text":"Formulate Neural Networks The package :cl-waffe2/vm.nodes provides a fundamental system for building neural networks. This package can be divided into three main parts. Shaping APIs defnode (Differentiable Operations) defmodel (Operations consisted of defnode) Note that there's a clear distinction between node and model. defnode => called with `forward` defmodel => called with `call` Also, defnode is a fundamental unit of operation, while defmodel is consisted of a set of nodes. Shaping APIs Introducing Subscript DSL I assume you have already seen defnode macro. This macro takes a strange syntax language after :where keyword. (defnode (TransposeNode (myself) :where (A[~ i j] -> A[~ j i]) ...)) (defnode (ScalarAdd (myself) :where (A[~] Scal[scal] -> A[~] where scal = 1) ...)) (defnode (ReshapeNode (myself tensor after &aux (before (shape tensor))) :where (A[before] -> A[after]) ...)) This is a DSL (Domain Specific Language) called Subscript DSL , which is used to notate the pointer and shape to be handled before and after the operation. For example, TransposeNode is said to be: Before and after the operation, we use the same pointer. A is a tensor with more than two dimensions, and after the operation, transposed the last two axes. (i.e.: A=(10 5 2), (10 2 5) is returned) ScalarAdd is said to be: The first argument A can be anything. The second argument Scal is a scalar tensor. The returned tensor shares the pointer with the given A . ReshapeNode is: Before and after the operation, pointers are common. The shape of A will be transformed from before into after Basic Grammar Let's start with learning the grammar. One line code of Subscript DSL follows this format: [Before The Operation] -> [After The Operation] where [symbol = expression (Optional)] ... Note that: the pharse where [symbol = expression (Optional)] ... is Optional One Subscript DSL place can include one line of code. [Before The Operation] and [After The Operation] has the common grammar rule. Let <Arguments> be a grammar rule of [Before The Operation] and [After The Operation], <Arguments> can be defined as: <Arguments> ::= <Arguments> <Argument> <Argument> ::= <PointerName> [ <SubScripts> ] | NIL <PointerName> ::= Symbol // the same as CL's symbol. <SubScripts> ::= <Subscripts> <Subscript> <Subscript> ::= Symbol | NIL To put it bluntly, can be a sequence of: PointerName[SubScripts] // SubScripts can be one of: [A], [A B] [~ i j] etc... Assigned task A[a b] B[a b] -> B[a b] In the DSL above, A and B indicates the name of pointer, they're not needed to be defined in advance. On the other hand a and b inside [ ... ], indicates subscripts of A and B , DSL's assigned work is to inference these undetermined symbols from: local variables declared in defnode form or where pharse. Shape of the given inputs at runtime. If any, DSL compiles and display a report on Shape-Error before performing the operation. (!add (randn `(3 2)) (randn `(2 4))) ;; will produce... [cl-waffe] Shaping-Error: Couldn't step forward because of shape-error. The operation was : <Node: ADDNODE-CPUTENSOR (A[~] B[~] -> A[~])> Input(s) : ((3 2) (2 4)) Predicted Output(s) : ((3 2)) Here's a list of reports. 1. Couldn't idenfity ~: ~ is determined as 3 butgot: 2. Excepted ~ = (3 2), butgot: (2 4) Also, these reports could be helpful for you (calculated ignoring the first errors.) 2. Couldn't idenfity ~: ~ is determined as 2 butgot: 4. Excepted ~ = (3 2), butgot: (2 4) Determine Rules (defnode (ExampleNode (myself) :where (A[~ i j] B[~ j k] C[~ k i] -> C[~ k i]) ...)) Symbols used in subscripts has a two state: Determined (those that can say i=1, j=2!) Undetermined (those that cannot say i=1, j=2) Before doing (call (ExampleNode) ...) , we create a table which stores determined/undetermined symbols and corresponding values. [TABLE] ~ -> ? // Undetermined before runtime i -> ? // Undetermined before runtime j -> ? // Undetermined before runtime k -> ? // Undetermined before runtime The moment we do (call (ExampleNode) TensorA TensorB TensorC) , we will be able to inference the value of i j k from the shape of given TensorA, TensorB, and TensorC. For Example, Let TensorA be a 2x3x4 Matrix, then the table become: [TABLE] ~ -> 2 i -> 3 j -> 4 k -> ? Then continue to do the same thing for TensorB. Let TensorB be a 2x4x9 Matrix, then the table become: [TABLE] ~ -> 2 i -> 3 j -> 4 k -> 9 Last, applying this operation into TensorC, but what if I gave the wrong shape to TensorC? Let TensorC be a 999x999x999 Matrix. (Obviously this is wrong). [TABLE] ~ -> 2 // \u2260999 i -> 3 // \u2260999 j -> 4 // \u2260999 k -> 9 // \u2260999 All subscripts in the table do not match with 999, resuting in shape-error. In that case, we can try again the operation with giving the correct shape to TensorC. Let TensorC be 2x9x3 Matrix. [TABLE] ~ -> 2 // =2 i -> 3 // = 3 j -> 4 // k -> 9 // = 9 All subscripts passed! (puts error If there's still undetermined symbol.) Using the determined table, we can also inference the shape of output tensor. The returned tensor is the shape of (~ k i) , that is, (2 9 3) . This operation can be done in a chain of lazy-evaluated nodes. Now, moving on to another topic, subscripts can be one of them. [TABLE] a = 1 // Fixnum b = `(1 2) // List consisted of fixnum ~ = `(1 2 3) // ~ is a special symbol which represents batched-input. DSL flattens the list in the subscript. (e.g.: b=(1 2) in A[b] is the equivalent to A[1 2] ) Note that ~ is a reserved word and has a special rule: ~ is used to express dimensions from 0 to N ~ can only be used once for one input of subscript. In tables, ~ is interpreted as one of: NIL or List In addition, ~ has a three behaviour: If ~ never appears in [Before The Operation] and [After The Operation] parts, the length of ~ could be Any. If ~ appears more than once, the length of ~ and content should be common. If ~ appears only in [After The Operation], returns error because we can't determine ~. In conclusion, I believe introducing Subscript DSL produces two benefits: Rigorous Shape Inspection in all operations with small code, and produce better Shape-Error (Initially I'm inspired in: nalgebra ). JIT Compiler can use a shape of given arguments in advance. (If only CL has a const-generics like Rust, Subscript DSL isn't needed anymore!). Where Pharse With where pharse, you can put local variables like: ;; Syntax is that: Symbol-Name = Expression A[i] B[j] -> C[k] where i = (1+ (random 1)) j = (1+ (random 1)) k = (1+ (random 1)) API: create-subscript-p (create-subscript-p subscripts &key macroexpand fixed return-body) Inputs: macroexpand[Boolean] If t, displays the generated program. fixed[Boolean] If t, ~ is ignored. return-body[Boolean] If t, the returned is S-exp. Outputs: (values compiled-function To-Refer-Pointer-Idx Broadcastable_List) Example: (TODO) defnode (defnode ((abstract-name (self &rest constructor-arguments) &key (where t) (out-scalar-p nil) (slots nil) (backward nil) (documentation \"\")) &body constructor-body)) defnode is a macro which is used to define a subclass of AbstractNode . The defined class is named after abstract-name , which has: Subscript DSL Slots that are shared at forward/backward time. Generic definition of backward Inputs abstract-name the class is named after it where the place to put Subscript DSL backward the general definition of backward (Optional). Place S-expression here If you wanna ignore define-impl's backward, otherwise define-impl's one is used. documentation docstring out-scalar-p Set t If the returned tensor is ScalarTensor. This can be dynamically modified via the accessor (out-scalar-p self) . Effects Defines a class named abstract-name Defines a function which is used to initialize the node named abstract-name Useful Tips In order to simplify parameter initialisation, if the keyword name of the :initarg is the same as the keyword name of the argument, the initialisation code is automatically generated. (defnode (ExampleNode (self arg) :slots ((arg :initarg :arg)))) (slot-value (ExampleNode 10) 'arg) ;; => 10 How and When to define backward? The backward follows this format: ((self dout dx dy ... dn) (values dx.grad dy.grad ... dn.grad)) dout is a previous node's gradient, and dx dy ... dn is a variables that used when forward. No guarantee that dx dy ... dn isn't being destructed due to in-place operation. If you need them in order to compute gradients, set :save-for-backward (t t ... t) at define-impl macro. Find the partial derivative of each variable according to the derivative of the composite function. The definition of backward must be placed either of defnode or define-impl. Basically, if the original defnode describes the backward, define-impl's backward is ignored. 1. ================================================================= AddNode (defnode) <- Place Backward | |-> (AddNode :CPUTensor) (define-impl) |-> (AddNode :LispTensor) (define-impl) |-> (AddNode :CUDATensor) (define-impl) ================================================================= 2. ================================================================= AddNode (defnode) <- Backward=nil | |-> (AddNode :CPUTensor) (define-impl) <- place backward |-> (AddNode :LispTensor) (define-impl) <- place backward |-> (AddNode :CUDATensor) (define-impl) <- place backward ================================================================= Depending on *using-backend* , the implementation to use is determined at node-building time. See also: with-devices. define-impl (define-impl ((abstract-name &key (device t) (reject-p nil)) &key save-for-backward forward backward) Defines a implementation of AbstractNode of device . Inputs device Set here symbol the impl working on. The symbol must be a subclass of AbstractTensor . If t, the impl has the highest priority assigned to all implementations. reject-p[null or predicate] Set here predicator, If the predicator is t, the implementation refures to be dispatched. save-for-backward The corresponding variable which is t will be made a copy when forward. (e.g.: forward=(x y) and save-for-backward=(t nil) , x is copied, y isn't copied.) forward Place the expanded lisp-code for forward propagation. backward Place the definition of backward as the same forward of defnode does. Tips: reject-p One of the practical usage of reject-p is to restrict dtypes that implementation can handle. reject-p takes an function: #'(lambda (&rest inputs) ...) where inputs is constructor-arguments in defnode. (e.g.: (AddNode :float) -> inputs=(list :float) ). AddNode for CPUTensor only supports dense matrix. (define-impl (AddNode :device CPUTensor :reject-p (supported-dtypes-are 0 :float :double)) :forward ((self x y) `(,@(expand-axpy-form x y) ,x))) The macro supported-dtypes-are returns an predicator which returns nil if the first argument is the equivalent to :float or :double . forward/backward forward/backward is given as: ((self &rest arguments) body) forward (forward node &rest inputs) Step forward of the given node , node is a subclass of AbstractNode . Note that forward can't handle with Composite . defmodel (defmodel ((name (self-name &rest constructor-arguments) &key (slots nil) (initargs) (on-call-> nil) (documentation \"\")) &body constructor-body) defmodel is a macro used to describe the model of neural network with Composite class. Effects defines a class named name defines a function named name with the constructor-arguments and constructor-body. Inputs name[Symbol] All models, and constructors for the model, are named after it. (self-name &rest constructor-arguments) The constructor function is defined as: (defun ,name (self-name ,@constructor-arguments) ...) slots ((slot-option1) (slot-option2) ...) Parameters of the inherited Composite class. It has the same syntax as defclass slots initargs (:accessor-name1 accessor-init-form1 :accessor-name2 accessor-init-form2 ... Unlike CL's structure, classes are tend to rebundant when writing the process of initializing slots. To make this simple, this argument was introduced. It works like a structure's constructor! documentation[String] on-call-> [One of: nil symbol-name function list] on-call-> is used to control the behaviour of call function. on-print-object [null or body] Example (defmodel (ExampleLayer (self features) ;; Options/Utils Here, :slots ((param :initarg :param)) :initargs (:param (make-tensor `(,features) :requires-grad t)) :documentation \"ExampleLayer is a ...\") ;; After make-instance is called, the form below is called. ;; make-instance -> make-instance :after -> this form. (print self) ;; <- Initialized ExampleLayer (print features) ;; <- constructor-arguments are also used here. (print \"ExampleLayer is created!\")) ;; The model you created, works like: (let ((layer (ExampleLayer 10))) (call layer ...)) Describe Forward Propagation The option on-call-> can control the behaviour of call function. on-call-> could be one of these case: First case, on-call-> is nil: cl-waffe2 calls the call function when doing forward propagation of the model. Second case, on-call-> is symbol-name: cl-waffe2 calls the specified function at on-call-> parameter, when doing forward propagation of the model. symbol-name could be also one of: method's name function's name. For example, set :on-call-> = call-example-layer which defined as: (defmethod call-example-layer ((model ExampleLayer) x y) (print \"call-example-layer is used!\") ...) (call (ExampleLayer 10) tensor) ;; call-example-layer is used! (Complex model assignments like ConvND, for example, can be achieved by assigning generic function names to symbols.) [Third case] on-call-> is function (i.e.: lambda): cl-waffe2 calls the given lambda function as a forward propagation. [Fourth case] on-call-> is a list: The List, should be this format. ((arguments) body) This argument is expanded into #'(lambda ,@on-call->) and works as well as 3. call All models in cl-waffe2, should implement this generic function. This generic function returns the computation node of the forward propagation of the model. The generic function call is also used to step forward of AbstractNode, that is, works as if forward. [generic-function] (call model &rest inputs) with-devices The macro with-devices declares the node's priority for the function forward to be used. Input: - backend-priority An list of device's name (e.g.: CPUTensor, LispTensor...) Devices on the left have higher priority. Example: Let ATensor and BTensor be compatible (i.e.: pointers are the same type), and subclass of AbstractNode, and all the operations they have are as follows: ATensor has !add. BTensor has !mul. This code works: (setq a (make-tensor `(10 10))) ;; The tensor a is ATensor. ;; (Priority1=ATensor Priority2=BTensor) (with-devices (ATensor BTensor) (!add a (!mul a a))) ATensor doesn't have any implementation of !mul, but it does work. This is because cl-waffe2's compatible backend system. cl-waffe2's backend dispatching rule is following: If the priority 1 backend does not have an implementation of the specified operation, check if the priority 2 backend does, if it still does not have it, 3, 4... and so on. The order of priority would be `(,@backend-priority ScalarTensor t). (t is a special name, and it implys the implement works for all the backends.) Example (with-devices (LispTensor CPUTensor) (!add a b)) Composite [class] Composite Composite is a fundamental datatype for all neural network models. The name composite is so named because it is used to bundle computation nodes constructed by defnode. In cl-waffe2, All models should be a subtype of this class, and shall return a forward propagation computation node using the call function. In order to define your model with Composite, two methods are available. Extend Composite Class (Slightly Complicated) First, define your class with extending Composite Class. (defclass LinearModel (Composite) ((weight ...) ; <- set parameters here. (bias ...)) Second, define forwarrd step with overriding call method. (defmethod call ((model LinearModel) &rest inputs) ... ) It should work like: (call (make-instance 'LinearModel in-features out-features) args1 ...) Using defmodel macro The defmodel macro simplifies the above redundant notation and also solves the problem that call can only use &rest as an argument. Therefore, I'm depcrecated with the method above, instead, use defmacro. For detailed usage, see the documentation of defmacro. AbstractNode [class] AbstractNode The class AbstractNode is a fundamental object of describing computation nodes in cl-waffe. AbstractNode must possess following: Transimission State Slots (for passing forward/backward) Variables (for building computation nodes) with-instant-kernel Creates an instant-kernel following tensor. This macro is used to embed condition-free Lisp code either in the process of creating a node or after it has been compiled. Use case: Embedding Lisp Code for building-time. (setq a (randn `(10 10))) (with-instant-kernel a (print a)) ;; -> (print a) is evaluated Embedding Lisp Code for compile-time. (setq a (randn `(10 10))) (with-instant-kernel a `(print ,a)) ;; -> (print a) isn't evaluated (funcall (build *)) ;; -> (print a) will be evaluated. Note that (equal (with-instant-kernel a) a) is NIL, that is, the returned value of this macro must be followed by a calculation node. If the return value of Body can be expanded as a macro, the values are compiled together at JIT compile time. Otherwise, the given tensor is returned as is. declare-local-variables TODO","title":"cl-waffe2/vm.nodes"},{"location":"nodes/#formulate-neural-networks","text":"The package :cl-waffe2/vm.nodes provides a fundamental system for building neural networks. This package can be divided into three main parts. Shaping APIs defnode (Differentiable Operations) defmodel (Operations consisted of defnode) Note that there's a clear distinction between node and model. defnode => called with `forward` defmodel => called with `call` Also, defnode is a fundamental unit of operation, while defmodel is consisted of a set of nodes.","title":"Formulate Neural Networks"},{"location":"nodes/#shaping-apis","text":"","title":"Shaping APIs"},{"location":"nodes/#introducing-subscript-dsl","text":"I assume you have already seen defnode macro. This macro takes a strange syntax language after :where keyword. (defnode (TransposeNode (myself) :where (A[~ i j] -> A[~ j i]) ...)) (defnode (ScalarAdd (myself) :where (A[~] Scal[scal] -> A[~] where scal = 1) ...)) (defnode (ReshapeNode (myself tensor after &aux (before (shape tensor))) :where (A[before] -> A[after]) ...)) This is a DSL (Domain Specific Language) called Subscript DSL , which is used to notate the pointer and shape to be handled before and after the operation. For example, TransposeNode is said to be: Before and after the operation, we use the same pointer. A is a tensor with more than two dimensions, and after the operation, transposed the last two axes. (i.e.: A=(10 5 2), (10 2 5) is returned) ScalarAdd is said to be: The first argument A can be anything. The second argument Scal is a scalar tensor. The returned tensor shares the pointer with the given A . ReshapeNode is: Before and after the operation, pointers are common. The shape of A will be transformed from before into after","title":"Introducing Subscript DSL"},{"location":"nodes/#basic-grammar","text":"Let's start with learning the grammar. One line code of Subscript DSL follows this format: [Before The Operation] -> [After The Operation] where [symbol = expression (Optional)] ... Note that: the pharse where [symbol = expression (Optional)] ... is Optional One Subscript DSL place can include one line of code. [Before The Operation] and [After The Operation] has the common grammar rule. Let <Arguments> be a grammar rule of [Before The Operation] and [After The Operation], <Arguments> can be defined as: <Arguments> ::= <Arguments> <Argument> <Argument> ::= <PointerName> [ <SubScripts> ] | NIL <PointerName> ::= Symbol // the same as CL's symbol. <SubScripts> ::= <Subscripts> <Subscript> <Subscript> ::= Symbol | NIL To put it bluntly, can be a sequence of: PointerName[SubScripts] // SubScripts can be one of: [A], [A B] [~ i j] etc...","title":"Basic Grammar"},{"location":"nodes/#assigned-task","text":"A[a b] B[a b] -> B[a b] In the DSL above, A and B indicates the name of pointer, they're not needed to be defined in advance. On the other hand a and b inside [ ... ], indicates subscripts of A and B , DSL's assigned work is to inference these undetermined symbols from: local variables declared in defnode form or where pharse. Shape of the given inputs at runtime. If any, DSL compiles and display a report on Shape-Error before performing the operation. (!add (randn `(3 2)) (randn `(2 4))) ;; will produce... [cl-waffe] Shaping-Error: Couldn't step forward because of shape-error. The operation was : <Node: ADDNODE-CPUTENSOR (A[~] B[~] -> A[~])> Input(s) : ((3 2) (2 4)) Predicted Output(s) : ((3 2)) Here's a list of reports. 1. Couldn't idenfity ~: ~ is determined as 3 butgot: 2. Excepted ~ = (3 2), butgot: (2 4) Also, these reports could be helpful for you (calculated ignoring the first errors.) 2. Couldn't idenfity ~: ~ is determined as 2 butgot: 4. Excepted ~ = (3 2), butgot: (2 4)","title":"Assigned task"},{"location":"nodes/#determine-rules","text":"(defnode (ExampleNode (myself) :where (A[~ i j] B[~ j k] C[~ k i] -> C[~ k i]) ...)) Symbols used in subscripts has a two state: Determined (those that can say i=1, j=2!) Undetermined (those that cannot say i=1, j=2) Before doing (call (ExampleNode) ...) , we create a table which stores determined/undetermined symbols and corresponding values. [TABLE] ~ -> ? // Undetermined before runtime i -> ? // Undetermined before runtime j -> ? // Undetermined before runtime k -> ? // Undetermined before runtime The moment we do (call (ExampleNode) TensorA TensorB TensorC) , we will be able to inference the value of i j k from the shape of given TensorA, TensorB, and TensorC. For Example, Let TensorA be a 2x3x4 Matrix, then the table become: [TABLE] ~ -> 2 i -> 3 j -> 4 k -> ? Then continue to do the same thing for TensorB. Let TensorB be a 2x4x9 Matrix, then the table become: [TABLE] ~ -> 2 i -> 3 j -> 4 k -> 9 Last, applying this operation into TensorC, but what if I gave the wrong shape to TensorC? Let TensorC be a 999x999x999 Matrix. (Obviously this is wrong). [TABLE] ~ -> 2 // \u2260999 i -> 3 // \u2260999 j -> 4 // \u2260999 k -> 9 // \u2260999 All subscripts in the table do not match with 999, resuting in shape-error. In that case, we can try again the operation with giving the correct shape to TensorC. Let TensorC be 2x9x3 Matrix. [TABLE] ~ -> 2 // =2 i -> 3 // = 3 j -> 4 // k -> 9 // = 9 All subscripts passed! (puts error If there's still undetermined symbol.) Using the determined table, we can also inference the shape of output tensor. The returned tensor is the shape of (~ k i) , that is, (2 9 3) . This operation can be done in a chain of lazy-evaluated nodes. Now, moving on to another topic, subscripts can be one of them. [TABLE] a = 1 // Fixnum b = `(1 2) // List consisted of fixnum ~ = `(1 2 3) // ~ is a special symbol which represents batched-input. DSL flattens the list in the subscript. (e.g.: b=(1 2) in A[b] is the equivalent to A[1 2] ) Note that ~ is a reserved word and has a special rule: ~ is used to express dimensions from 0 to N ~ can only be used once for one input of subscript. In tables, ~ is interpreted as one of: NIL or List In addition, ~ has a three behaviour: If ~ never appears in [Before The Operation] and [After The Operation] parts, the length of ~ could be Any. If ~ appears more than once, the length of ~ and content should be common. If ~ appears only in [After The Operation], returns error because we can't determine ~. In conclusion, I believe introducing Subscript DSL produces two benefits: Rigorous Shape Inspection in all operations with small code, and produce better Shape-Error (Initially I'm inspired in: nalgebra ). JIT Compiler can use a shape of given arguments in advance. (If only CL has a const-generics like Rust, Subscript DSL isn't needed anymore!).","title":"Determine Rules"},{"location":"nodes/#where-pharse","text":"With where pharse, you can put local variables like: ;; Syntax is that: Symbol-Name = Expression A[i] B[j] -> C[k] where i = (1+ (random 1)) j = (1+ (random 1)) k = (1+ (random 1))","title":"Where Pharse"},{"location":"nodes/#api-create-subscript-p","text":"(create-subscript-p subscripts &key macroexpand fixed return-body) Inputs: macroexpand[Boolean] If t, displays the generated program. fixed[Boolean] If t, ~ is ignored. return-body[Boolean] If t, the returned is S-exp. Outputs: (values compiled-function To-Refer-Pointer-Idx Broadcastable_List) Example: (TODO)","title":"API: create-subscript-p"},{"location":"nodes/#defnode","text":"(defnode ((abstract-name (self &rest constructor-arguments) &key (where t) (out-scalar-p nil) (slots nil) (backward nil) (documentation \"\")) &body constructor-body)) defnode is a macro which is used to define a subclass of AbstractNode . The defined class is named after abstract-name , which has: Subscript DSL Slots that are shared at forward/backward time. Generic definition of backward","title":"defnode"},{"location":"nodes/#inputs","text":"abstract-name the class is named after it where the place to put Subscript DSL backward the general definition of backward (Optional). Place S-expression here If you wanna ignore define-impl's backward, otherwise define-impl's one is used. documentation docstring out-scalar-p Set t If the returned tensor is ScalarTensor. This can be dynamically modified via the accessor (out-scalar-p self) .","title":"Inputs"},{"location":"nodes/#effects","text":"Defines a class named abstract-name Defines a function which is used to initialize the node named abstract-name","title":"Effects"},{"location":"nodes/#useful-tips","text":"In order to simplify parameter initialisation, if the keyword name of the :initarg is the same as the keyword name of the argument, the initialisation code is automatically generated. (defnode (ExampleNode (self arg) :slots ((arg :initarg :arg)))) (slot-value (ExampleNode 10) 'arg) ;; => 10","title":"Useful Tips"},{"location":"nodes/#how-and-when-to-define-backward","text":"The backward follows this format: ((self dout dx dy ... dn) (values dx.grad dy.grad ... dn.grad)) dout is a previous node's gradient, and dx dy ... dn is a variables that used when forward. No guarantee that dx dy ... dn isn't being destructed due to in-place operation. If you need them in order to compute gradients, set :save-for-backward (t t ... t) at define-impl macro. Find the partial derivative of each variable according to the derivative of the composite function. The definition of backward must be placed either of defnode or define-impl. Basically, if the original defnode describes the backward, define-impl's backward is ignored. 1. ================================================================= AddNode (defnode) <- Place Backward | |-> (AddNode :CPUTensor) (define-impl) |-> (AddNode :LispTensor) (define-impl) |-> (AddNode :CUDATensor) (define-impl) ================================================================= 2. ================================================================= AddNode (defnode) <- Backward=nil | |-> (AddNode :CPUTensor) (define-impl) <- place backward |-> (AddNode :LispTensor) (define-impl) <- place backward |-> (AddNode :CUDATensor) (define-impl) <- place backward ================================================================= Depending on *using-backend* , the implementation to use is determined at node-building time. See also: with-devices.","title":"How and When to define backward?"},{"location":"nodes/#define-impl","text":"(define-impl ((abstract-name &key (device t) (reject-p nil)) &key save-for-backward forward backward) Defines a implementation of AbstractNode of device .","title":"define-impl"},{"location":"nodes/#inputs_1","text":"device Set here symbol the impl working on. The symbol must be a subclass of AbstractTensor . If t, the impl has the highest priority assigned to all implementations. reject-p[null or predicate] Set here predicator, If the predicator is t, the implementation refures to be dispatched. save-for-backward The corresponding variable which is t will be made a copy when forward. (e.g.: forward=(x y) and save-for-backward=(t nil) , x is copied, y isn't copied.) forward Place the expanded lisp-code for forward propagation. backward Place the definition of backward as the same forward of defnode does.","title":"Inputs"},{"location":"nodes/#tips-reject-p","text":"One of the practical usage of reject-p is to restrict dtypes that implementation can handle. reject-p takes an function: #'(lambda (&rest inputs) ...) where inputs is constructor-arguments in defnode. (e.g.: (AddNode :float) -> inputs=(list :float) ). AddNode for CPUTensor only supports dense matrix. (define-impl (AddNode :device CPUTensor :reject-p (supported-dtypes-are 0 :float :double)) :forward ((self x y) `(,@(expand-axpy-form x y) ,x))) The macro supported-dtypes-are returns an predicator which returns nil if the first argument is the equivalent to :float or :double .","title":"Tips: reject-p"},{"location":"nodes/#forwardbackward","text":"forward/backward is given as: ((self &rest arguments) body)","title":"forward/backward"},{"location":"nodes/#forward","text":"(forward node &rest inputs) Step forward of the given node , node is a subclass of AbstractNode . Note that forward can't handle with Composite .","title":"forward"},{"location":"nodes/#defmodel","text":"(defmodel ((name (self-name &rest constructor-arguments) &key (slots nil) (initargs) (on-call-> nil) (documentation \"\")) &body constructor-body) defmodel is a macro used to describe the model of neural network with Composite class.","title":"defmodel"},{"location":"nodes/#effects_1","text":"defines a class named name defines a function named name with the constructor-arguments and constructor-body.","title":"Effects"},{"location":"nodes/#inputs_2","text":"name[Symbol] All models, and constructors for the model, are named after it. (self-name &rest constructor-arguments) The constructor function is defined as: (defun ,name (self-name ,@constructor-arguments) ...) slots ((slot-option1) (slot-option2) ...) Parameters of the inherited Composite class. It has the same syntax as defclass slots initargs (:accessor-name1 accessor-init-form1 :accessor-name2 accessor-init-form2 ... Unlike CL's structure, classes are tend to rebundant when writing the process of initializing slots. To make this simple, this argument was introduced. It works like a structure's constructor! documentation[String] on-call-> [One of: nil symbol-name function list] on-call-> is used to control the behaviour of call function. on-print-object [null or body]","title":"Inputs"},{"location":"nodes/#example","text":"(defmodel (ExampleLayer (self features) ;; Options/Utils Here, :slots ((param :initarg :param)) :initargs (:param (make-tensor `(,features) :requires-grad t)) :documentation \"ExampleLayer is a ...\") ;; After make-instance is called, the form below is called. ;; make-instance -> make-instance :after -> this form. (print self) ;; <- Initialized ExampleLayer (print features) ;; <- constructor-arguments are also used here. (print \"ExampleLayer is created!\")) ;; The model you created, works like: (let ((layer (ExampleLayer 10))) (call layer ...))","title":"Example"},{"location":"nodes/#describe-forward-propagation","text":"The option on-call-> can control the behaviour of call function. on-call-> could be one of these case: First case, on-call-> is nil: cl-waffe2 calls the call function when doing forward propagation of the model. Second case, on-call-> is symbol-name: cl-waffe2 calls the specified function at on-call-> parameter, when doing forward propagation of the model. symbol-name could be also one of: method's name function's name. For example, set :on-call-> = call-example-layer which defined as: (defmethod call-example-layer ((model ExampleLayer) x y) (print \"call-example-layer is used!\") ...) (call (ExampleLayer 10) tensor) ;; call-example-layer is used! (Complex model assignments like ConvND, for example, can be achieved by assigning generic function names to symbols.) [Third case] on-call-> is function (i.e.: lambda): cl-waffe2 calls the given lambda function as a forward propagation. [Fourth case] on-call-> is a list: The List, should be this format. ((arguments) body) This argument is expanded into #'(lambda ,@on-call->) and works as well as 3.","title":"Describe Forward Propagation"},{"location":"nodes/#call","text":"All models in cl-waffe2, should implement this generic function. This generic function returns the computation node of the forward propagation of the model. The generic function call is also used to step forward of AbstractNode, that is, works as if forward. [generic-function] (call model &rest inputs)","title":"call"},{"location":"nodes/#with-devices","text":"The macro with-devices declares the node's priority for the function forward to be used. Input: - backend-priority An list of device's name (e.g.: CPUTensor, LispTensor...) Devices on the left have higher priority. Example: Let ATensor and BTensor be compatible (i.e.: pointers are the same type), and subclass of AbstractNode, and all the operations they have are as follows: ATensor has !add. BTensor has !mul. This code works: (setq a (make-tensor `(10 10))) ;; The tensor a is ATensor. ;; (Priority1=ATensor Priority2=BTensor) (with-devices (ATensor BTensor) (!add a (!mul a a))) ATensor doesn't have any implementation of !mul, but it does work. This is because cl-waffe2's compatible backend system. cl-waffe2's backend dispatching rule is following: If the priority 1 backend does not have an implementation of the specified operation, check if the priority 2 backend does, if it still does not have it, 3, 4... and so on. The order of priority would be `(,@backend-priority ScalarTensor t). (t is a special name, and it implys the implement works for all the backends.)","title":"with-devices"},{"location":"nodes/#example_1","text":"(with-devices (LispTensor CPUTensor) (!add a b))","title":"Example"},{"location":"nodes/#composite","text":"[class] Composite Composite is a fundamental datatype for all neural network models. The name composite is so named because it is used to bundle computation nodes constructed by defnode. In cl-waffe2, All models should be a subtype of this class, and shall return a forward propagation computation node using the call function. In order to define your model with Composite, two methods are available.","title":"Composite"},{"location":"nodes/#extend-composite-class-slightly-complicated","text":"First, define your class with extending Composite Class. (defclass LinearModel (Composite) ((weight ...) ; <- set parameters here. (bias ...)) Second, define forwarrd step with overriding call method. (defmethod call ((model LinearModel) &rest inputs) ... ) It should work like: (call (make-instance 'LinearModel in-features out-features) args1 ...)","title":"Extend Composite Class (Slightly Complicated)"},{"location":"nodes/#using-defmodel-macro","text":"The defmodel macro simplifies the above redundant notation and also solves the problem that call can only use &rest as an argument. Therefore, I'm depcrecated with the method above, instead, use defmacro. For detailed usage, see the documentation of defmacro.","title":"Using defmodel macro"},{"location":"nodes/#abstractnode","text":"[class] AbstractNode The class AbstractNode is a fundamental object of describing computation nodes in cl-waffe. AbstractNode must possess following: Transimission State Slots (for passing forward/backward) Variables (for building computation nodes)","title":"AbstractNode"},{"location":"nodes/#with-instant-kernel","text":"Creates an instant-kernel following tensor. This macro is used to embed condition-free Lisp code either in the process of creating a node or after it has been compiled. Use case:","title":"with-instant-kernel"},{"location":"nodes/#embedding-lisp-code-for-building-time","text":"(setq a (randn `(10 10))) (with-instant-kernel a (print a)) ;; -> (print a) is evaluated","title":"Embedding Lisp Code for building-time."},{"location":"nodes/#embedding-lisp-code-for-compile-time","text":"(setq a (randn `(10 10))) (with-instant-kernel a `(print ,a)) ;; -> (print a) isn't evaluated (funcall (build *)) ;; -> (print a) will be evaluated. Note that (equal (with-instant-kernel a) a) is NIL, that is, the returned value of this macro must be followed by a calculation node. If the return value of Body can be expanded as a macro, the values are compiled together at JIT compile time. Otherwise, the given tensor is returned as is.","title":"Embedding Lisp Code for compile-time."},{"location":"nodes/#declare-local-variables","text":"TODO","title":"declare-local-variables"},{"location":"overview/","text":".katex img { object-fit: fill; padding: unset; display: block; position: absolute; width: 100%; height: inherit; } Concepts Project Structure cl-waffe2 consists of the following systems. Fundamental System :cl-waffe2/vm.nodes :cl-waffe2/vm.generic-tensor :cl-waffe2/vm.nodes provides a system on constructing networks (e.g.: AbstractNode and defnode , Composite and defmodel etc...), and management to shape (e.g.: :where pharse , subscript etc...). :cl-waffe2/vm.generic-tensor provides on the other hand a system on differentiable AbstractTensor (e.g.: build make-tensor make-input etc...) All other packages are built on this package. Standard APIs :cl-waffe2/base-impl :cl-waffe2/base-impl provides a standard implementation for operations (e.g.: !add proceed !view etc...), including defnode definition and defun parts. Standard Backends/Implemenetations :cl-waffe2/backends.lisp :cl-waffe2/backends.cpu Both of them are standard implementation of :cl-waffe2/base-impl for CPU. As of this writing (2023/06/20), cl-waffe2 has a only impls for CPU, however, If only time and money would permit, I'm willing to implement CUDA/Metal Backends. :cl-waffe2/backends.lisp is work enough first, it is Portable (based on ANSI Common Lisp) and supports AVX2 but far from full speed . On the other hand :cl-waffe2/backends.cpu is accelerated by OpenBLAS (maybe MKL is ok) and other foreign backends, this is SBCL-Dependant and sometimes could be unsafe, but provides full speed . :cl-waffe2/backends.fastmath (NOT IMPLEMENTED YET!) (TODO) Other Utils :cl-waffe2 ;; Provides multi-threading APIs and config macros! :cl-waffe2/nn ;; Provides Basic neural-network Implementations. :cl-waffe2/optimizers ;; Provides Basic Optimizers :cl-waffe2/viz ;; Provides Vizualizing APIs etc... To Get Started! It is recommended to load the system to be used when defining the package. Please pick up the packages that you need depending on your needs. For Example: (in-package :cl-user) (defpackage :your-project-name (:use :cl :cl-waffe2 :cl-waffe2/vm.generic-tensor :cl-waffe2/vm.nodes :cl-waffe2/base-impl :cl-waffe2/distributions :cl-waffe2/backends.lisp :cl-waffe2/backends.cpu :cl-waffe2/nn :cl-waffe2/optimizers :cl-waffe2/viz)) (in-package :your-project-name) ;; Your code follows... cl-waffe2 does not cause name clashes with other libraries or existing functions. In addition, with regard to the function that generates the node is ! , there is a rule that functions that create a node must start with ! (ignoring the exception of the proceed function). Differentiable operations based on multiple backends All operations in cl-waffe2 can be performed in the following form. [AbstractNode] | |--------------------|------------------------| [CPU Implementation1] [CPU Implementation2] [CUDA Implementation1] ... AbstractNode is a class declared via defnode macro, and each implementation is implemented via define-impl macro. There can be more than one implementation for a single device. (e.g.: it is possible to have a normal implementation and an approximate implementation for the exp function). One of the concepts is to minimise code re-writing by defining abstract nodes and switching the backends that executes them depending on the device they run on and the speed required. As an example, consider implementing the addition operation !add . The addition operation AddNode is the operation of finding the sum of two given matrices A and B and storing the result in A. (defnode (AddNode (myself) :where (A[~] B[~] -> A[~]) :documentation \"A <- A + B\" :backward ((self dout dx dy) (declare (ignore dx dy)) (values dout dout)) ;; Here follows constructor's body ;; AddNode class initialized is passed as *myself* ) Here, :where describes how matrices are performed. Before -> clause refers to the arguments, after -> clause refers to the shape of matrix after the operation. It describes: Takes A and B as arguments and returns a matrix of pointers of A All matrices have the same shape before and after the operation. :backward defines the operation of backward. This declaration can be made either in defnode or in define-impl , whichever you declare. The declared node can be initialized using the function (AddNode) , but seems returning errors. (AddNode) ;; -> Couldn't find any implementation of AddNode for (CPUTENSOR LISPTENSOR). This is because there is not yet a single implementation for AddNode , so let's define how AddNode works via define-impl macro. One operation can be defined for a backend that can be declared by inheriting from the cl-waffe2/vm.generic-tensor:AbstractTensor class. As of this writing(2023/06/20), cl-waffe2 provides following backends as standard. LispTensor - Portable/Safety First, speed comes second. Everything works on ANSI Common Lisp. CPUTensor - This is SBCL-dependant, but supports OpenBLAS linear-algebra APIs. Of course, if necessary, you can create a new backend. (defclass MyTensor (AbstractTensor) nil) (See also: tensor.lisp ) After defining a new backend, it is NOT necessary to give a re-implementation for all standard implementations in cl-waffe2. Select the appropriate backends in order of array compatibility. Anyway, this is how AddNode is defined for LispTensor in cl-waffe2. (define-impl (AddNode :device LispTensor) :forward ((self x y) (let ((adder (matrix-add (dtype x)))) `(,@(call-with-view #'(lambda (x-view y-view) `(funcall ,adder (tensor-vec ,x) (tensor-vec ,y) ,(offset-of x-view 0) ,(offset-of y-view 0) ,(size-of x-view 0) ,(stride-of x-view 0) ,(stride-of y-view 0))) `(,x ,y)) ,x)))) In :forward write the expansion expression for the operation in the same way as when defining a macro with defmacro . (see below for details). Why define-impl takes such a roundabout approach? To generate a fast code depending on the matrix size and data type at runtime. To pre-calculate all Indexes It is possible to generate, for example, C code without necessarily performing the same operations. (I believe that ideas on this macro needed to be given more thoughts, indeed, this is ugly...) Let's Perform operations with the defined AddNode! (forward (AddNode) (randn `(10 10)) (randn `(10 10))) {CPUTENSOR[float] :shape (10 10) :named ChainTMP9412 :vec-state [maybe-not-computed] ((-0.33475596 1.0127474 -0.060175765 ~ 1.4573603 -0.987001 -1.0165008) (-0.045512 -0.17995936 0.23593931 ~ 0.8409552 2.6434622 -0.5789532) ... (0.13282542 1.9386152 0.16213055 ~ 0.4363958 0.8294802 -0.1558509) (1.1732875 -1.5769591 -1.2152125 ~ -0.2833903 -0.81108683 0.9846606)) :facet :input :requires-grad NIL :backward <Node: ADDNODE-CPUTENSOR (A[~] B[~] -> A[~])>} Look at :vec-state, at that moment, the operation is still not done yet. The tensor displayed is the equivalent to the first argument. In cl-waffe2, all operations are lazy-evaluated, being JIT-compiled/Optimized/Parallelized via build , or proceed function. You would think that this style programming would make your task more complex, but don't worry, we provide APIs that is as close as possible to defined-by-run, and REPL-Friendly. (proceed (!add (AddNode) (randn `(10 10)) (randn `(10 10)))) ;; proceed-time function measures execution time without compiling time. (proceed-time (!add (AddNode) (randn `(10 10)) (randn `(10 10)))) Evaluation took: 0.000 seconds of real time 0.000014 seconds of total run time (0.000014 user, 0.000000 system) 100.00% CPU 30,512 processor cycles 0 bytes consed {CPUTENSOR[float] :shape (10 10) :named ChainTMP9447 :vec-state [computed] ((-1.5820543 2.2804832 -0.5613338 ~ 1.1143546 -1.3096298 -1.3756635) (-1.5208249 0.21621853 2.660368 ~ -1.032644 0.25917292 -1.9737494) ... (2.2557664 2.4791012 -0.04298857 ~ -1.2520232 1.8216541 -2.818116) (0.8615336 0.92017823 -0.25378937 ~ 0.9697968 -0.6300591 1.5660275)) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} You can switch backends via (with-devices (&rest devices) &body body) macro seamlessly. (with-devices (LispTensor CPUTensor) ;; The priority of dispatching backends is: LispTensor(First) -> CPUTensor(Second) ;; If there's no any impls on LispTensor, use CPUTensor instead. (!add (randn `(10 10)) (randn `(10 10)))) JIT compile, In-place optimizing As I said Everything is lazy-evaluated, and compiled , JIT Compiling is a one of main idea of this project. Mainly, this produces two benefits. Infinite number of Epochs, No Overheads of funcall. As all lisper know, there is a unignorable overhead when calling methods. (defmethod test-method ((a fixnum) (b fixnum)) (+ a b)) (defmethod test-method ((a single-float) (b single-float)) (+ a b)) (time (dotimes (i 100000000) (test-method 1.0 1.0))) Evaluation took: 0.560 seconds of real time 0.554936 seconds of total run time (0.551612 user, 0.003324 system) 99.11% CPU 1,291,693,656 processor cycles 0 bytes consed (defun test-fun (a b) (declare (type single-float a b)) (+ a b)) ;; Also, defun can be inlined at the end. (time (dotimes (i 100000000) (test-fun 1.0 1.0))) Evaluation took: 0.298 seconds of real time 0.297827 seconds of total run time (0.296968 user, 0.000859 system) 100.00% CPU 688,686,522 processor cycles 0 bytes consed In this project, which uses a large number of generic functions!, this overhead becomes non-negligible at every Epoch, especially when the matrix size is small. Therefore, we took the approach of defining a new function by cutting out the necessary operations from the lazy-evaluated nodes, part by part. ;; cl-waffe2's benchmark TODO: Update This section (let ((f (build (!sin 1.0)))) (time (dotimes (i 100000) (funcall f)))) ;; Fix: tensor-reset!'s overhead... (defun test-f (x) (sin (sin (sin (sin x))))) (time (dotimes (i 100000) (test-f 1.0))) In-place optimizing This is a usual function in cl-waffe2, which finds the sum of A and B. (!add a b) But internally, the operation makes a copy not to produce side effects. (forward (AddNode) (!copy a) b) Without making a copy, the value of A would be destructed instead of having to allocate extra memory. (let ((a (make-tensor `(3 3) :initial-element 1.0))) (print a) ;; {CPUTENSOR[float] :shape (3 3) ;; ((1.0 1.0 1.0) ;; (1.0 1.0 1.0) ;; (1.0 1.0 1.0)) ;; :facet :exist ;; :requires-grad NIL ;; :backward NIL} ;; (eval A <- A + B) (proceed (forward (AddNode :float) a (randn `(3 3)))) (print a) ;; {CPUTENSOR[float] :shape (3 3) ;; ((2.0100088 0.2906983 1.5334041) ;; (-0.50357413 2.389317 0.7051847) ;; (1.3005692 1.5925546 0.95498145)) ;; :facet :exist ;; :requires-grad NIL ;; :backward NIL} ) Operations that do not allocate extra space are called in-place (or sometimes destructive operations?). Making operations in-place is a rational way to optimize your programs, but this is a trade-off with readability, because the coding style is more like a programming notation than a mathematical notation. Let's take another example. o u t = f ( I n p u t ) + f ( f ( T e n s o r ) ) out = f(Input) + f(f(Tensor)) o u t = f ( I n p u t ) + f ( f ( T e n sor )) (TODO) (defnode (1DFunc (self) :where (A[~] -> A[~]))) (define-impl (1DFunc :device LispTensor) :forward ((self x) `(progn ,x)) :backward ((self dout dx) (values dout))) (defun f (tensor) (forward (1DFunc) (!copy tensor))) (let ((k (!add (make-input `(3 3) nil) (f (f (randn `(3 3) :requires-grad t)))))) (build k) (cl-waffe2/viz:viz-computation-node k \"assets/1d_fn_arg.dot\")) Before Optimized Vs After Optimized. (TODO) Broadcasting APIs, View APIs !flexible !view REPL-Friendly function, proceed Proceed Proceed-backward Proceed-time Strong Shaping APIs syntax of :where pharse Shape Error Reports Constructing networks with defnode/defmodel defnode defmodel call forward Data Structures Parameter/Tensor/Input/ScalarTensor Optimizing Parameters defoptimizer Tutorials Over! I'll keep my finger crossed.","title":"Tutorials"},{"location":"overview/#concepts","text":"","title":"Concepts"},{"location":"overview/#project-structure","text":"cl-waffe2 consists of the following systems.","title":"Project Structure"},{"location":"overview/#fundamental-system","text":":cl-waffe2/vm.nodes :cl-waffe2/vm.generic-tensor :cl-waffe2/vm.nodes provides a system on constructing networks (e.g.: AbstractNode and defnode , Composite and defmodel etc...), and management to shape (e.g.: :where pharse , subscript etc...). :cl-waffe2/vm.generic-tensor provides on the other hand a system on differentiable AbstractTensor (e.g.: build make-tensor make-input etc...) All other packages are built on this package.","title":"Fundamental System"},{"location":"overview/#standard-apis","text":":cl-waffe2/base-impl :cl-waffe2/base-impl provides a standard implementation for operations (e.g.: !add proceed !view etc...), including defnode definition and defun parts.","title":"Standard APIs"},{"location":"overview/#standard-backendsimplemenetations","text":":cl-waffe2/backends.lisp :cl-waffe2/backends.cpu Both of them are standard implementation of :cl-waffe2/base-impl for CPU. As of this writing (2023/06/20), cl-waffe2 has a only impls for CPU, however, If only time and money would permit, I'm willing to implement CUDA/Metal Backends. :cl-waffe2/backends.lisp is work enough first, it is Portable (based on ANSI Common Lisp) and supports AVX2 but far from full speed . On the other hand :cl-waffe2/backends.cpu is accelerated by OpenBLAS (maybe MKL is ok) and other foreign backends, this is SBCL-Dependant and sometimes could be unsafe, but provides full speed . :cl-waffe2/backends.fastmath (NOT IMPLEMENTED YET!) (TODO)","title":"Standard Backends/Implemenetations"},{"location":"overview/#other-utils","text":":cl-waffe2 ;; Provides multi-threading APIs and config macros! :cl-waffe2/nn ;; Provides Basic neural-network Implementations. :cl-waffe2/optimizers ;; Provides Basic Optimizers :cl-waffe2/viz ;; Provides Vizualizing APIs etc...","title":"Other Utils"},{"location":"overview/#to-get-started","text":"It is recommended to load the system to be used when defining the package. Please pick up the packages that you need depending on your needs. For Example: (in-package :cl-user) (defpackage :your-project-name (:use :cl :cl-waffe2 :cl-waffe2/vm.generic-tensor :cl-waffe2/vm.nodes :cl-waffe2/base-impl :cl-waffe2/distributions :cl-waffe2/backends.lisp :cl-waffe2/backends.cpu :cl-waffe2/nn :cl-waffe2/optimizers :cl-waffe2/viz)) (in-package :your-project-name) ;; Your code follows... cl-waffe2 does not cause name clashes with other libraries or existing functions. In addition, with regard to the function that generates the node is ! , there is a rule that functions that create a node must start with ! (ignoring the exception of the proceed function).","title":"To Get Started!"},{"location":"overview/#differentiable-operations-based-on-multiple-backends","text":"All operations in cl-waffe2 can be performed in the following form. [AbstractNode] | |--------------------|------------------------| [CPU Implementation1] [CPU Implementation2] [CUDA Implementation1] ... AbstractNode is a class declared via defnode macro, and each implementation is implemented via define-impl macro. There can be more than one implementation for a single device. (e.g.: it is possible to have a normal implementation and an approximate implementation for the exp function). One of the concepts is to minimise code re-writing by defining abstract nodes and switching the backends that executes them depending on the device they run on and the speed required. As an example, consider implementing the addition operation !add . The addition operation AddNode is the operation of finding the sum of two given matrices A and B and storing the result in A. (defnode (AddNode (myself) :where (A[~] B[~] -> A[~]) :documentation \"A <- A + B\" :backward ((self dout dx dy) (declare (ignore dx dy)) (values dout dout)) ;; Here follows constructor's body ;; AddNode class initialized is passed as *myself* ) Here, :where describes how matrices are performed. Before -> clause refers to the arguments, after -> clause refers to the shape of matrix after the operation. It describes: Takes A and B as arguments and returns a matrix of pointers of A All matrices have the same shape before and after the operation. :backward defines the operation of backward. This declaration can be made either in defnode or in define-impl , whichever you declare. The declared node can be initialized using the function (AddNode) , but seems returning errors. (AddNode) ;; -> Couldn't find any implementation of AddNode for (CPUTENSOR LISPTENSOR). This is because there is not yet a single implementation for AddNode , so let's define how AddNode works via define-impl macro. One operation can be defined for a backend that can be declared by inheriting from the cl-waffe2/vm.generic-tensor:AbstractTensor class. As of this writing(2023/06/20), cl-waffe2 provides following backends as standard. LispTensor - Portable/Safety First, speed comes second. Everything works on ANSI Common Lisp. CPUTensor - This is SBCL-dependant, but supports OpenBLAS linear-algebra APIs. Of course, if necessary, you can create a new backend. (defclass MyTensor (AbstractTensor) nil) (See also: tensor.lisp ) After defining a new backend, it is NOT necessary to give a re-implementation for all standard implementations in cl-waffe2. Select the appropriate backends in order of array compatibility. Anyway, this is how AddNode is defined for LispTensor in cl-waffe2. (define-impl (AddNode :device LispTensor) :forward ((self x y) (let ((adder (matrix-add (dtype x)))) `(,@(call-with-view #'(lambda (x-view y-view) `(funcall ,adder (tensor-vec ,x) (tensor-vec ,y) ,(offset-of x-view 0) ,(offset-of y-view 0) ,(size-of x-view 0) ,(stride-of x-view 0) ,(stride-of y-view 0))) `(,x ,y)) ,x)))) In :forward write the expansion expression for the operation in the same way as when defining a macro with defmacro . (see below for details). Why define-impl takes such a roundabout approach? To generate a fast code depending on the matrix size and data type at runtime. To pre-calculate all Indexes It is possible to generate, for example, C code without necessarily performing the same operations. (I believe that ideas on this macro needed to be given more thoughts, indeed, this is ugly...) Let's Perform operations with the defined AddNode! (forward (AddNode) (randn `(10 10)) (randn `(10 10))) {CPUTENSOR[float] :shape (10 10) :named ChainTMP9412 :vec-state [maybe-not-computed] ((-0.33475596 1.0127474 -0.060175765 ~ 1.4573603 -0.987001 -1.0165008) (-0.045512 -0.17995936 0.23593931 ~ 0.8409552 2.6434622 -0.5789532) ... (0.13282542 1.9386152 0.16213055 ~ 0.4363958 0.8294802 -0.1558509) (1.1732875 -1.5769591 -1.2152125 ~ -0.2833903 -0.81108683 0.9846606)) :facet :input :requires-grad NIL :backward <Node: ADDNODE-CPUTENSOR (A[~] B[~] -> A[~])>} Look at :vec-state, at that moment, the operation is still not done yet. The tensor displayed is the equivalent to the first argument. In cl-waffe2, all operations are lazy-evaluated, being JIT-compiled/Optimized/Parallelized via build , or proceed function. You would think that this style programming would make your task more complex, but don't worry, we provide APIs that is as close as possible to defined-by-run, and REPL-Friendly. (proceed (!add (AddNode) (randn `(10 10)) (randn `(10 10)))) ;; proceed-time function measures execution time without compiling time. (proceed-time (!add (AddNode) (randn `(10 10)) (randn `(10 10)))) Evaluation took: 0.000 seconds of real time 0.000014 seconds of total run time (0.000014 user, 0.000000 system) 100.00% CPU 30,512 processor cycles 0 bytes consed {CPUTENSOR[float] :shape (10 10) :named ChainTMP9447 :vec-state [computed] ((-1.5820543 2.2804832 -0.5613338 ~ 1.1143546 -1.3096298 -1.3756635) (-1.5208249 0.21621853 2.660368 ~ -1.032644 0.25917292 -1.9737494) ... (2.2557664 2.4791012 -0.04298857 ~ -1.2520232 1.8216541 -2.818116) (0.8615336 0.92017823 -0.25378937 ~ 0.9697968 -0.6300591 1.5660275)) :facet :input :requires-grad NIL :backward <Node: PROCEEDNODE-T (A[~] -> A[~])>} You can switch backends via (with-devices (&rest devices) &body body) macro seamlessly. (with-devices (LispTensor CPUTensor) ;; The priority of dispatching backends is: LispTensor(First) -> CPUTensor(Second) ;; If there's no any impls on LispTensor, use CPUTensor instead. (!add (randn `(10 10)) (randn `(10 10))))","title":"Differentiable operations based on multiple backends"},{"location":"overview/#jit-compile-in-place-optimizing","text":"As I said Everything is lazy-evaluated, and compiled , JIT Compiling is a one of main idea of this project. Mainly, this produces two benefits.","title":"JIT compile, In-place optimizing"},{"location":"overview/#infinite-number-of-epochs-no-overheads-of-funcall","text":"As all lisper know, there is a unignorable overhead when calling methods. (defmethod test-method ((a fixnum) (b fixnum)) (+ a b)) (defmethod test-method ((a single-float) (b single-float)) (+ a b)) (time (dotimes (i 100000000) (test-method 1.0 1.0))) Evaluation took: 0.560 seconds of real time 0.554936 seconds of total run time (0.551612 user, 0.003324 system) 99.11% CPU 1,291,693,656 processor cycles 0 bytes consed (defun test-fun (a b) (declare (type single-float a b)) (+ a b)) ;; Also, defun can be inlined at the end. (time (dotimes (i 100000000) (test-fun 1.0 1.0))) Evaluation took: 0.298 seconds of real time 0.297827 seconds of total run time (0.296968 user, 0.000859 system) 100.00% CPU 688,686,522 processor cycles 0 bytes consed In this project, which uses a large number of generic functions!, this overhead becomes non-negligible at every Epoch, especially when the matrix size is small. Therefore, we took the approach of defining a new function by cutting out the necessary operations from the lazy-evaluated nodes, part by part. ;; cl-waffe2's benchmark TODO: Update This section (let ((f (build (!sin 1.0)))) (time (dotimes (i 100000) (funcall f)))) ;; Fix: tensor-reset!'s overhead... (defun test-f (x) (sin (sin (sin (sin x))))) (time (dotimes (i 100000) (test-f 1.0)))","title":"Infinite number of Epochs, No Overheads of funcall."},{"location":"overview/#in-place-optimizing","text":"This is a usual function in cl-waffe2, which finds the sum of A and B. (!add a b) But internally, the operation makes a copy not to produce side effects. (forward (AddNode) (!copy a) b) Without making a copy, the value of A would be destructed instead of having to allocate extra memory. (let ((a (make-tensor `(3 3) :initial-element 1.0))) (print a) ;; {CPUTENSOR[float] :shape (3 3) ;; ((1.0 1.0 1.0) ;; (1.0 1.0 1.0) ;; (1.0 1.0 1.0)) ;; :facet :exist ;; :requires-grad NIL ;; :backward NIL} ;; (eval A <- A + B) (proceed (forward (AddNode :float) a (randn `(3 3)))) (print a) ;; {CPUTENSOR[float] :shape (3 3) ;; ((2.0100088 0.2906983 1.5334041) ;; (-0.50357413 2.389317 0.7051847) ;; (1.3005692 1.5925546 0.95498145)) ;; :facet :exist ;; :requires-grad NIL ;; :backward NIL} ) Operations that do not allocate extra space are called in-place (or sometimes destructive operations?). Making operations in-place is a rational way to optimize your programs, but this is a trade-off with readability, because the coding style is more like a programming notation than a mathematical notation. Let's take another example. o u t = f ( I n p u t ) + f ( f ( T e n s o r ) ) out = f(Input) + f(f(Tensor)) o u t = f ( I n p u t ) + f ( f ( T e n sor )) (TODO) (defnode (1DFunc (self) :where (A[~] -> A[~]))) (define-impl (1DFunc :device LispTensor) :forward ((self x) `(progn ,x)) :backward ((self dout dx) (values dout))) (defun f (tensor) (forward (1DFunc) (!copy tensor))) (let ((k (!add (make-input `(3 3) nil) (f (f (randn `(3 3) :requires-grad t)))))) (build k) (cl-waffe2/viz:viz-computation-node k \"assets/1d_fn_arg.dot\"))","title":"In-place optimizing"},{"location":"overview/#before-optimized-vs-after-optimized","text":"(TODO)","title":"Before Optimized Vs After Optimized."},{"location":"overview/#broadcasting-apis-view-apis","text":"!flexible !view","title":"Broadcasting APIs, View APIs"},{"location":"overview/#repl-friendly-function-proceed","text":"Proceed Proceed-backward Proceed-time","title":"REPL-Friendly function, proceed"},{"location":"overview/#strong-shaping-apis","text":"syntax of :where pharse Shape Error Reports","title":"Strong Shaping APIs"},{"location":"overview/#constructing-networks-with-defnodedefmodel","text":"defnode defmodel call forward","title":"Constructing networks with defnode/defmodel"},{"location":"overview/#data-structures","text":"Parameter/Tensor/Input/ScalarTensor","title":"Data Structures"},{"location":"overview/#optimizing-parameters","text":"defoptimizer Tutorials Over! I'll keep my finger crossed.","title":"Optimizing Parameters"}]}